2019-01-29 08:13:41.961 UTC [localconfig] completeInitialization -> INFO 001[0m Kafka.Version unset, setting to 0.10.2.0
[36m2019-01-29 08:13:41.961 UTC [bccsp_sw] openKeyStore -> DEBU 002[0m KeyStore opened at [/etc/hyperledger/fabric/orderer/msp/keystore]...done
[36m2019-01-29 08:13:41.961 UTC [bccsp] initBCCSP -> DEBU 003[0m Initialize BCCSP [SW]
[36m2019-01-29 08:13:41.961 UTC [msp] getPemMaterialFromDir -> DEBU 004[0m Reading directory /etc/hyperledger/fabric/orderer/msp/signcerts
[36m2019-01-29 08:13:41.961 UTC [msp] getPemMaterialFromDir -> DEBU 005[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/signcerts/orderer0.hospital1.switch2logic.co.za-cert.pem
[36m2019-01-29 08:13:41.961 UTC [msp] getPemMaterialFromDir -> DEBU 006[0m Reading directory /etc/hyperledger/fabric/orderer/msp/cacerts
[36m2019-01-29 08:13:41.961 UTC [msp] getPemMaterialFromDir -> DEBU 007[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/cacerts/ca.switch2logic.co.za-cert.pem
[36m2019-01-29 08:13:41.962 UTC [msp] getPemMaterialFromDir -> DEBU 008[0m Reading directory /etc/hyperledger/fabric/orderer/msp/admincerts
[36m2019-01-29 08:13:41.962 UTC [msp] getPemMaterialFromDir -> DEBU 009[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/admincerts/Admin@switch2logic.co.za-cert.pem
[36m2019-01-29 08:13:41.962 UTC [msp] getPemMaterialFromDir -> DEBU 00a[0m Reading directory /etc/hyperledger/fabric/orderer/msp/intermediatecerts
[36m2019-01-29 08:13:41.962 UTC [msp] getMspConfig -> DEBU 00b[0m Intermediate certs folder not found at [/etc/hyperledger/fabric/orderer/msp/intermediatecerts]. Skipping. [stat /etc/hyperledger/fabric/orderer/msp/intermediatecerts: no such file or directory]
[36m2019-01-29 08:13:41.962 UTC [msp] getPemMaterialFromDir -> DEBU 00c[0m Reading directory /etc/hyperledger/fabric/orderer/msp/tlscacerts
[36m2019-01-29 08:13:41.962 UTC [msp] getPemMaterialFromDir -> DEBU 00d[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/tlscacerts/tlsca.switch2logic.co.za-cert.pem
[36m2019-01-29 08:13:41.962 UTC [msp] getPemMaterialFromDir -> DEBU 00e[0m Reading directory /etc/hyperledger/fabric/orderer/msp/tlsintermediatecerts
[36m2019-01-29 08:13:41.962 UTC [msp] getMspConfig -> DEBU 00f[0m TLS intermediate certs folder not found at [/etc/hyperledger/fabric/orderer/msp/tlsintermediatecerts]. Skipping. [stat /etc/hyperledger/fabric/orderer/msp/tlsintermediatecerts: no such file or directory]
[36m2019-01-29 08:13:41.962 UTC [msp] getPemMaterialFromDir -> DEBU 010[0m Reading directory /etc/hyperledger/fabric/orderer/msp/crls
[36m2019-01-29 08:13:41.962 UTC [msp] getMspConfig -> DEBU 011[0m crls folder not found at [/etc/hyperledger/fabric/orderer/msp/crls]. Skipping. [stat /etc/hyperledger/fabric/orderer/msp/crls: no such file or directory]
[36m2019-01-29 08:13:41.962 UTC [msp] getMspConfig -> DEBU 012[0m MSP configuration file not found at [/etc/hyperledger/fabric/orderer/msp/config.yaml]: [stat /etc/hyperledger/fabric/orderer/msp/config.yaml: no such file or directory]
[36m2019-01-29 08:13:41.962 UTC [msp] newBccspMsp -> DEBU 013[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:13:41.962 UTC [msp] New -> DEBU 014[0m Creating Cache-MSP instance
[36m2019-01-29 08:13:41.963 UTC [msp] loadLocaMSP -> DEBU 015[0m Created new local MSP
[36m2019-01-29 08:13:41.963 UTC [msp] Setup -> DEBU 016[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 08:13:41.964 UTC [msp/identity] newIdentity -> DEBU 017[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 08:13:41.964 UTC [msp/identity] newIdentity -> DEBU 018[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 08:13:42.007 UTC [msp/identity] newIdentity -> DEBU 019[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICLDCCAdKgAwIBAgIQIoN1EApgEjYu/O7unXVqYzAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBqMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEuMCwGA1UEAxMlb3JkZXJlcjAuaG9zcGl0YWwxLnN3
aXRjaDJsb2dpYy5jby56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABH2/gH3f
8uhcO2CKglQg53cAJL79dENZNdRWfaRikU5j3X60lh9Bx5YpznQ5l0qMaCK1Ogec
Nw5LGgie9ESxrHyjTTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsG
A1UdIwQkMCKAIBiGNntD5tBlG9C8QNsdXde3ItTRRa+p/+3yJwRCsbJxMAoGCCqG
SM49BAMCA0gAMEUCIQCBtzCy6LE3S8b/XG08Qcs01A8QjmCwqmZJQrDvRA8ZhgIg
aAwP9tYtEZs1DBv2bH7HwYIwmi8nlgD3+Xp6CWwvAkg=
-----END CERTIFICATE-----
[36m2019-01-29 08:13:42.007 UTC [bccsp_sw] loadPrivateKey -> DEBU 01a[0m Loading private key [f9564620d77ba2b2650b4916b06387b38f212a58c63268d4c08f9b019eaa0f87] at [/etc/hyperledger/fabric/orderer/msp/keystore/f9564620d77ba2b2650b4916b06387b38f212a58c63268d4c08f9b019eaa0f87_sk]...
[36m2019-01-29 08:13:42.007 UTC [msp/identity] newIdentity -> DEBU 01b[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICLDCCAdKgAwIBAgIQIoN1EApgEjYu/O7unXVqYzAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBqMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEuMCwGA1UEAxMlb3JkZXJlcjAuaG9zcGl0YWwxLnN3
aXRjaDJsb2dpYy5jby56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABH2/gH3f
8uhcO2CKglQg53cAJL79dENZNdRWfaRikU5j3X60lh9Bx5YpznQ5l0qMaCK1Ogec
Nw5LGgie9ESxrHyjTTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsG
A1UdIwQkMCKAIBiGNntD5tBlG9C8QNsdXde3ItTRRa+p/+3yJwRCsbJxMAoGCCqG
SM49BAMCA0gAMEUCIQCBtzCy6LE3S8b/XG08Qcs01A8QjmCwqmZJQrDvRA8ZhgIg
aAwP9tYtEZs1DBv2bH7HwYIwmi8nlgD3+Xp6CWwvAkg=
-----END CERTIFICATE-----
[36m2019-01-29 08:13:42.008 UTC [msp] setupSigningIdentity -> DEBU 01c[0m Signing identity expires at 2029-01-26 07:41:54 +0000 UTC
[36m2019-01-29 08:13:42.008 UTC [msp] Validate -> DEBU 01d[0m MSP OrdererMSP validating identity
2019-01-29 08:13:42.008 UTC [orderer/common/server] prettyPrintStruct -> INFO 01e[0m Orderer config values:
	General.LedgerType = "file"
	General.ListenAddress = "0.0.0.0"
	General.ListenPort = 7050
	General.TLS.Enabled = true
	General.TLS.PrivateKey = "/etc/hyperledger/fabric/orderer/tls/server.key"
	General.TLS.Certificate = "/etc/hyperledger/fabric/orderer/tls/server.crt"
	General.TLS.RootCAs = [/etc/hyperledger/fabric/orderer/tls/ca.crt]
	General.TLS.ClientAuthRequired = false
	General.TLS.ClientRootCAs = []
	General.Keepalive.ServerMinInterval = 1m0s
	General.Keepalive.ServerInterval = 2h0m0s
	General.Keepalive.ServerTimeout = 20s
	General.GenesisMethod = "file"
	General.GenesisProfile = "SampleInsecureSolo"
	General.SystemChannel = "test-system-channel-name"
	General.GenesisFile = "/etc/hyperledger/fabric/orderer/channel-artifacts/genesis.block"
	General.Profile.Enabled = false
	General.Profile.Address = "0.0.0.0:6060"
	General.LogLevel = "debug"
	General.LogFormat = "%{color}%{time:2006-01-02 15:04:05.000 MST} [%{module}] %{shortfunc} -> %{level:.4s} %{id:03x}%{color:reset} %{message}"
	General.LocalMSPDir = "/etc/hyperledger/fabric/orderer/msp"
	General.LocalMSPID = "OrdererMSP"
	General.BCCSP.ProviderName = "SW"
	General.BCCSP.SwOpts.SecLevel = 256
	General.BCCSP.SwOpts.HashFamily = "SHA2"
	General.BCCSP.SwOpts.Ephemeral = false
	General.BCCSP.SwOpts.FileKeystore.KeyStorePath = "/etc/hyperledger/fabric/orderer/msp/keystore"
	General.BCCSP.SwOpts.DummyKeystore =
	General.BCCSP.PluginOpts =
	General.Authentication.TimeWindow = 15m0s
	FileLedger.Location = "/var/hyperledger/production/orderer"
	FileLedger.Prefix = "hyperledger-fabric-ordererledger"
	RAMLedger.HistorySize = 1000
	Kafka.Retry.ShortInterval = 1s
	Kafka.Retry.ShortTotal = 30s
	Kafka.Retry.LongInterval = 5m0s
	Kafka.Retry.LongTotal = 12h0m0s
	Kafka.Retry.NetworkTimeouts.DialTimeout = 10s
	Kafka.Retry.NetworkTimeouts.ReadTimeout = 10s
	Kafka.Retry.NetworkTimeouts.WriteTimeout = 10s
	Kafka.Retry.Metadata.RetryMax = 3
	Kafka.Retry.Metadata.RetryBackoff = 250ms
	Kafka.Retry.Producer.RetryMax = 3
	Kafka.Retry.Producer.RetryBackoff = 100ms
	Kafka.Retry.Consumer.RetryBackoff = 2s
	Kafka.Verbose = true
	Kafka.Version = 0.10.2.0
	Kafka.TLS.Enabled = false
	Kafka.TLS.PrivateKey = ""
	Kafka.TLS.Certificate = ""
	Kafka.TLS.RootCAs = []
	Kafka.TLS.ClientAuthRequired = false
	Kafka.TLS.ClientRootCAs = []
	Debug.BroadcastTraceDir = ""
	Debug.DeliverTraceDir = ""
2019-01-29 08:13:42.008 UTC [orderer/common/server] initializeServerConfig -> INFO 01f[0m Starting orderer with TLS enabled
[36m2019-01-29 08:13:42.009 UTC [orderer/common/server] createLedgerFactory -> DEBU 020[0m Ledger dir: /var/hyperledger/production/orderer
[36m2019-01-29 08:13:42.009 UTC [kvledger.util] CreateDirIfMissing -> DEBU 021[0m CreateDirIfMissing [/var/hyperledger/production/orderer/index/]
[36m2019-01-29 08:13:42.009 UTC [kvledger.util] logDirStatus -> DEBU 022[0m Before creating dir - [/var/hyperledger/production/orderer/index/] does not exist
[36m2019-01-29 08:13:42.009 UTC [kvledger.util] logDirStatus -> DEBU 023[0m After creating dir - [/var/hyperledger/production/orderer/index/] exists
[36m2019-01-29 08:13:42.029 UTC [fsblkstorage] newBlockfileMgr -> DEBU 024[0m newBlockfileMgr() initializing file-based block storage for ledger: testchainid 
[36m2019-01-29 08:13:42.029 UTC [kvledger.util] CreateDirIfMissing -> DEBU 025[0m CreateDirIfMissing [/var/hyperledger/production/orderer/chains/testchainid/]
[36m2019-01-29 08:13:42.029 UTC [kvledger.util] logDirStatus -> DEBU 026[0m Before creating dir - [/var/hyperledger/production/orderer/chains/testchainid/] does not exist
[36m2019-01-29 08:13:42.030 UTC [kvledger.util] logDirStatus -> DEBU 027[0m After creating dir - [/var/hyperledger/production/orderer/chains/testchainid/] exists
2019-01-29 08:13:42.030 UTC [fsblkstorage] newBlockfileMgr -> INFO 028[0m Getting block information from block storage
[36m2019-01-29 08:13:42.030 UTC [fsblkstorage] constructCheckpointInfoFromBlockFiles -> DEBU 029[0m Retrieving checkpoint info from block files
[36m2019-01-29 08:13:42.030 UTC [fsblkstorage] retrieveLastFileSuffix -> DEBU 02a[0m retrieveLastFileSuffix()
[36m2019-01-29 08:13:42.030 UTC [fsblkstorage] retrieveLastFileSuffix -> DEBU 02b[0m retrieveLastFileSuffix() - biggestFileNum = -1
[36m2019-01-29 08:13:42.030 UTC [fsblkstorage] constructCheckpointInfoFromBlockFiles -> DEBU 02c[0m Last file number found = -1
[36m2019-01-29 08:13:42.030 UTC [fsblkstorage] constructCheckpointInfoFromBlockFiles -> DEBU 02d[0m No block file found
[36m2019-01-29 08:13:42.030 UTC [fsblkstorage] newBlockfileMgr -> DEBU 02e[0m Info constructed by scanning the blocks dir = (*fsblkstorage.checkpointInfo)(0xc4200b8d80)(latestFileChunkSuffixNum=[0], latestFileChunksize=[0], isChainEmpty=[true], lastBlockNumber=[0])
[36m2019-01-29 08:13:42.033 UTC [fsblkstorage] newBlockIndex -> DEBU 02f[0m newBlockIndex() - indexItems:[[BlockNum]]
[36m2019-01-29 08:13:42.035 UTC [fsblkstorage] indexBlock -> DEBU 030[0m Indexing block [blockNum=0, blockHash=[]byte{0xc8, 0x4c, 0x51, 0xe5, 0x9, 0x21, 0xc0, 0x29, 0x44, 0x5, 0xe4, 0x1d, 0x16, 0xcb, 0x62, 0x34, 0x27, 0x3f, 0x63, 0xb3, 0x7a, 0x24, 0x4, 0x38, 0xcc, 0x9f, 0x32, 0x57, 0x82, 0x77, 0xd7, 0xe3} txOffsets=
txId=1ca8e655ef3c6dabf8d05124961ee605d33f75882de63b75c8df80edea408fb6 locPointer=offset=39, bytesLength=18451
]
[36m2019-01-29 08:13:42.037 UTC [fsblkstorage] updateCheckpoint -> DEBU 031[0m Broadcasting about update checkpointInfo: latestFileChunkSuffixNum=[0], latestFileChunksize=[18495], isChainEmpty=[false], lastBlockNumber=[0]
[36m2019-01-29 08:13:42.038 UTC [fsblkstorage] Next -> DEBU 032[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 08:13:42.038 UTC [fsblkstorage] newBlockfileStream -> DEBU 033[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 08:13:42.038 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 034[0m Remaining bytes=[18495], Going to peek [8] bytes
[36m2019-01-29 08:13:42.038 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 035[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 08:13:42.038 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 036[0m blockbytes [18492] read from file [0]
[36m2019-01-29 08:13:42.038 UTC [fsblkstorage] Next -> DEBU 037[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 08:13:42.038 UTC [fsblkstorage] newBlockfileStream -> DEBU 038[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 08:13:42.038 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 039[0m Remaining bytes=[18495], Going to peek [8] bytes
[36m2019-01-29 08:13:42.038 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 03a[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 08:13:42.038 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 03b[0m blockbytes [18492] read from file [0]
[36m2019-01-29 08:13:42.039 UTC [common/channelconfig] NewStandardValues -> DEBU 03c[0m Initializing protos for *channelconfig.ChannelProtos
[36m2019-01-29 08:13:42.039 UTC [common/channelconfig] initializeProtosStruct -> DEBU 03d[0m Processing field: HashingAlgorithm
[36m2019-01-29 08:13:42.039 UTC [common/channelconfig] initializeProtosStruct -> DEBU 03e[0m Processing field: BlockDataHashingStructure
[36m2019-01-29 08:13:42.039 UTC [common/channelconfig] initializeProtosStruct -> DEBU 03f[0m Processing field: OrdererAddresses
[36m2019-01-29 08:13:42.039 UTC [common/channelconfig] initializeProtosStruct -> DEBU 040[0m Processing field: Consortium
[36m2019-01-29 08:13:42.039 UTC [common/channelconfig] initializeProtosStruct -> DEBU 041[0m Processing field: Capabilities
[36m2019-01-29 08:13:42.039 UTC [common/channelconfig] NewStandardValues -> DEBU 042[0m Initializing protos for *channelconfig.ConsortiumProtos
[36m2019-01-29 08:13:42.039 UTC [common/channelconfig] initializeProtosStruct -> DEBU 043[0m Processing field: ChannelCreationPolicy
[36m2019-01-29 08:13:42.039 UTC [common/channelconfig] NewStandardValues -> DEBU 044[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:13:42.039 UTC [common/channelconfig] initializeProtosStruct -> DEBU 045[0m Processing field: MSP
[36m2019-01-29 08:13:42.039 UTC [common/channelconfig] validateMSP -> DEBU 046[0m Setting up MSP for org Hospital1MSP
[36m2019-01-29 08:13:42.039 UTC [msp] newBccspMsp -> DEBU 047[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:13:42.039 UTC [msp] New -> DEBU 048[0m Creating Cache-MSP instance
[36m2019-01-29 08:13:42.039 UTC [msp] Setup -> DEBU 049[0m Setting up MSP instance Hospital1MSP
[36m2019-01-29 08:13:42.040 UTC [msp/identity] newIdentity -> DEBU 04a[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdDCCAhugAwIBAgIQZxH8VsnceZxXgKxlURLgVTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMS5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQX9V4k/h/3pFxByKDscjgQrC0YS/bSbKvzSG2Z2PvltEWp40qB9OaKUN0GQYzo
yK2Etdvyjfd1OFwaTjpGSvv9o18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCBVU7DoP9Msf/nNTNl6
5FxwAXZ82eaGBT7zWYdyh4QbBjAKBggqhkjOPQQDAgNHADBEAiBmXGWQbxCGRp0c
5/y9qJyciB1CDHsknRZDjNzt5f563wIgMy7VPi1R8DpIIIRN9B3mfhDpRa+J+8PA
S4DOJy7rjp4=
-----END CERTIFICATE-----
[36m2019-01-29 08:13:42.040 UTC [msp/identity] newIdentity -> DEBU 04b[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 08:13:42.041 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 04c[0m Checking if identity satisfies role [CLIENT] for Hospital1MSP
[36m2019-01-29 08:13:42.041 UTC [msp] Validate -> DEBU 04d[0m MSP Hospital1MSP validating identity
[36m2019-01-29 08:13:42.041 UTC [msp] getCertificationChain -> DEBU 04e[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 08:13:42.041 UTC [msp] hasOURole -> DEBU 04f[0m MSP Hospital1MSP checking if the identity is a client
[36m2019-01-29 08:13:42.041 UTC [msp] getCertificationChain -> DEBU 050[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 08:13:42.041 UTC [common/channelconfig] NewStandardValues -> DEBU 051[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:13:42.041 UTC [common/channelconfig] initializeProtosStruct -> DEBU 052[0m Processing field: MSP
[36m2019-01-29 08:13:42.041 UTC [common/channelconfig] validateMSP -> DEBU 053[0m Setting up MSP for org Hospital2MSP
[36m2019-01-29 08:13:42.041 UTC [msp] newBccspMsp -> DEBU 054[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:13:42.041 UTC [msp] New -> DEBU 055[0m Creating Cache-MSP instance
[36m2019-01-29 08:13:42.042 UTC [msp] Setup -> DEBU 056[0m Setting up MSP instance Hospital2MSP
[36m2019-01-29 08:13:42.042 UTC [msp/identity] newIdentity -> DEBU 057[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdjCCAhygAwIBAgIRALZ3Pffn/xWUtUWPSK6CYVAwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowgYsxCzAJBgNVBAYTAlVTMRMwEQYD
VQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4gRnJhbmNpc2NvMSUwIwYDVQQK
Exxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMSgwJgYDVQQDEx9jYS5ob3Nw
aXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcD
QgAESzVpiz6P4mIb+KwvrttcXJKZR5i137Is4298E7OrkYdAg0tB4rpfKTXzUMwW
vI12cTDujpyaZSiqQ3/zP4fOQqNfMF0wDgYDVR0PAQH/BAQDAgGmMA8GA1UdJQQI
MAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQg8QJqWKPaX24HINwR
S6sINFSwRD+440/ccSnvFVaGF24wCgYIKoZIzj0EAwIDSAAwRQIhAPO2Nng3/nqb
cKPliUqVPVjBMu+D55M90+jexWSOpkjcAiADPx8qv9exZ4BfP3+G49Qzpx0Meblj
53vzUgYFa/0EXw==
-----END CERTIFICATE-----
[36m2019-01-29 08:13:42.042 UTC [msp/identity] newIdentity -> DEBU 058[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAJ6XtHMTTW1JzdpD54qCc3wwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA+z1rb9aA1HwHfNLi9V9VZe
bdwT4MXak445zGz4DXzRUsmNeBngzntKb6qpgHQpeFtzbzLx+zwiynRDvfdzORGj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPEC
alij2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQC
IA8A3CCJeCasyy7gYd/M1aHYmwWsecY7UY7cVUGy4XOxAiB0rrmfnGW6JA11PDCu
+5c5Rpn0jF5ddsFn+UIpzBFvmg==
-----END CERTIFICATE-----
[36m2019-01-29 08:13:42.047 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 059[0m Checking if identity satisfies role [CLIENT] for Hospital2MSP
[36m2019-01-29 08:13:42.047 UTC [msp] Validate -> DEBU 05a[0m MSP Hospital2MSP validating identity
[36m2019-01-29 08:13:42.047 UTC [msp] getCertificationChain -> DEBU 05b[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 08:13:42.047 UTC [msp] hasOURole -> DEBU 05c[0m MSP Hospital2MSP checking if the identity is a client
[36m2019-01-29 08:13:42.047 UTC [msp] getCertificationChain -> DEBU 05d[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 08:13:42.047 UTC [common/channelconfig] NewStandardValues -> DEBU 05e[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:13:42.048 UTC [common/channelconfig] initializeProtosStruct -> DEBU 05f[0m Processing field: MSP
[36m2019-01-29 08:13:42.048 UTC [common/channelconfig] validateMSP -> DEBU 060[0m Setting up MSP for org Hospital3MSP
[36m2019-01-29 08:13:42.048 UTC [msp] newBccspMsp -> DEBU 061[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:13:42.048 UTC [msp] New -> DEBU 062[0m Creating Cache-MSP instance
[36m2019-01-29 08:13:42.048 UTC [msp] Setup -> DEBU 063[0m Setting up MSP instance Hospital3MSP
[36m2019-01-29 08:13:42.048 UTC [msp/identity] newIdentity -> DEBU 064[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdTCCAhugAwIBAgIQCCXeHGgObIJo8aq4AdvgVjAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMy5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQm1Oo/AEu7ZZToE15Y5ntlaN128Hs/exX3jNH5nO6r4tvyFtBBNEn1ecXxxPtm
dmQ+nPHONorLwDC42SMAKwfro18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCA7vVWFXWpgZBojQ1ZL
xUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEAlOUoPtpfO8V7
vFRCanjU5zZWGO5ajFlPDJjjDTy3UrwCICrUYV32JrR/m3dslCl15dSThHrKtYCC
Ehvyw1+lNqiG
-----END CERTIFICATE-----
[36m2019-01-29 08:13:42.048 UTC [msp/identity] newIdentity -> DEBU 065[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAI8BA/5Jf+5m8ncI+D7aSiMwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwzLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABF1IehIKIjh8tXYZ7rw4QOuc
g/tpBAkJeOt44zr/LepbjhyFW8vIRnkw75i6Wcg+LOoAS+2NFNg2oNjqg//yCHWj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIDu9
VYVdamBkGiNDVkvFS+AEt6Jrn0yK8hFyPGZhqenJMAoGCCqGSM49BAMCA0cAMEQC
IFyc85pwbCAxGNdw4fflsX2I3HrP3h93YAB2zSDC8HtkAiAQxblAZfEnXUiQKrqE
8I10bYJqD6pJz1jb3Pv+4xplTA==
-----END CERTIFICATE-----
[36m2019-01-29 08:13:42.048 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 066[0m Checking if identity satisfies role [CLIENT] for Hospital3MSP
[36m2019-01-29 08:13:42.048 UTC [msp] Validate -> DEBU 067[0m MSP Hospital3MSP validating identity
[36m2019-01-29 08:13:42.049 UTC [msp] getCertificationChain -> DEBU 068[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 08:13:42.049 UTC [msp] hasOURole -> DEBU 069[0m MSP Hospital3MSP checking if the identity is a client
[36m2019-01-29 08:13:42.049 UTC [msp] getCertificationChain -> DEBU 06a[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 08:13:42.049 UTC [common/channelconfig] NewStandardValues -> DEBU 06b[0m Initializing protos for *channelconfig.OrdererProtos
[36m2019-01-29 08:13:42.049 UTC [common/channelconfig] initializeProtosStruct -> DEBU 06c[0m Processing field: ConsensusType
[36m2019-01-29 08:13:42.049 UTC [common/channelconfig] initializeProtosStruct -> DEBU 06d[0m Processing field: BatchSize
[36m2019-01-29 08:13:42.049 UTC [common/channelconfig] initializeProtosStruct -> DEBU 06e[0m Processing field: BatchTimeout
[36m2019-01-29 08:13:42.049 UTC [common/channelconfig] initializeProtosStruct -> DEBU 06f[0m Processing field: KafkaBrokers
[36m2019-01-29 08:13:42.049 UTC [common/channelconfig] initializeProtosStruct -> DEBU 070[0m Processing field: ChannelRestrictions
[36m2019-01-29 08:13:42.049 UTC [common/channelconfig] initializeProtosStruct -> DEBU 071[0m Processing field: Capabilities
[36m2019-01-29 08:13:42.050 UTC [common/channelconfig] NewStandardValues -> DEBU 072[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:13:42.050 UTC [common/channelconfig] initializeProtosStruct -> DEBU 073[0m Processing field: MSP
[36m2019-01-29 08:13:42.050 UTC [common/channelconfig] validateMSP -> DEBU 074[0m Setting up MSP for org OrdererOrg
[36m2019-01-29 08:13:42.050 UTC [msp] newBccspMsp -> DEBU 075[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:13:42.050 UTC [msp] New -> DEBU 076[0m Creating Cache-MSP instance
[36m2019-01-29 08:13:42.050 UTC [msp] Setup -> DEBU 077[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 08:13:42.050 UTC [msp/identity] newIdentity -> DEBU 078[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 08:13:42.050 UTC [msp/identity] newIdentity -> DEBU 079[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 08:13:42.051 UTC [msp] Validate -> DEBU 07a[0m MSP OrdererMSP validating identity
[36m2019-01-29 08:13:42.051 UTC [msp] Setup -> DEBU 07b[0m Setting up the MSP manager (4 msps)
[36m2019-01-29 08:13:42.051 UTC [msp] Setup -> DEBU 07c[0m MSP manager setup complete, setup 4 msps
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 07d[0m Proposed new policy Admins for Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 07e[0m Proposed new policy Readers for Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 07f[0m Proposed new policy Writers for Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 080[0m Proposed new policy Admins for Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 081[0m Proposed new policy Readers for Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 082[0m Proposed new policy Writers for Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 083[0m Proposed new policy Writers for Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 084[0m Proposed new policy Admins for Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 085[0m Proposed new policy Readers for Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 086[0m Proposed new policy Admins for Channel/Consortiums
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 087[0m Proposed new policy Readers for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 088[0m Proposed new policy Writers for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 089[0m Proposed new policy Admins for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 08a[0m Proposed new policy Admins for Channel/Orderer
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 08b[0m Proposed new policy Readers for Channel/Orderer
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 08c[0m Proposed new policy Writers for Channel/Orderer
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 08d[0m Proposed new policy BlockValidation for Channel/Orderer
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 08e[0m Proposed new policy Admins for Channel
[36m2019-01-29 08:13:42.051 UTC [policies] GetPolicy -> DEBU 08f[0m Returning dummy reject all policy because Readers could not be found in Channel/Consortiums/Readers
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 090[0m Proposed new policy Readers for Channel
[36m2019-01-29 08:13:42.051 UTC [policies] GetPolicy -> DEBU 091[0m Returning dummy reject all policy because Writers could not be found in Channel/Consortiums/Writers
[36m2019-01-29 08:13:42.051 UTC [policies] NewManagerImpl -> DEBU 092[0m Proposed new policy Writers for Channel
[36m2019-01-29 08:13:42.051 UTC [common/configtx] addToMap -> DEBU 093[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 08:13:42.051 UTC [common/configtx] addToMap -> DEBU 094[0m Adding to config map: [Group]  /Channel/Consortiums
[36m2019-01-29 08:13:42.051 UTC [common/configtx] addToMap -> DEBU 095[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium
[36m2019-01-29 08:13:42.051 UTC [common/configtx] addToMap -> DEBU 096[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 08:13:42.051 UTC [common/configtx] addToMap -> DEBU 097[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/Hospital3MSP/MSP
[36m2019-01-29 08:13:42.051 UTC [common/configtx] addToMap -> DEBU 098[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital3MSP/Admins
[36m2019-01-29 08:13:42.051 UTC [common/configtx] addToMap -> DEBU 099[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital3MSP/Readers
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 09a[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital3MSP/Writers
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 09b[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 09c[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/Hospital1MSP/MSP
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 09d[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital1MSP/Admins
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 09e[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital1MSP/Readers
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 09f[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital1MSP/Writers
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0a0[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0a1[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/Hospital2MSP/MSP
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0a2[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital2MSP/Readers
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0a3[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital2MSP/Writers
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0a4[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital2MSP/Admins
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0a5[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/ChannelCreationPolicy
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0a6[0m Adding to config map: [Policy] /Channel/Consortiums/Admins
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0a7[0m Adding to config map: [Group]  /Channel/Orderer
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0a8[0m Adding to config map: [Group]  /Channel/Orderer/OrdererOrg
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0a9[0m Adding to config map: [Value]  /Channel/Orderer/OrdererOrg/MSP
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0aa[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0ab[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0ac[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Admins
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0ad[0m Adding to config map: [Value]  /Channel/Orderer/ChannelRestrictions
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0ae[0m Adding to config map: [Value]  /Channel/Orderer/Capabilities
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0af[0m Adding to config map: [Value]  /Channel/Orderer/KafkaBrokers
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0b0[0m Adding to config map: [Value]  /Channel/Orderer/ConsensusType
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0b1[0m Adding to config map: [Value]  /Channel/Orderer/BatchSize
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0b2[0m Adding to config map: [Value]  /Channel/Orderer/BatchTimeout
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0b3[0m Adding to config map: [Policy] /Channel/Orderer/Readers
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0b4[0m Adding to config map: [Policy] /Channel/Orderer/Writers
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0b5[0m Adding to config map: [Policy] /Channel/Orderer/BlockValidation
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0b6[0m Adding to config map: [Policy] /Channel/Orderer/Admins
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0b7[0m Adding to config map: [Value]  /Channel/HashingAlgorithm
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0b8[0m Adding to config map: [Value]  /Channel/BlockDataHashingStructure
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0b9[0m Adding to config map: [Value]  /Channel/OrdererAddresses
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0ba[0m Adding to config map: [Value]  /Channel/Capabilities
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0bb[0m Adding to config map: [Policy] /Channel/Writers
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0bc[0m Adding to config map: [Policy] /Channel/Admins
[36m2019-01-29 08:13:42.052 UTC [common/configtx] addToMap -> DEBU 0bd[0m Adding to config map: [Policy] /Channel/Readers
[36m2019-01-29 08:13:42.052 UTC [common/channelconfig] LogSanityChecks -> DEBU 0be[0m As expected, current configuration has policy '/Channel/Readers'
[36m2019-01-29 08:13:42.052 UTC [common/channelconfig] LogSanityChecks -> DEBU 0bf[0m As expected, current configuration has policy '/Channel/Writers'
[36m2019-01-29 08:13:42.052 UTC [policies] Manager -> DEBU 0c0[0m Manager Channel looking up path [Application]
[36m2019-01-29 08:13:42.052 UTC [policies] Manager -> DEBU 0c1[0m Manager Channel has managers Consortiums
[36m2019-01-29 08:13:42.052 UTC [policies] Manager -> DEBU 0c2[0m Manager Channel has managers Orderer
[36m2019-01-29 08:13:42.052 UTC [policies] Manager -> DEBU 0c3[0m Manager Channel looking up path [Orderer]
[36m2019-01-29 08:13:42.052 UTC [policies] Manager -> DEBU 0c4[0m Manager Channel has managers Consortiums
[36m2019-01-29 08:13:42.052 UTC [policies] Manager -> DEBU 0c5[0m Manager Channel has managers Orderer
[36m2019-01-29 08:13:42.052 UTC [policies] Manager -> DEBU 0c6[0m Manager Channel/Orderer looking up path []
[36m2019-01-29 08:13:42.052 UTC [policies] Manager -> DEBU 0c7[0m Manager Channel/Orderer has managers OrdererOrg
[36m2019-01-29 08:13:42.052 UTC [common/channelconfig] LogSanityChecks -> DEBU 0c8[0m As expected, current configuration has policy '/Channel/Orderer/BlockValidation'
[36m2019-01-29 08:13:42.052 UTC [common/capabilities] Supported -> DEBU 0c9[0m Orderer capability V1_1 is supported and is enabled
[36m2019-01-29 08:13:42.052 UTC [common/capabilities] Supported -> DEBU 0ca[0m Channel capability V1_1 is supported and is enabled
[36m2019-01-29 08:13:42.052 UTC [fsblkstorage] Next -> DEBU 0cb[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 08:13:42.052 UTC [fsblkstorage] newBlockfileStream -> DEBU 0cc[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 08:13:42.053 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0cd[0m Remaining bytes=[18495], Going to peek [8] bytes
[36m2019-01-29 08:13:42.053 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0ce[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 08:13:42.053 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0cf[0m blockbytes [18492] read from file [0]
[36m2019-01-29 08:13:42.053 UTC [orderer/commmon/multichannel] newBlockWriter -> DEBU 0d0[0m [channel: testchainid] Creating block writer for tip of chain (blockNumber=0, lastConfigBlockNum=0, lastConfigSeq=0)
2019-01-29 08:13:42.053 UTC [orderer/consensus/kafka] newChain -> INFO 0d1[0m [channel: testchainid] Starting chain with last persisted offset -3 and last recorded block 0
[36m2019-01-29 08:13:42.053 UTC [orderer/commmon/multichannel] newChainSupport -> DEBU 0d2[0m [channel: testchainid] Done creating channel support resources
[36m2019-01-29 08:13:42.053 UTC [orderer/common/msgprocessor] NewSystemChannel -> DEBU 0d3[0m Creating system channel msg processor for channel testchainid
[36m2019-01-29 08:13:42.053 UTC [fsblkstorage] Next -> DEBU 0d4[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 08:13:42.053 UTC [fsblkstorage] newBlockfileStream -> DEBU 0d5[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 08:13:42.053 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0d6[0m Remaining bytes=[18495], Going to peek [8] bytes
[36m2019-01-29 08:13:42.053 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0d7[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 08:13:42.053 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0d8[0m blockbytes [18492] read from file [0]
2019-01-29 08:13:42.053 UTC [orderer/commmon/multichannel] NewRegistrar -> INFO 0d9[0m Starting system channel 'testchainid' with genesis block hash c84c51e50921c0294405e41d16cb6234273f63b37a240438cc9f32578277d7e3 and orderer type kafka
2019-01-29 08:13:42.053 UTC [orderer/common/server] Start -> INFO 0da[0m Starting orderer:
 Version: 1.2.0
 Commit SHA: cae2ad4
 Go version: go1.10
 OS/Arch: linux/amd64
 Experimental features: false
2019-01-29 08:13:42.053 UTC [orderer/common/server] Start -> INFO 0db[0m Beginning to serve requests
2019-01-29 08:13:42.053 UTC [orderer/consensus/kafka] setupProducerForChannel -> INFO 0dc[0m [channel: testchainid] Setting up the producer for this channel...
[36m2019-01-29 08:13:42.053 UTC [orderer/consensus/kafka] try -> DEBU 0dd[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 08:13:42.053 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 0de[0m Initializing new client
[36m2019-01-29 08:13:42.053 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 0df[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:13:42.053 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 0e0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:13:42.053 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0e1[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 08:13:42.055 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 0e2[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.149:11092: connect: connection refused
[36m2019-01-29 08:13:42.055 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0e3[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.149:11092: connect: connection refused
[36m2019-01-29 08:13:42.055 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 0e4[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:13:42.055 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0e5[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:13:42.056 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 0e6[0m Connected to broker at kafka0.switch2logic.co.za:9092 (unregistered)
[36m2019-01-29 08:13:42.068 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 0e7[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 08:13:42.068 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 0e8[0m client/brokers registered new broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 08:13:42.068 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 0e9[0m client/brokers registered new broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:13:42.068 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 0ea[0m Successfully initialized new client
[36m2019-01-29 08:13:42.068 UTC [orderer/consensus/kafka] try -> DEBU 0eb[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 08:13:42.068 UTC [orderer/consensus/kafka] startThread -> INFO 0ec[0m [channel: testchainid] Producer set up successfully
2019-01-29 08:13:42.068 UTC [orderer/consensus/kafka] sendConnectMessage -> INFO 0ed[0m [channel: testchainid] About to post the CONNECT message...
[36m2019-01-29 08:13:42.068 UTC [orderer/consensus/kafka] try -> DEBU 0ee[0m [channel: testchainid] Attempting to post the CONNECT message...
[36m2019-01-29 08:13:42.068 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0ef[0m client/metadata fetching metadata for [testchainid] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:13:42.133 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0f0[0m client/metadata found some partitions to be leaderless
[36m2019-01-29 08:13:42.133 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 0f1[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 08:13:42.384 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 0f2[0m client/metadata fetching metadata for [testchainid] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:13:42.409 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 0f3[0m client/metadata found some partitions to be leaderless
[36m2019-01-29 08:13:42.410 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 0f4[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 08:13:42.660 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 0f5[0m client/metadata fetching metadata for [testchainid] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:13:42.673 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 0f6[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:13:42.674 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU 0f7[0m producer/broker/1 starting up
[36m2019-01-29 08:13:42.674 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU 0f8[0m producer/broker/1 state change to [open] on testchainid/0
[36m2019-01-29 08:13:42.676 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 0f9[0m Connected to broker at kafka1.switch2logic.co.za:9092 (registered as #1)
[36m2019-01-29 08:13:42.915 UTC [orderer/consensus/kafka] try -> DEBU 0fa[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 08:13:42.916 UTC [orderer/consensus/kafka] startThread -> INFO 0fb[0m [channel: testchainid] CONNECT message posted successfully
2019-01-29 08:13:42.916 UTC [orderer/consensus/kafka] setupParentConsumerForChannel -> INFO 0fc[0m [channel: testchainid] Setting up the parent consumer for this channel...
[36m2019-01-29 08:13:42.916 UTC [orderer/consensus/kafka] try -> DEBU 0fd[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 08:13:42.916 UTC [orderer/consensus/kafka/sarama] NewConsumer -> DEBU 0fe[0m Initializing new client
[36m2019-01-29 08:13:42.916 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 0ff[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:13:42.916 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 100[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:13:42.916 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 101[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 08:13:42.919 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 102[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.150:10092: connect: connection refused
[36m2019-01-29 08:13:42.919 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 103[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.150:10092: connect: connection refused
[36m2019-01-29 08:13:42.919 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 104[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:13:42.919 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 105[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 08:13:42.920 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 106[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.149:11092: connect: connection refused
[36m2019-01-29 08:13:42.920 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 107[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.149:11092: connect: connection refused
[36m2019-01-29 08:13:42.920 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 108[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:13:42.920 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 109[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:13:42.920 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 10a[0m Connected to broker at kafka0.switch2logic.co.za:9092 (unregistered)
[36m2019-01-29 08:13:42.923 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 10b[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 08:13:42.923 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 10c[0m client/brokers registered new broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 08:13:42.923 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 10d[0m client/brokers registered new broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:13:42.923 UTC [orderer/consensus/kafka/sarama] NewConsumer -> DEBU 10e[0m Successfully initialized new client
[36m2019-01-29 08:13:42.923 UTC [orderer/consensus/kafka] try -> DEBU 10f[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 08:13:42.923 UTC [orderer/consensus/kafka] startThread -> INFO 110[0m [channel: testchainid] Parent consumer set up successfully
2019-01-29 08:13:42.923 UTC [orderer/consensus/kafka] setupChannelConsumerForChannel -> INFO 111[0m [channel: testchainid] Setting up the channel consumer for this channel (start offset: -2)...
[36m2019-01-29 08:13:42.923 UTC [orderer/consensus/kafka] try -> DEBU 112[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 08:13:42.924 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 113[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:13:42.924 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 114[0m Connected to broker at kafka1.switch2logic.co.za:9092 (registered as #1)
[36m2019-01-29 08:13:42.936 UTC [orderer/consensus/kafka/sarama] subscriptionConsumer -> DEBU 115[0m consumer/broker/1 added subscription to testchainid/0
[36m2019-01-29 08:13:42.936 UTC [orderer/consensus/kafka] try -> DEBU 116[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 08:13:42.936 UTC [orderer/consensus/kafka] startThread -> INFO 117[0m [channel: testchainid] Channel consumer set up successfully
2019-01-29 08:13:42.937 UTC [orderer/consensus/kafka] startThread -> INFO 118[0m [channel: testchainid] Start phase completed successfully
[36m2019-01-29 08:13:42.952 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 119[0m [channel: testchainid] Successfully unmarshalled consumed message, offset is 0. Inspecting type...
[36m2019-01-29 08:13:42.953 UTC [orderer/consensus/kafka] processConnect -> DEBU 11a[0m [channel: testchainid] It's a connect message - ignoring
2019-01-29 08:18:08.663 UTC [localconfig] completeInitialization -> INFO 001[0m Kafka.Version unset, setting to 0.10.2.0
[36m2019-01-29 08:18:08.663 UTC [bccsp_sw] openKeyStore -> DEBU 002[0m KeyStore opened at [/etc/hyperledger/fabric/orderer/msp/keystore]...done
[36m2019-01-29 08:18:08.663 UTC [bccsp] initBCCSP -> DEBU 003[0m Initialize BCCSP [SW]
[36m2019-01-29 08:18:08.663 UTC [msp] getPemMaterialFromDir -> DEBU 004[0m Reading directory /etc/hyperledger/fabric/orderer/msp/signcerts
[36m2019-01-29 08:18:08.663 UTC [msp] getPemMaterialFromDir -> DEBU 005[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/signcerts/orderer0.hospital1.switch2logic.co.za-cert.pem
[36m2019-01-29 08:18:08.663 UTC [msp] getPemMaterialFromDir -> DEBU 006[0m Reading directory /etc/hyperledger/fabric/orderer/msp/cacerts
[36m2019-01-29 08:18:08.664 UTC [msp] getPemMaterialFromDir -> DEBU 007[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/cacerts/ca.switch2logic.co.za-cert.pem
[36m2019-01-29 08:18:08.664 UTC [msp] getPemMaterialFromDir -> DEBU 008[0m Reading directory /etc/hyperledger/fabric/orderer/msp/admincerts
[36m2019-01-29 08:18:08.664 UTC [msp] getPemMaterialFromDir -> DEBU 009[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/admincerts/Admin@switch2logic.co.za-cert.pem
[36m2019-01-29 08:18:08.664 UTC [msp] getPemMaterialFromDir -> DEBU 00a[0m Reading directory /etc/hyperledger/fabric/orderer/msp/intermediatecerts
[36m2019-01-29 08:18:08.664 UTC [msp] getMspConfig -> DEBU 00b[0m Intermediate certs folder not found at [/etc/hyperledger/fabric/orderer/msp/intermediatecerts]. Skipping. [stat /etc/hyperledger/fabric/orderer/msp/intermediatecerts: no such file or directory]
[36m2019-01-29 08:18:08.664 UTC [msp] getPemMaterialFromDir -> DEBU 00c[0m Reading directory /etc/hyperledger/fabric/orderer/msp/tlscacerts
[36m2019-01-29 08:18:08.664 UTC [msp] getPemMaterialFromDir -> DEBU 00d[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/tlscacerts/tlsca.switch2logic.co.za-cert.pem
[36m2019-01-29 08:18:08.664 UTC [msp] getPemMaterialFromDir -> DEBU 00e[0m Reading directory /etc/hyperledger/fabric/orderer/msp/tlsintermediatecerts
[36m2019-01-29 08:18:08.664 UTC [msp] getMspConfig -> DEBU 00f[0m TLS intermediate certs folder not found at [/etc/hyperledger/fabric/orderer/msp/tlsintermediatecerts]. Skipping. [stat /etc/hyperledger/fabric/orderer/msp/tlsintermediatecerts: no such file or directory]
[36m2019-01-29 08:18:08.664 UTC [msp] getPemMaterialFromDir -> DEBU 010[0m Reading directory /etc/hyperledger/fabric/orderer/msp/crls
[36m2019-01-29 08:18:08.664 UTC [msp] getMspConfig -> DEBU 011[0m crls folder not found at [/etc/hyperledger/fabric/orderer/msp/crls]. Skipping. [stat /etc/hyperledger/fabric/orderer/msp/crls: no such file or directory]
[36m2019-01-29 08:18:08.664 UTC [msp] getMspConfig -> DEBU 012[0m MSP configuration file not found at [/etc/hyperledger/fabric/orderer/msp/config.yaml]: [stat /etc/hyperledger/fabric/orderer/msp/config.yaml: no such file or directory]
[36m2019-01-29 08:18:08.664 UTC [msp] newBccspMsp -> DEBU 013[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:18:08.664 UTC [msp] New -> DEBU 014[0m Creating Cache-MSP instance
[36m2019-01-29 08:18:08.664 UTC [msp] loadLocaMSP -> DEBU 015[0m Created new local MSP
[36m2019-01-29 08:18:08.664 UTC [msp] Setup -> DEBU 016[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 08:18:08.665 UTC [msp/identity] newIdentity -> DEBU 017[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 08:18:08.665 UTC [msp/identity] newIdentity -> DEBU 018[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 08:18:08.686 UTC [msp/identity] newIdentity -> DEBU 019[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICLDCCAdKgAwIBAgIQIoN1EApgEjYu/O7unXVqYzAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBqMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEuMCwGA1UEAxMlb3JkZXJlcjAuaG9zcGl0YWwxLnN3
aXRjaDJsb2dpYy5jby56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABH2/gH3f
8uhcO2CKglQg53cAJL79dENZNdRWfaRikU5j3X60lh9Bx5YpznQ5l0qMaCK1Ogec
Nw5LGgie9ESxrHyjTTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsG
A1UdIwQkMCKAIBiGNntD5tBlG9C8QNsdXde3ItTRRa+p/+3yJwRCsbJxMAoGCCqG
SM49BAMCA0gAMEUCIQCBtzCy6LE3S8b/XG08Qcs01A8QjmCwqmZJQrDvRA8ZhgIg
aAwP9tYtEZs1DBv2bH7HwYIwmi8nlgD3+Xp6CWwvAkg=
-----END CERTIFICATE-----
[36m2019-01-29 08:18:08.687 UTC [bccsp_sw] loadPrivateKey -> DEBU 01a[0m Loading private key [f9564620d77ba2b2650b4916b06387b38f212a58c63268d4c08f9b019eaa0f87] at [/etc/hyperledger/fabric/orderer/msp/keystore/f9564620d77ba2b2650b4916b06387b38f212a58c63268d4c08f9b019eaa0f87_sk]...
[36m2019-01-29 08:18:08.687 UTC [msp/identity] newIdentity -> DEBU 01b[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICLDCCAdKgAwIBAgIQIoN1EApgEjYu/O7unXVqYzAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBqMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEuMCwGA1UEAxMlb3JkZXJlcjAuaG9zcGl0YWwxLnN3
aXRjaDJsb2dpYy5jby56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABH2/gH3f
8uhcO2CKglQg53cAJL79dENZNdRWfaRikU5j3X60lh9Bx5YpznQ5l0qMaCK1Ogec
Nw5LGgie9ESxrHyjTTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsG
A1UdIwQkMCKAIBiGNntD5tBlG9C8QNsdXde3ItTRRa+p/+3yJwRCsbJxMAoGCCqG
SM49BAMCA0gAMEUCIQCBtzCy6LE3S8b/XG08Qcs01A8QjmCwqmZJQrDvRA8ZhgIg
aAwP9tYtEZs1DBv2bH7HwYIwmi8nlgD3+Xp6CWwvAkg=
-----END CERTIFICATE-----
[36m2019-01-29 08:18:08.687 UTC [msp] setupSigningIdentity -> DEBU 01c[0m Signing identity expires at 2029-01-26 07:41:54 +0000 UTC
[36m2019-01-29 08:18:08.687 UTC [msp] Validate -> DEBU 01d[0m MSP OrdererMSP validating identity
2019-01-29 08:18:08.688 UTC [orderer/common/server] prettyPrintStruct -> INFO 01e[0m Orderer config values:
	General.LedgerType = "file"
	General.ListenAddress = "0.0.0.0"
	General.ListenPort = 7050
	General.TLS.Enabled = true
	General.TLS.PrivateKey = "/etc/hyperledger/fabric/orderer/tls/server.key"
	General.TLS.Certificate = "/etc/hyperledger/fabric/orderer/tls/server.crt"
	General.TLS.RootCAs = [/etc/hyperledger/fabric/orderer/tls/ca.crt]
	General.TLS.ClientAuthRequired = false
	General.TLS.ClientRootCAs = []
	General.Keepalive.ServerMinInterval = 1m0s
	General.Keepalive.ServerInterval = 2h0m0s
	General.Keepalive.ServerTimeout = 20s
	General.GenesisMethod = "file"
	General.GenesisProfile = "SampleInsecureSolo"
	General.SystemChannel = "test-system-channel-name"
	General.GenesisFile = "/etc/hyperledger/fabric/orderer/channel-artifacts/genesis.block"
	General.Profile.Enabled = false
	General.Profile.Address = "0.0.0.0:6060"
	General.LogLevel = "debug"
	General.LogFormat = "%{color}%{time:2006-01-02 15:04:05.000 MST} [%{module}] %{shortfunc} -> %{level:.4s} %{id:03x}%{color:reset} %{message}"
	General.LocalMSPDir = "/etc/hyperledger/fabric/orderer/msp"
	General.LocalMSPID = "OrdererMSP"
	General.BCCSP.ProviderName = "SW"
	General.BCCSP.SwOpts.SecLevel = 256
	General.BCCSP.SwOpts.HashFamily = "SHA2"
	General.BCCSP.SwOpts.Ephemeral = false
	General.BCCSP.SwOpts.FileKeystore.KeyStorePath = "/etc/hyperledger/fabric/orderer/msp/keystore"
	General.BCCSP.SwOpts.DummyKeystore =
	General.BCCSP.PluginOpts =
	General.Authentication.TimeWindow = 15m0s
	FileLedger.Location = "/var/hyperledger/production/orderer"
	FileLedger.Prefix = "hyperledger-fabric-ordererledger"
	RAMLedger.HistorySize = 1000
	Kafka.Retry.ShortInterval = 1s
	Kafka.Retry.ShortTotal = 30s
	Kafka.Retry.LongInterval = 5m0s
	Kafka.Retry.LongTotal = 12h0m0s
	Kafka.Retry.NetworkTimeouts.DialTimeout = 10s
	Kafka.Retry.NetworkTimeouts.ReadTimeout = 10s
	Kafka.Retry.NetworkTimeouts.WriteTimeout = 10s
	Kafka.Retry.Metadata.RetryMax = 3
	Kafka.Retry.Metadata.RetryBackoff = 250ms
	Kafka.Retry.Producer.RetryMax = 3
	Kafka.Retry.Producer.RetryBackoff = 100ms
	Kafka.Retry.Consumer.RetryBackoff = 2s
	Kafka.Verbose = true
	Kafka.Version = 0.10.2.0
	Kafka.TLS.Enabled = false
	Kafka.TLS.PrivateKey = ""
	Kafka.TLS.Certificate = ""
	Kafka.TLS.RootCAs = []
	Kafka.TLS.ClientAuthRequired = false
	Kafka.TLS.ClientRootCAs = []
	Debug.BroadcastTraceDir = ""
	Debug.DeliverTraceDir = ""
2019-01-29 08:18:08.688 UTC [orderer/common/server] initializeServerConfig -> INFO 01f[0m Starting orderer with TLS enabled
[36m2019-01-29 08:18:08.688 UTC [orderer/common/server] createLedgerFactory -> DEBU 020[0m Ledger dir: /var/hyperledger/production/orderer
[36m2019-01-29 08:18:08.688 UTC [kvledger.util] CreateDirIfMissing -> DEBU 021[0m CreateDirIfMissing [/var/hyperledger/production/orderer/index/]
[36m2019-01-29 08:18:08.688 UTC [kvledger.util] logDirStatus -> DEBU 022[0m Before creating dir - [/var/hyperledger/production/orderer/index/] exists
[36m2019-01-29 08:18:08.688 UTC [kvledger.util] logDirStatus -> DEBU 023[0m After creating dir - [/var/hyperledger/production/orderer/index/] exists
[36m2019-01-29 08:18:08.700 UTC [orderer/common/server] createSubDir -> DEBU 024[0m Found chains sub-dir and using it
2019-01-29 08:18:08.700 UTC [orderer/common/server] initializeMultichannelRegistrar -> INFO 025[0m Not bootstrapping because of existing chains
[36m2019-01-29 08:18:08.700 UTC [fsblkstorage] newBlockfileMgr -> DEBU 026[0m newBlockfileMgr() initializing file-based block storage for ledger: testchainid 
[36m2019-01-29 08:18:08.700 UTC [kvledger.util] CreateDirIfMissing -> DEBU 027[0m CreateDirIfMissing [/var/hyperledger/production/orderer/chains/testchainid/]
[36m2019-01-29 08:18:08.700 UTC [kvledger.util] logDirStatus -> DEBU 028[0m Before creating dir - [/var/hyperledger/production/orderer/chains/testchainid/] exists
[36m2019-01-29 08:18:08.700 UTC [kvledger.util] logDirStatus -> DEBU 029[0m After creating dir - [/var/hyperledger/production/orderer/chains/testchainid/] exists
[36m2019-01-29 08:18:08.700 UTC [fsblkstorage] loadCurrentInfo -> DEBU 02a[0m loaded checkpointInfo:latestFileChunkSuffixNum=[0], latestFileChunksize=[18495], isChainEmpty=[false], lastBlockNumber=[0]
[36m2019-01-29 08:18:08.701 UTC [fsblkstorage] newBlockfileMgr -> DEBU 02b[0m Synching block information from block storage (if needed)
[36m2019-01-29 08:18:08.701 UTC [fsblkstorage] syncCPInfoFromFS -> DEBU 02c[0m Starting checkpoint=latestFileChunkSuffixNum=[0], latestFileChunksize=[18495], isChainEmpty=[false], lastBlockNumber=[0]
[36m2019-01-29 08:18:08.701 UTC [fsblkstorage] syncCPInfoFromFS -> DEBU 02d[0m status of file [/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000]: exists=[true], size=[18495]
[36m2019-01-29 08:18:08.703 UTC [fsblkstorage] newBlockIndex -> DEBU 02e[0m newBlockIndex() - indexItems:[[BlockNum]]
[36m2019-01-29 08:18:08.703 UTC [fsblkstorage] syncIndex -> DEBU 02f[0m Both the block files and indices are in sync.
[36m2019-01-29 08:18:08.703 UTC [fsblkstorage] retrieveBlockHeaderByNumber -> DEBU 030[0m retrieveBlockHeaderByNumber() - blockNum = [0]
[36m2019-01-29 08:18:08.703 UTC [fsblkstorage] newBlockfileStream -> DEBU 031[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 08:18:08.703 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 032[0m Remaining bytes=[18495], Going to peek [8] bytes
[36m2019-01-29 08:18:08.703 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 033[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 08:18:08.703 UTC [fsblkstorage] Next -> DEBU 034[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 08:18:08.703 UTC [fsblkstorage] newBlockfileStream -> DEBU 035[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 08:18:08.703 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 036[0m Remaining bytes=[18495], Going to peek [8] bytes
[36m2019-01-29 08:18:08.703 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 037[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 08:18:08.703 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 038[0m blockbytes [18492] read from file [0]
[36m2019-01-29 08:18:08.703 UTC [fsblkstorage] Next -> DEBU 039[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 08:18:08.703 UTC [fsblkstorage] newBlockfileStream -> DEBU 03a[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 08:18:08.703 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 03b[0m Remaining bytes=[18495], Going to peek [8] bytes
[36m2019-01-29 08:18:08.703 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 03c[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 08:18:08.703 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 03d[0m blockbytes [18492] read from file [0]
[36m2019-01-29 08:18:08.704 UTC [common/channelconfig] NewStandardValues -> DEBU 03e[0m Initializing protos for *channelconfig.ChannelProtos
[36m2019-01-29 08:18:08.704 UTC [common/channelconfig] initializeProtosStruct -> DEBU 03f[0m Processing field: HashingAlgorithm
[36m2019-01-29 08:18:08.704 UTC [common/channelconfig] initializeProtosStruct -> DEBU 040[0m Processing field: BlockDataHashingStructure
[36m2019-01-29 08:18:08.704 UTC [common/channelconfig] initializeProtosStruct -> DEBU 041[0m Processing field: OrdererAddresses
[36m2019-01-29 08:18:08.704 UTC [common/channelconfig] initializeProtosStruct -> DEBU 042[0m Processing field: Consortium
[36m2019-01-29 08:18:08.704 UTC [common/channelconfig] initializeProtosStruct -> DEBU 043[0m Processing field: Capabilities
[36m2019-01-29 08:18:08.704 UTC [common/channelconfig] NewStandardValues -> DEBU 044[0m Initializing protos for *channelconfig.ConsortiumProtos
[36m2019-01-29 08:18:08.704 UTC [common/channelconfig] initializeProtosStruct -> DEBU 045[0m Processing field: ChannelCreationPolicy
[36m2019-01-29 08:18:08.704 UTC [common/channelconfig] NewStandardValues -> DEBU 046[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:18:08.704 UTC [common/channelconfig] initializeProtosStruct -> DEBU 047[0m Processing field: MSP
[36m2019-01-29 08:18:08.704 UTC [common/channelconfig] validateMSP -> DEBU 048[0m Setting up MSP for org Hospital1MSP
[36m2019-01-29 08:18:08.704 UTC [msp] newBccspMsp -> DEBU 049[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:18:08.704 UTC [msp] New -> DEBU 04a[0m Creating Cache-MSP instance
[36m2019-01-29 08:18:08.704 UTC [msp] Setup -> DEBU 04b[0m Setting up MSP instance Hospital1MSP
[36m2019-01-29 08:18:08.704 UTC [msp/identity] newIdentity -> DEBU 04c[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdDCCAhugAwIBAgIQZxH8VsnceZxXgKxlURLgVTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMS5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQX9V4k/h/3pFxByKDscjgQrC0YS/bSbKvzSG2Z2PvltEWp40qB9OaKUN0GQYzo
yK2Etdvyjfd1OFwaTjpGSvv9o18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCBVU7DoP9Msf/nNTNl6
5FxwAXZ82eaGBT7zWYdyh4QbBjAKBggqhkjOPQQDAgNHADBEAiBmXGWQbxCGRp0c
5/y9qJyciB1CDHsknRZDjNzt5f563wIgMy7VPi1R8DpIIIRN9B3mfhDpRa+J+8PA
S4DOJy7rjp4=
-----END CERTIFICATE-----
[36m2019-01-29 08:18:08.705 UTC [msp/identity] newIdentity -> DEBU 04d[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 08:18:08.705 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 04e[0m Checking if identity satisfies role [CLIENT] for Hospital1MSP
[36m2019-01-29 08:18:08.705 UTC [msp] Validate -> DEBU 04f[0m MSP Hospital1MSP validating identity
[36m2019-01-29 08:18:08.705 UTC [msp] getCertificationChain -> DEBU 050[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 08:18:08.706 UTC [msp] hasOURole -> DEBU 051[0m MSP Hospital1MSP checking if the identity is a client
[36m2019-01-29 08:18:08.706 UTC [msp] getCertificationChain -> DEBU 052[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 08:18:08.706 UTC [common/channelconfig] NewStandardValues -> DEBU 053[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:18:08.706 UTC [common/channelconfig] initializeProtosStruct -> DEBU 054[0m Processing field: MSP
[36m2019-01-29 08:18:08.706 UTC [common/channelconfig] validateMSP -> DEBU 055[0m Setting up MSP for org Hospital2MSP
[36m2019-01-29 08:18:08.706 UTC [msp] newBccspMsp -> DEBU 056[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:18:08.706 UTC [msp] New -> DEBU 057[0m Creating Cache-MSP instance
[36m2019-01-29 08:18:08.706 UTC [msp] Setup -> DEBU 058[0m Setting up MSP instance Hospital2MSP
[36m2019-01-29 08:18:08.706 UTC [msp/identity] newIdentity -> DEBU 059[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdjCCAhygAwIBAgIRALZ3Pffn/xWUtUWPSK6CYVAwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowgYsxCzAJBgNVBAYTAlVTMRMwEQYD
VQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4gRnJhbmNpc2NvMSUwIwYDVQQK
Exxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMSgwJgYDVQQDEx9jYS5ob3Nw
aXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcD
QgAESzVpiz6P4mIb+KwvrttcXJKZR5i137Is4298E7OrkYdAg0tB4rpfKTXzUMwW
vI12cTDujpyaZSiqQ3/zP4fOQqNfMF0wDgYDVR0PAQH/BAQDAgGmMA8GA1UdJQQI
MAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQg8QJqWKPaX24HINwR
S6sINFSwRD+440/ccSnvFVaGF24wCgYIKoZIzj0EAwIDSAAwRQIhAPO2Nng3/nqb
cKPliUqVPVjBMu+D55M90+jexWSOpkjcAiADPx8qv9exZ4BfP3+G49Qzpx0Meblj
53vzUgYFa/0EXw==
-----END CERTIFICATE-----
[36m2019-01-29 08:18:08.706 UTC [msp/identity] newIdentity -> DEBU 05a[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAJ6XtHMTTW1JzdpD54qCc3wwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA+z1rb9aA1HwHfNLi9V9VZe
bdwT4MXak445zGz4DXzRUsmNeBngzntKb6qpgHQpeFtzbzLx+zwiynRDvfdzORGj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPEC
alij2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQC
IA8A3CCJeCasyy7gYd/M1aHYmwWsecY7UY7cVUGy4XOxAiB0rrmfnGW6JA11PDCu
+5c5Rpn0jF5ddsFn+UIpzBFvmg==
-----END CERTIFICATE-----
[36m2019-01-29 08:18:08.707 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 05b[0m Checking if identity satisfies role [CLIENT] for Hospital2MSP
[36m2019-01-29 08:18:08.707 UTC [msp] Validate -> DEBU 05c[0m MSP Hospital2MSP validating identity
[36m2019-01-29 08:18:08.707 UTC [msp] getCertificationChain -> DEBU 05d[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 08:18:08.707 UTC [msp] hasOURole -> DEBU 05e[0m MSP Hospital2MSP checking if the identity is a client
[36m2019-01-29 08:18:08.707 UTC [msp] getCertificationChain -> DEBU 05f[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 08:18:08.707 UTC [common/channelconfig] NewStandardValues -> DEBU 060[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:18:08.707 UTC [common/channelconfig] initializeProtosStruct -> DEBU 061[0m Processing field: MSP
[36m2019-01-29 08:18:08.707 UTC [common/channelconfig] validateMSP -> DEBU 062[0m Setting up MSP for org Hospital3MSP
[36m2019-01-29 08:18:08.707 UTC [msp] newBccspMsp -> DEBU 063[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:18:08.707 UTC [msp] New -> DEBU 064[0m Creating Cache-MSP instance
[36m2019-01-29 08:18:08.707 UTC [msp] Setup -> DEBU 065[0m Setting up MSP instance Hospital3MSP
[36m2019-01-29 08:18:08.708 UTC [msp/identity] newIdentity -> DEBU 066[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdTCCAhugAwIBAgIQCCXeHGgObIJo8aq4AdvgVjAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMy5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQm1Oo/AEu7ZZToE15Y5ntlaN128Hs/exX3jNH5nO6r4tvyFtBBNEn1ecXxxPtm
dmQ+nPHONorLwDC42SMAKwfro18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCA7vVWFXWpgZBojQ1ZL
xUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEAlOUoPtpfO8V7
vFRCanjU5zZWGO5ajFlPDJjjDTy3UrwCICrUYV32JrR/m3dslCl15dSThHrKtYCC
Ehvyw1+lNqiG
-----END CERTIFICATE-----
[36m2019-01-29 08:18:08.708 UTC [msp/identity] newIdentity -> DEBU 067[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAI8BA/5Jf+5m8ncI+D7aSiMwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwzLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABF1IehIKIjh8tXYZ7rw4QOuc
g/tpBAkJeOt44zr/LepbjhyFW8vIRnkw75i6Wcg+LOoAS+2NFNg2oNjqg//yCHWj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIDu9
VYVdamBkGiNDVkvFS+AEt6Jrn0yK8hFyPGZhqenJMAoGCCqGSM49BAMCA0cAMEQC
IFyc85pwbCAxGNdw4fflsX2I3HrP3h93YAB2zSDC8HtkAiAQxblAZfEnXUiQKrqE
8I10bYJqD6pJz1jb3Pv+4xplTA==
-----END CERTIFICATE-----
[36m2019-01-29 08:18:08.708 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 068[0m Checking if identity satisfies role [CLIENT] for Hospital3MSP
[36m2019-01-29 08:18:08.708 UTC [msp] Validate -> DEBU 069[0m MSP Hospital3MSP validating identity
[36m2019-01-29 08:18:08.709 UTC [msp] getCertificationChain -> DEBU 06a[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 08:18:08.709 UTC [msp] hasOURole -> DEBU 06b[0m MSP Hospital3MSP checking if the identity is a client
[36m2019-01-29 08:18:08.709 UTC [msp] getCertificationChain -> DEBU 06c[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 08:18:08.709 UTC [common/channelconfig] NewStandardValues -> DEBU 06d[0m Initializing protos for *channelconfig.OrdererProtos
[36m2019-01-29 08:18:08.709 UTC [common/channelconfig] initializeProtosStruct -> DEBU 06e[0m Processing field: ConsensusType
[36m2019-01-29 08:18:08.709 UTC [common/channelconfig] initializeProtosStruct -> DEBU 06f[0m Processing field: BatchSize
[36m2019-01-29 08:18:08.709 UTC [common/channelconfig] initializeProtosStruct -> DEBU 070[0m Processing field: BatchTimeout
[36m2019-01-29 08:18:08.709 UTC [common/channelconfig] initializeProtosStruct -> DEBU 071[0m Processing field: KafkaBrokers
[36m2019-01-29 08:18:08.709 UTC [common/channelconfig] initializeProtosStruct -> DEBU 072[0m Processing field: ChannelRestrictions
[36m2019-01-29 08:18:08.709 UTC [common/channelconfig] initializeProtosStruct -> DEBU 073[0m Processing field: Capabilities
[36m2019-01-29 08:18:08.709 UTC [common/channelconfig] NewStandardValues -> DEBU 074[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:18:08.709 UTC [common/channelconfig] initializeProtosStruct -> DEBU 075[0m Processing field: MSP
[36m2019-01-29 08:18:08.709 UTC [common/channelconfig] validateMSP -> DEBU 076[0m Setting up MSP for org OrdererOrg
[36m2019-01-29 08:18:08.709 UTC [msp] newBccspMsp -> DEBU 077[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:18:08.709 UTC [msp] New -> DEBU 078[0m Creating Cache-MSP instance
[36m2019-01-29 08:18:08.709 UTC [msp] Setup -> DEBU 079[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 08:18:08.710 UTC [msp/identity] newIdentity -> DEBU 07a[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 08:18:08.710 UTC [msp/identity] newIdentity -> DEBU 07b[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 08:18:08.710 UTC [msp] Validate -> DEBU 07c[0m MSP OrdererMSP validating identity
[36m2019-01-29 08:18:08.710 UTC [msp] Setup -> DEBU 07d[0m Setting up the MSP manager (4 msps)
[36m2019-01-29 08:18:08.710 UTC [msp] Setup -> DEBU 07e[0m MSP manager setup complete, setup 4 msps
[36m2019-01-29 08:18:08.710 UTC [policies] NewManagerImpl -> DEBU 07f[0m Proposed new policy Admins for Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 08:18:08.710 UTC [policies] NewManagerImpl -> DEBU 080[0m Proposed new policy Readers for Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 08:18:08.710 UTC [policies] NewManagerImpl -> DEBU 081[0m Proposed new policy Writers for Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 08:18:08.710 UTC [policies] NewManagerImpl -> DEBU 082[0m Proposed new policy Writers for Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 08:18:08.710 UTC [policies] NewManagerImpl -> DEBU 083[0m Proposed new policy Admins for Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 08:18:08.711 UTC [policies] NewManagerImpl -> DEBU 084[0m Proposed new policy Readers for Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 08:18:08.711 UTC [policies] NewManagerImpl -> DEBU 085[0m Proposed new policy Admins for Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 08:18:08.711 UTC [policies] NewManagerImpl -> DEBU 086[0m Proposed new policy Readers for Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 08:18:08.711 UTC [policies] NewManagerImpl -> DEBU 087[0m Proposed new policy Writers for Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 08:18:08.711 UTC [policies] NewManagerImpl -> DEBU 088[0m Proposed new policy Admins for Channel/Consortiums
[36m2019-01-29 08:18:08.711 UTC [policies] NewManagerImpl -> DEBU 089[0m Proposed new policy Writers for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:18:08.711 UTC [policies] NewManagerImpl -> DEBU 08a[0m Proposed new policy Admins for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:18:08.711 UTC [policies] NewManagerImpl -> DEBU 08b[0m Proposed new policy Readers for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:18:08.711 UTC [policies] NewManagerImpl -> DEBU 08c[0m Proposed new policy Admins for Channel/Orderer
[36m2019-01-29 08:18:08.711 UTC [policies] NewManagerImpl -> DEBU 08d[0m Proposed new policy Readers for Channel/Orderer
[36m2019-01-29 08:18:08.711 UTC [policies] NewManagerImpl -> DEBU 08e[0m Proposed new policy Writers for Channel/Orderer
[36m2019-01-29 08:18:08.711 UTC [policies] NewManagerImpl -> DEBU 08f[0m Proposed new policy BlockValidation for Channel/Orderer
[36m2019-01-29 08:18:08.711 UTC [policies] NewManagerImpl -> DEBU 090[0m Proposed new policy Admins for Channel
[36m2019-01-29 08:18:08.711 UTC [policies] GetPolicy -> DEBU 091[0m Returning dummy reject all policy because Readers could not be found in Channel/Consortiums/Readers
[36m2019-01-29 08:18:08.711 UTC [policies] NewManagerImpl -> DEBU 092[0m Proposed new policy Readers for Channel
[36m2019-01-29 08:18:08.711 UTC [policies] GetPolicy -> DEBU 093[0m Returning dummy reject all policy because Writers could not be found in Channel/Consortiums/Writers
[36m2019-01-29 08:18:08.711 UTC [policies] NewManagerImpl -> DEBU 094[0m Proposed new policy Writers for Channel
[36m2019-01-29 08:18:08.711 UTC [common/configtx] addToMap -> DEBU 095[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 08:18:08.711 UTC [common/configtx] addToMap -> DEBU 096[0m Adding to config map: [Group]  /Channel/Consortiums
[36m2019-01-29 08:18:08.711 UTC [common/configtx] addToMap -> DEBU 097[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium
[36m2019-01-29 08:18:08.711 UTC [common/configtx] addToMap -> DEBU 098[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 08:18:08.711 UTC [common/configtx] addToMap -> DEBU 099[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/Hospital1MSP/MSP
[36m2019-01-29 08:18:08.711 UTC [common/configtx] addToMap -> DEBU 09a[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital1MSP/Readers
[36m2019-01-29 08:18:08.711 UTC [common/configtx] addToMap -> DEBU 09b[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital1MSP/Writers
[36m2019-01-29 08:18:08.711 UTC [common/configtx] addToMap -> DEBU 09c[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital1MSP/Admins
[36m2019-01-29 08:18:08.711 UTC [common/configtx] addToMap -> DEBU 09d[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 08:18:08.711 UTC [common/configtx] addToMap -> DEBU 09e[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/Hospital2MSP/MSP
[36m2019-01-29 08:18:08.711 UTC [common/configtx] addToMap -> DEBU 09f[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital2MSP/Admins
[36m2019-01-29 08:18:08.711 UTC [common/configtx] addToMap -> DEBU 0a0[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital2MSP/Readers
[36m2019-01-29 08:18:08.711 UTC [common/configtx] addToMap -> DEBU 0a1[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital2MSP/Writers
[36m2019-01-29 08:18:08.711 UTC [common/configtx] addToMap -> DEBU 0a2[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 08:18:08.711 UTC [common/configtx] addToMap -> DEBU 0a3[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/Hospital3MSP/MSP
[36m2019-01-29 08:18:08.711 UTC [common/configtx] addToMap -> DEBU 0a4[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital3MSP/Admins
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0a5[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital3MSP/Readers
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0a6[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital3MSP/Writers
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0a7[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/ChannelCreationPolicy
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0a8[0m Adding to config map: [Policy] /Channel/Consortiums/Admins
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0a9[0m Adding to config map: [Group]  /Channel/Orderer
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0aa[0m Adding to config map: [Group]  /Channel/Orderer/OrdererOrg
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0ab[0m Adding to config map: [Value]  /Channel/Orderer/OrdererOrg/MSP
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0ac[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0ad[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0ae[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Admins
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0af[0m Adding to config map: [Value]  /Channel/Orderer/KafkaBrokers
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0b0[0m Adding to config map: [Value]  /Channel/Orderer/ConsensusType
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0b1[0m Adding to config map: [Value]  /Channel/Orderer/BatchSize
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0b2[0m Adding to config map: [Value]  /Channel/Orderer/BatchTimeout
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0b3[0m Adding to config map: [Value]  /Channel/Orderer/ChannelRestrictions
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0b4[0m Adding to config map: [Value]  /Channel/Orderer/Capabilities
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0b5[0m Adding to config map: [Policy] /Channel/Orderer/Admins
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0b6[0m Adding to config map: [Policy] /Channel/Orderer/Readers
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0b7[0m Adding to config map: [Policy] /Channel/Orderer/Writers
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0b8[0m Adding to config map: [Policy] /Channel/Orderer/BlockValidation
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0b9[0m Adding to config map: [Value]  /Channel/HashingAlgorithm
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0ba[0m Adding to config map: [Value]  /Channel/BlockDataHashingStructure
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0bb[0m Adding to config map: [Value]  /Channel/OrdererAddresses
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0bc[0m Adding to config map: [Value]  /Channel/Capabilities
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0bd[0m Adding to config map: [Policy] /Channel/Admins
[36m2019-01-29 08:18:08.712 UTC [common/configtx] addToMap -> DEBU 0be[0m Adding to config map: [Policy] /Channel/Readers
[36m2019-01-29 08:18:08.713 UTC [common/configtx] addToMap -> DEBU 0bf[0m Adding to config map: [Policy] /Channel/Writers
[36m2019-01-29 08:18:08.713 UTC [common/channelconfig] LogSanityChecks -> DEBU 0c0[0m As expected, current configuration has policy '/Channel/Readers'
[36m2019-01-29 08:18:08.713 UTC [common/channelconfig] LogSanityChecks -> DEBU 0c1[0m As expected, current configuration has policy '/Channel/Writers'
[36m2019-01-29 08:18:08.713 UTC [policies] Manager -> DEBU 0c2[0m Manager Channel looking up path [Application]
[36m2019-01-29 08:18:08.713 UTC [policies] Manager -> DEBU 0c3[0m Manager Channel has managers Consortiums
[36m2019-01-29 08:18:08.713 UTC [policies] Manager -> DEBU 0c4[0m Manager Channel has managers Orderer
[36m2019-01-29 08:18:08.713 UTC [policies] Manager -> DEBU 0c5[0m Manager Channel looking up path [Orderer]
[36m2019-01-29 08:18:08.713 UTC [policies] Manager -> DEBU 0c6[0m Manager Channel has managers Consortiums
[36m2019-01-29 08:18:08.713 UTC [policies] Manager -> DEBU 0c7[0m Manager Channel has managers Orderer
[36m2019-01-29 08:18:08.713 UTC [policies] Manager -> DEBU 0c8[0m Manager Channel/Orderer looking up path []
[36m2019-01-29 08:18:08.713 UTC [policies] Manager -> DEBU 0c9[0m Manager Channel/Orderer has managers OrdererOrg
[36m2019-01-29 08:18:08.713 UTC [common/channelconfig] LogSanityChecks -> DEBU 0ca[0m As expected, current configuration has policy '/Channel/Orderer/BlockValidation'
[36m2019-01-29 08:18:08.713 UTC [common/capabilities] Supported -> DEBU 0cb[0m Orderer capability V1_1 is supported and is enabled
[36m2019-01-29 08:18:08.713 UTC [common/capabilities] Supported -> DEBU 0cc[0m Channel capability V1_1 is supported and is enabled
[36m2019-01-29 08:18:08.713 UTC [fsblkstorage] Next -> DEBU 0cd[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 08:18:08.713 UTC [fsblkstorage] newBlockfileStream -> DEBU 0ce[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 08:18:08.713 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0cf[0m Remaining bytes=[18495], Going to peek [8] bytes
[36m2019-01-29 08:18:08.713 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0d0[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 08:18:08.713 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0d1[0m blockbytes [18492] read from file [0]
[36m2019-01-29 08:18:08.713 UTC [orderer/commmon/multichannel] newBlockWriter -> DEBU 0d2[0m [channel: testchainid] Creating block writer for tip of chain (blockNumber=0, lastConfigBlockNum=0, lastConfigSeq=0)
2019-01-29 08:18:08.713 UTC [orderer/consensus/kafka] newChain -> INFO 0d3[0m [channel: testchainid] Starting chain with last persisted offset -3 and last recorded block 0
[36m2019-01-29 08:18:08.713 UTC [orderer/commmon/multichannel] newChainSupport -> DEBU 0d4[0m [channel: testchainid] Done creating channel support resources
[36m2019-01-29 08:18:08.713 UTC [orderer/common/msgprocessor] NewSystemChannel -> DEBU 0d5[0m Creating system channel msg processor for channel testchainid
[36m2019-01-29 08:18:08.713 UTC [fsblkstorage] Next -> DEBU 0d6[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 08:18:08.713 UTC [fsblkstorage] newBlockfileStream -> DEBU 0d7[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 08:18:08.713 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0d8[0m Remaining bytes=[18495], Going to peek [8] bytes
[36m2019-01-29 08:18:08.713 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0d9[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 08:18:08.713 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0da[0m blockbytes [18492] read from file [0]
2019-01-29 08:18:08.713 UTC [orderer/commmon/multichannel] NewRegistrar -> INFO 0db[0m Starting system channel 'testchainid' with genesis block hash c84c51e50921c0294405e41d16cb6234273f63b37a240438cc9f32578277d7e3 and orderer type kafka
2019-01-29 08:18:08.713 UTC [orderer/common/server] Start -> INFO 0dc[0m Starting orderer:
 Version: 1.2.0
 Commit SHA: cae2ad4
 Go version: go1.10
 OS/Arch: linux/amd64
 Experimental features: false
2019-01-29 08:18:08.713 UTC [orderer/common/server] Start -> INFO 0dd[0m Beginning to serve requests
2019-01-29 08:18:08.714 UTC [orderer/consensus/kafka] setupProducerForChannel -> INFO 0de[0m [channel: testchainid] Setting up the producer for this channel...
[36m2019-01-29 08:18:08.714 UTC [orderer/consensus/kafka] try -> DEBU 0df[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 08:18:08.714 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 0e0[0m Initializing new client
[36m2019-01-29 08:18:08.714 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 0e1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:18:08.714 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 0e2[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:18:08.714 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0e3[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 08:18:08.715 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 0e4[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.149:11092: connect: connection refused
[36m2019-01-29 08:18:08.715 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0e5[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.149:11092: connect: connection refused
[36m2019-01-29 08:18:08.715 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 0e6[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:18:08.715 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0e7[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 08:18:08.716 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 0e8[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.150:10092: connect: connection refused
[36m2019-01-29 08:18:08.716 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0e9[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.150:10092: connect: connection refused
[36m2019-01-29 08:18:08.716 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 0ea[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:18:08.716 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0eb[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:18:08.717 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 0ec[0m Connected to broker at kafka0.switch2logic.co.za:9092 (unregistered)
[36m2019-01-29 08:18:08.719 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 0ed[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 08:18:08.719 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 0ee[0m client/brokers registered new broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 08:18:08.719 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 0ef[0m client/brokers registered new broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:18:08.719 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 0f0[0m Successfully initialized new client
[36m2019-01-29 08:18:08.719 UTC [orderer/consensus/kafka] try -> DEBU 0f1[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 08:18:08.719 UTC [orderer/consensus/kafka] startThread -> INFO 0f2[0m [channel: testchainid] Producer set up successfully
2019-01-29 08:18:08.719 UTC [orderer/consensus/kafka] sendConnectMessage -> INFO 0f3[0m [channel: testchainid] About to post the CONNECT message...
[36m2019-01-29 08:18:08.719 UTC [orderer/consensus/kafka] try -> DEBU 0f4[0m [channel: testchainid] Attempting to post the CONNECT message...
[36m2019-01-29 08:18:08.719 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 0f5[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:18:08.719 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU 0f6[0m producer/broker/1 starting up
[36m2019-01-29 08:18:08.719 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU 0f7[0m producer/broker/1 state change to [open] on testchainid/0
[36m2019-01-29 08:18:08.720 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 0f8[0m Connected to broker at kafka1.switch2logic.co.za:9092 (registered as #1)
[36m2019-01-29 08:18:08.726 UTC [orderer/consensus/kafka] try -> DEBU 0f9[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 08:18:08.726 UTC [orderer/consensus/kafka] startThread -> INFO 0fa[0m [channel: testchainid] CONNECT message posted successfully
2019-01-29 08:18:08.726 UTC [orderer/consensus/kafka] setupParentConsumerForChannel -> INFO 0fb[0m [channel: testchainid] Setting up the parent consumer for this channel...
[36m2019-01-29 08:18:08.726 UTC [orderer/consensus/kafka] try -> DEBU 0fc[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 08:18:08.726 UTC [orderer/consensus/kafka/sarama] NewConsumer -> DEBU 0fd[0m Initializing new client
[36m2019-01-29 08:18:08.726 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 0fe[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:18:08.726 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 0ff[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:18:08.726 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 100[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 08:18:08.727 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 101[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.150:10092: connect: connection refused
[36m2019-01-29 08:18:08.727 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 102[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.150:10092: connect: connection refused
[36m2019-01-29 08:18:08.727 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 103[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:18:08.727 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 104[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 08:18:08.727 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 105[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.149:11092: connect: connection refused
[36m2019-01-29 08:18:08.727 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 106[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.149:11092: connect: connection refused
[36m2019-01-29 08:18:08.727 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 107[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:18:08.727 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 108[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:18:08.728 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 109[0m Connected to broker at kafka0.switch2logic.co.za:9092 (unregistered)
[36m2019-01-29 08:18:08.730 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 10a[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 08:18:08.730 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 10b[0m client/brokers registered new broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 08:18:08.730 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 10c[0m client/brokers registered new broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:18:08.730 UTC [orderer/consensus/kafka/sarama] NewConsumer -> DEBU 10d[0m Successfully initialized new client
[36m2019-01-29 08:18:08.730 UTC [orderer/consensus/kafka] try -> DEBU 10e[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 08:18:08.730 UTC [orderer/consensus/kafka] startThread -> INFO 10f[0m [channel: testchainid] Parent consumer set up successfully
2019-01-29 08:18:08.730 UTC [orderer/consensus/kafka] setupChannelConsumerForChannel -> INFO 110[0m [channel: testchainid] Setting up the channel consumer for this channel (start offset: -2)...
[36m2019-01-29 08:18:08.730 UTC [orderer/consensus/kafka] try -> DEBU 111[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 08:18:08.730 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 112[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:18:08.731 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 113[0m Connected to broker at kafka1.switch2logic.co.za:9092 (registered as #1)
[36m2019-01-29 08:18:08.732 UTC [orderer/consensus/kafka/sarama] subscriptionConsumer -> DEBU 114[0m consumer/broker/1 added subscription to testchainid/0
[36m2019-01-29 08:18:08.732 UTC [orderer/consensus/kafka] try -> DEBU 115[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 08:18:08.732 UTC [orderer/consensus/kafka] startThread -> INFO 116[0m [channel: testchainid] Channel consumer set up successfully
2019-01-29 08:18:08.733 UTC [orderer/consensus/kafka] startThread -> INFO 117[0m [channel: testchainid] Start phase completed successfully
[36m2019-01-29 08:18:08.734 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 118[0m [channel: testchainid] Successfully unmarshalled consumed message, offset is 0. Inspecting type...
[36m2019-01-29 08:18:08.734 UTC [orderer/consensus/kafka] processConnect -> DEBU 119[0m [channel: testchainid] It's a connect message - ignoring
[36m2019-01-29 08:18:08.734 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 11a[0m [channel: testchainid] Successfully unmarshalled consumed message, offset is 1. Inspecting type...
[36m2019-01-29 08:18:08.734 UTC [orderer/consensus/kafka] processConnect -> DEBU 11b[0m [channel: testchainid] It's a connect message - ignoring
2019-01-29 08:19:54.888 UTC [localconfig] completeInitialization -> INFO 001[0m Kafka.Version unset, setting to 0.10.2.0
[36m2019-01-29 08:19:54.889 UTC [bccsp_sw] openKeyStore -> DEBU 002[0m KeyStore opened at [/etc/hyperledger/fabric/orderer/msp/keystore]...done
[36m2019-01-29 08:19:54.889 UTC [bccsp] initBCCSP -> DEBU 003[0m Initialize BCCSP [SW]
[36m2019-01-29 08:19:54.889 UTC [msp] getPemMaterialFromDir -> DEBU 004[0m Reading directory /etc/hyperledger/fabric/orderer/msp/signcerts
[36m2019-01-29 08:19:54.889 UTC [msp] getPemMaterialFromDir -> DEBU 005[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/signcerts/orderer0.hospital1.switch2logic.co.za-cert.pem
[36m2019-01-29 08:19:54.889 UTC [msp] getPemMaterialFromDir -> DEBU 006[0m Reading directory /etc/hyperledger/fabric/orderer/msp/cacerts
[36m2019-01-29 08:19:54.889 UTC [msp] getPemMaterialFromDir -> DEBU 007[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/cacerts/ca.switch2logic.co.za-cert.pem
[36m2019-01-29 08:19:54.889 UTC [msp] getPemMaterialFromDir -> DEBU 008[0m Reading directory /etc/hyperledger/fabric/orderer/msp/admincerts
[36m2019-01-29 08:19:54.889 UTC [msp] getPemMaterialFromDir -> DEBU 009[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/admincerts/Admin@switch2logic.co.za-cert.pem
[36m2019-01-29 08:19:54.889 UTC [msp] getPemMaterialFromDir -> DEBU 00a[0m Reading directory /etc/hyperledger/fabric/orderer/msp/intermediatecerts
[36m2019-01-29 08:19:54.889 UTC [msp] getMspConfig -> DEBU 00b[0m Intermediate certs folder not found at [/etc/hyperledger/fabric/orderer/msp/intermediatecerts]. Skipping. [stat /etc/hyperledger/fabric/orderer/msp/intermediatecerts: no such file or directory]
[36m2019-01-29 08:19:54.889 UTC [msp] getPemMaterialFromDir -> DEBU 00c[0m Reading directory /etc/hyperledger/fabric/orderer/msp/tlscacerts
[36m2019-01-29 08:19:54.889 UTC [msp] getPemMaterialFromDir -> DEBU 00d[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/tlscacerts/tlsca.switch2logic.co.za-cert.pem
[36m2019-01-29 08:19:54.889 UTC [msp] getPemMaterialFromDir -> DEBU 00e[0m Reading directory /etc/hyperledger/fabric/orderer/msp/tlsintermediatecerts
[36m2019-01-29 08:19:54.889 UTC [msp] getMspConfig -> DEBU 00f[0m TLS intermediate certs folder not found at [/etc/hyperledger/fabric/orderer/msp/tlsintermediatecerts]. Skipping. [stat /etc/hyperledger/fabric/orderer/msp/tlsintermediatecerts: no such file or directory]
[36m2019-01-29 08:19:54.890 UTC [msp] getPemMaterialFromDir -> DEBU 010[0m Reading directory /etc/hyperledger/fabric/orderer/msp/crls
[36m2019-01-29 08:19:54.890 UTC [msp] getMspConfig -> DEBU 011[0m crls folder not found at [/etc/hyperledger/fabric/orderer/msp/crls]. Skipping. [stat /etc/hyperledger/fabric/orderer/msp/crls: no such file or directory]
[36m2019-01-29 08:19:54.890 UTC [msp] getMspConfig -> DEBU 012[0m MSP configuration file not found at [/etc/hyperledger/fabric/orderer/msp/config.yaml]: [stat /etc/hyperledger/fabric/orderer/msp/config.yaml: no such file or directory]
[36m2019-01-29 08:19:54.890 UTC [msp] newBccspMsp -> DEBU 013[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:19:54.890 UTC [msp] New -> DEBU 014[0m Creating Cache-MSP instance
[36m2019-01-29 08:19:54.890 UTC [msp] loadLocaMSP -> DEBU 015[0m Created new local MSP
[36m2019-01-29 08:19:54.890 UTC [msp] Setup -> DEBU 016[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 08:19:54.890 UTC [msp/identity] newIdentity -> DEBU 017[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 08:19:54.891 UTC [msp/identity] newIdentity -> DEBU 018[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 08:19:54.924 UTC [msp/identity] newIdentity -> DEBU 019[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICLDCCAdKgAwIBAgIQIoN1EApgEjYu/O7unXVqYzAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBqMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEuMCwGA1UEAxMlb3JkZXJlcjAuaG9zcGl0YWwxLnN3
aXRjaDJsb2dpYy5jby56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABH2/gH3f
8uhcO2CKglQg53cAJL79dENZNdRWfaRikU5j3X60lh9Bx5YpznQ5l0qMaCK1Ogec
Nw5LGgie9ESxrHyjTTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsG
A1UdIwQkMCKAIBiGNntD5tBlG9C8QNsdXde3ItTRRa+p/+3yJwRCsbJxMAoGCCqG
SM49BAMCA0gAMEUCIQCBtzCy6LE3S8b/XG08Qcs01A8QjmCwqmZJQrDvRA8ZhgIg
aAwP9tYtEZs1DBv2bH7HwYIwmi8nlgD3+Xp6CWwvAkg=
-----END CERTIFICATE-----
[36m2019-01-29 08:19:54.925 UTC [bccsp_sw] loadPrivateKey -> DEBU 01a[0m Loading private key [f9564620d77ba2b2650b4916b06387b38f212a58c63268d4c08f9b019eaa0f87] at [/etc/hyperledger/fabric/orderer/msp/keystore/f9564620d77ba2b2650b4916b06387b38f212a58c63268d4c08f9b019eaa0f87_sk]...
[36m2019-01-29 08:19:54.925 UTC [msp/identity] newIdentity -> DEBU 01b[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICLDCCAdKgAwIBAgIQIoN1EApgEjYu/O7unXVqYzAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBqMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEuMCwGA1UEAxMlb3JkZXJlcjAuaG9zcGl0YWwxLnN3
aXRjaDJsb2dpYy5jby56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABH2/gH3f
8uhcO2CKglQg53cAJL79dENZNdRWfaRikU5j3X60lh9Bx5YpznQ5l0qMaCK1Ogec
Nw5LGgie9ESxrHyjTTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsG
A1UdIwQkMCKAIBiGNntD5tBlG9C8QNsdXde3ItTRRa+p/+3yJwRCsbJxMAoGCCqG
SM49BAMCA0gAMEUCIQCBtzCy6LE3S8b/XG08Qcs01A8QjmCwqmZJQrDvRA8ZhgIg
aAwP9tYtEZs1DBv2bH7HwYIwmi8nlgD3+Xp6CWwvAkg=
-----END CERTIFICATE-----
[36m2019-01-29 08:19:54.925 UTC [msp] setupSigningIdentity -> DEBU 01c[0m Signing identity expires at 2029-01-26 07:41:54 +0000 UTC
[36m2019-01-29 08:19:54.925 UTC [msp] Validate -> DEBU 01d[0m MSP OrdererMSP validating identity
2019-01-29 08:19:54.926 UTC [orderer/common/server] prettyPrintStruct -> INFO 01e[0m Orderer config values:
	General.LedgerType = "file"
	General.ListenAddress = "0.0.0.0"
	General.ListenPort = 7050
	General.TLS.Enabled = true
	General.TLS.PrivateKey = "/etc/hyperledger/fabric/orderer/tls/server.key"
	General.TLS.Certificate = "/etc/hyperledger/fabric/orderer/tls/server.crt"
	General.TLS.RootCAs = [/etc/hyperledger/fabric/orderer/tls/ca.crt]
	General.TLS.ClientAuthRequired = false
	General.TLS.ClientRootCAs = []
	General.Keepalive.ServerMinInterval = 1m0s
	General.Keepalive.ServerInterval = 2h0m0s
	General.Keepalive.ServerTimeout = 20s
	General.GenesisMethod = "file"
	General.GenesisProfile = "SampleInsecureSolo"
	General.SystemChannel = "test-system-channel-name"
	General.GenesisFile = "/etc/hyperledger/fabric/orderer/channel-artifacts/genesis.block"
	General.Profile.Enabled = false
	General.Profile.Address = "0.0.0.0:6060"
	General.LogLevel = "debug"
	General.LogFormat = "%{color}%{time:2006-01-02 15:04:05.000 MST} [%{module}] %{shortfunc} -> %{level:.4s} %{id:03x}%{color:reset} %{message}"
	General.LocalMSPDir = "/etc/hyperledger/fabric/orderer/msp"
	General.LocalMSPID = "OrdererMSP"
	General.BCCSP.ProviderName = "SW"
	General.BCCSP.SwOpts.SecLevel = 256
	General.BCCSP.SwOpts.HashFamily = "SHA2"
	General.BCCSP.SwOpts.Ephemeral = false
	General.BCCSP.SwOpts.FileKeystore.KeyStorePath = "/etc/hyperledger/fabric/orderer/msp/keystore"
	General.BCCSP.SwOpts.DummyKeystore =
	General.BCCSP.PluginOpts =
	General.Authentication.TimeWindow = 15m0s
	FileLedger.Location = "/var/hyperledger/production/orderer"
	FileLedger.Prefix = "hyperledger-fabric-ordererledger"
	RAMLedger.HistorySize = 1000
	Kafka.Retry.ShortInterval = 1s
	Kafka.Retry.ShortTotal = 30s
	Kafka.Retry.LongInterval = 5m0s
	Kafka.Retry.LongTotal = 12h0m0s
	Kafka.Retry.NetworkTimeouts.DialTimeout = 10s
	Kafka.Retry.NetworkTimeouts.ReadTimeout = 10s
	Kafka.Retry.NetworkTimeouts.WriteTimeout = 10s
	Kafka.Retry.Metadata.RetryMax = 3
	Kafka.Retry.Metadata.RetryBackoff = 250ms
	Kafka.Retry.Producer.RetryMax = 3
	Kafka.Retry.Producer.RetryBackoff = 100ms
	Kafka.Retry.Consumer.RetryBackoff = 2s
	Kafka.Verbose = true
	Kafka.Version = 0.10.2.0
	Kafka.TLS.Enabled = false
	Kafka.TLS.PrivateKey = ""
	Kafka.TLS.Certificate = ""
	Kafka.TLS.RootCAs = []
	Kafka.TLS.ClientAuthRequired = false
	Kafka.TLS.ClientRootCAs = []
	Debug.BroadcastTraceDir = ""
	Debug.DeliverTraceDir = ""
2019-01-29 08:19:54.926 UTC [orderer/common/server] initializeServerConfig -> INFO 01f[0m Starting orderer with TLS enabled
[36m2019-01-29 08:19:54.926 UTC [orderer/common/server] createLedgerFactory -> DEBU 020[0m Ledger dir: /var/hyperledger/production/orderer
[36m2019-01-29 08:19:54.926 UTC [kvledger.util] CreateDirIfMissing -> DEBU 021[0m CreateDirIfMissing [/var/hyperledger/production/orderer/index/]
[36m2019-01-29 08:19:54.926 UTC [kvledger.util] logDirStatus -> DEBU 022[0m Before creating dir - [/var/hyperledger/production/orderer/index/] exists
[36m2019-01-29 08:19:54.926 UTC [kvledger.util] logDirStatus -> DEBU 023[0m After creating dir - [/var/hyperledger/production/orderer/index/] exists
[36m2019-01-29 08:19:54.935 UTC [orderer/common/server] createSubDir -> DEBU 024[0m Found chains sub-dir and using it
2019-01-29 08:19:54.935 UTC [orderer/common/server] initializeMultichannelRegistrar -> INFO 025[0m Not bootstrapping because of existing chains
[36m2019-01-29 08:19:54.936 UTC [fsblkstorage] newBlockfileMgr -> DEBU 026[0m newBlockfileMgr() initializing file-based block storage for ledger: testchainid 
[36m2019-01-29 08:19:54.936 UTC [kvledger.util] CreateDirIfMissing -> DEBU 027[0m CreateDirIfMissing [/var/hyperledger/production/orderer/chains/testchainid/]
[36m2019-01-29 08:19:54.936 UTC [kvledger.util] logDirStatus -> DEBU 028[0m Before creating dir - [/var/hyperledger/production/orderer/chains/testchainid/] exists
[36m2019-01-29 08:19:54.936 UTC [kvledger.util] logDirStatus -> DEBU 029[0m After creating dir - [/var/hyperledger/production/orderer/chains/testchainid/] exists
[36m2019-01-29 08:19:54.936 UTC [fsblkstorage] loadCurrentInfo -> DEBU 02a[0m loaded checkpointInfo:latestFileChunkSuffixNum=[0], latestFileChunksize=[18495], isChainEmpty=[false], lastBlockNumber=[0]
[36m2019-01-29 08:19:54.936 UTC [fsblkstorage] newBlockfileMgr -> DEBU 02b[0m Synching block information from block storage (if needed)
[36m2019-01-29 08:19:54.936 UTC [fsblkstorage] syncCPInfoFromFS -> DEBU 02c[0m Starting checkpoint=latestFileChunkSuffixNum=[0], latestFileChunksize=[18495], isChainEmpty=[false], lastBlockNumber=[0]
[36m2019-01-29 08:19:54.936 UTC [fsblkstorage] syncCPInfoFromFS -> DEBU 02d[0m status of file [/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000]: exists=[true], size=[18495]
[36m2019-01-29 08:19:54.938 UTC [fsblkstorage] newBlockIndex -> DEBU 02e[0m newBlockIndex() - indexItems:[[BlockNum]]
[36m2019-01-29 08:19:54.938 UTC [fsblkstorage] syncIndex -> DEBU 02f[0m Both the block files and indices are in sync.
[36m2019-01-29 08:19:54.938 UTC [fsblkstorage] retrieveBlockHeaderByNumber -> DEBU 030[0m retrieveBlockHeaderByNumber() - blockNum = [0]
[36m2019-01-29 08:19:54.938 UTC [fsblkstorage] newBlockfileStream -> DEBU 031[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 08:19:54.938 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 032[0m Remaining bytes=[18495], Going to peek [8] bytes
[36m2019-01-29 08:19:54.938 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 033[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 08:19:54.938 UTC [fsblkstorage] Next -> DEBU 034[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 08:19:54.938 UTC [fsblkstorage] newBlockfileStream -> DEBU 035[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 08:19:54.938 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 036[0m Remaining bytes=[18495], Going to peek [8] bytes
[36m2019-01-29 08:19:54.938 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 037[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 08:19:54.938 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 038[0m blockbytes [18492] read from file [0]
[36m2019-01-29 08:19:54.939 UTC [fsblkstorage] Next -> DEBU 039[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 08:19:54.939 UTC [fsblkstorage] newBlockfileStream -> DEBU 03a[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 08:19:54.939 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 03b[0m Remaining bytes=[18495], Going to peek [8] bytes
[36m2019-01-29 08:19:54.939 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 03c[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 08:19:54.939 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 03d[0m blockbytes [18492] read from file [0]
[36m2019-01-29 08:19:54.939 UTC [common/channelconfig] NewStandardValues -> DEBU 03e[0m Initializing protos for *channelconfig.ChannelProtos
[36m2019-01-29 08:19:54.939 UTC [common/channelconfig] initializeProtosStruct -> DEBU 03f[0m Processing field: HashingAlgorithm
[36m2019-01-29 08:19:54.939 UTC [common/channelconfig] initializeProtosStruct -> DEBU 040[0m Processing field: BlockDataHashingStructure
[36m2019-01-29 08:19:54.939 UTC [common/channelconfig] initializeProtosStruct -> DEBU 041[0m Processing field: OrdererAddresses
[36m2019-01-29 08:19:54.939 UTC [common/channelconfig] initializeProtosStruct -> DEBU 042[0m Processing field: Consortium
[36m2019-01-29 08:19:54.939 UTC [common/channelconfig] initializeProtosStruct -> DEBU 043[0m Processing field: Capabilities
[36m2019-01-29 08:19:54.939 UTC [common/channelconfig] NewStandardValues -> DEBU 044[0m Initializing protos for *channelconfig.ConsortiumProtos
[36m2019-01-29 08:19:54.939 UTC [common/channelconfig] initializeProtosStruct -> DEBU 045[0m Processing field: ChannelCreationPolicy
[36m2019-01-29 08:19:54.940 UTC [common/channelconfig] NewStandardValues -> DEBU 046[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:19:54.940 UTC [common/channelconfig] initializeProtosStruct -> DEBU 047[0m Processing field: MSP
[36m2019-01-29 08:19:54.940 UTC [common/channelconfig] validateMSP -> DEBU 048[0m Setting up MSP for org Hospital1MSP
[36m2019-01-29 08:19:54.940 UTC [msp] newBccspMsp -> DEBU 049[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:19:54.940 UTC [msp] New -> DEBU 04a[0m Creating Cache-MSP instance
[36m2019-01-29 08:19:54.940 UTC [msp] Setup -> DEBU 04b[0m Setting up MSP instance Hospital1MSP
[36m2019-01-29 08:19:54.940 UTC [msp/identity] newIdentity -> DEBU 04c[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdDCCAhugAwIBAgIQZxH8VsnceZxXgKxlURLgVTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMS5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQX9V4k/h/3pFxByKDscjgQrC0YS/bSbKvzSG2Z2PvltEWp40qB9OaKUN0GQYzo
yK2Etdvyjfd1OFwaTjpGSvv9o18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCBVU7DoP9Msf/nNTNl6
5FxwAXZ82eaGBT7zWYdyh4QbBjAKBggqhkjOPQQDAgNHADBEAiBmXGWQbxCGRp0c
5/y9qJyciB1CDHsknRZDjNzt5f563wIgMy7VPi1R8DpIIIRN9B3mfhDpRa+J+8PA
S4DOJy7rjp4=
-----END CERTIFICATE-----
[36m2019-01-29 08:19:54.940 UTC [msp/identity] newIdentity -> DEBU 04d[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 08:19:54.941 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 04e[0m Checking if identity satisfies role [CLIENT] for Hospital1MSP
[36m2019-01-29 08:19:54.941 UTC [msp] Validate -> DEBU 04f[0m MSP Hospital1MSP validating identity
[36m2019-01-29 08:19:54.941 UTC [msp] getCertificationChain -> DEBU 050[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 08:19:54.941 UTC [msp] hasOURole -> DEBU 051[0m MSP Hospital1MSP checking if the identity is a client
[36m2019-01-29 08:19:54.941 UTC [msp] getCertificationChain -> DEBU 052[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 08:19:54.941 UTC [common/channelconfig] NewStandardValues -> DEBU 053[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:19:54.941 UTC [common/channelconfig] initializeProtosStruct -> DEBU 054[0m Processing field: MSP
[36m2019-01-29 08:19:54.941 UTC [common/channelconfig] validateMSP -> DEBU 055[0m Setting up MSP for org Hospital2MSP
[36m2019-01-29 08:19:54.941 UTC [msp] newBccspMsp -> DEBU 056[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:19:54.941 UTC [msp] New -> DEBU 057[0m Creating Cache-MSP instance
[36m2019-01-29 08:19:54.941 UTC [msp] Setup -> DEBU 058[0m Setting up MSP instance Hospital2MSP
[36m2019-01-29 08:19:54.941 UTC [msp/identity] newIdentity -> DEBU 059[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdjCCAhygAwIBAgIRALZ3Pffn/xWUtUWPSK6CYVAwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowgYsxCzAJBgNVBAYTAlVTMRMwEQYD
VQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4gRnJhbmNpc2NvMSUwIwYDVQQK
Exxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMSgwJgYDVQQDEx9jYS5ob3Nw
aXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcD
QgAESzVpiz6P4mIb+KwvrttcXJKZR5i137Is4298E7OrkYdAg0tB4rpfKTXzUMwW
vI12cTDujpyaZSiqQ3/zP4fOQqNfMF0wDgYDVR0PAQH/BAQDAgGmMA8GA1UdJQQI
MAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQg8QJqWKPaX24HINwR
S6sINFSwRD+440/ccSnvFVaGF24wCgYIKoZIzj0EAwIDSAAwRQIhAPO2Nng3/nqb
cKPliUqVPVjBMu+D55M90+jexWSOpkjcAiADPx8qv9exZ4BfP3+G49Qzpx0Meblj
53vzUgYFa/0EXw==
-----END CERTIFICATE-----
[36m2019-01-29 08:19:54.942 UTC [msp/identity] newIdentity -> DEBU 05a[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAJ6XtHMTTW1JzdpD54qCc3wwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA+z1rb9aA1HwHfNLi9V9VZe
bdwT4MXak445zGz4DXzRUsmNeBngzntKb6qpgHQpeFtzbzLx+zwiynRDvfdzORGj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPEC
alij2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQC
IA8A3CCJeCasyy7gYd/M1aHYmwWsecY7UY7cVUGy4XOxAiB0rrmfnGW6JA11PDCu
+5c5Rpn0jF5ddsFn+UIpzBFvmg==
-----END CERTIFICATE-----
[36m2019-01-29 08:19:54.942 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 05b[0m Checking if identity satisfies role [CLIENT] for Hospital2MSP
[36m2019-01-29 08:19:54.942 UTC [msp] Validate -> DEBU 05c[0m MSP Hospital2MSP validating identity
[36m2019-01-29 08:19:54.943 UTC [msp] getCertificationChain -> DEBU 05d[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 08:19:54.943 UTC [msp] hasOURole -> DEBU 05e[0m MSP Hospital2MSP checking if the identity is a client
[36m2019-01-29 08:19:54.943 UTC [msp] getCertificationChain -> DEBU 05f[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 08:19:54.943 UTC [common/channelconfig] NewStandardValues -> DEBU 060[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:19:54.943 UTC [common/channelconfig] initializeProtosStruct -> DEBU 061[0m Processing field: MSP
[36m2019-01-29 08:19:54.943 UTC [common/channelconfig] validateMSP -> DEBU 062[0m Setting up MSP for org Hospital3MSP
[36m2019-01-29 08:19:54.943 UTC [msp] newBccspMsp -> DEBU 063[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:19:54.943 UTC [msp] New -> DEBU 064[0m Creating Cache-MSP instance
[36m2019-01-29 08:19:54.943 UTC [msp] Setup -> DEBU 065[0m Setting up MSP instance Hospital3MSP
[36m2019-01-29 08:19:54.943 UTC [msp/identity] newIdentity -> DEBU 066[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdTCCAhugAwIBAgIQCCXeHGgObIJo8aq4AdvgVjAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMy5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQm1Oo/AEu7ZZToE15Y5ntlaN128Hs/exX3jNH5nO6r4tvyFtBBNEn1ecXxxPtm
dmQ+nPHONorLwDC42SMAKwfro18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCA7vVWFXWpgZBojQ1ZL
xUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEAlOUoPtpfO8V7
vFRCanjU5zZWGO5ajFlPDJjjDTy3UrwCICrUYV32JrR/m3dslCl15dSThHrKtYCC
Ehvyw1+lNqiG
-----END CERTIFICATE-----
[36m2019-01-29 08:19:54.944 UTC [msp/identity] newIdentity -> DEBU 067[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAI8BA/5Jf+5m8ncI+D7aSiMwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwzLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABF1IehIKIjh8tXYZ7rw4QOuc
g/tpBAkJeOt44zr/LepbjhyFW8vIRnkw75i6Wcg+LOoAS+2NFNg2oNjqg//yCHWj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIDu9
VYVdamBkGiNDVkvFS+AEt6Jrn0yK8hFyPGZhqenJMAoGCCqGSM49BAMCA0cAMEQC
IFyc85pwbCAxGNdw4fflsX2I3HrP3h93YAB2zSDC8HtkAiAQxblAZfEnXUiQKrqE
8I10bYJqD6pJz1jb3Pv+4xplTA==
-----END CERTIFICATE-----
[36m2019-01-29 08:19:54.945 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 068[0m Checking if identity satisfies role [CLIENT] for Hospital3MSP
[36m2019-01-29 08:19:54.945 UTC [msp] Validate -> DEBU 069[0m MSP Hospital3MSP validating identity
[36m2019-01-29 08:19:54.945 UTC [msp] getCertificationChain -> DEBU 06a[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 08:19:54.946 UTC [msp] hasOURole -> DEBU 06b[0m MSP Hospital3MSP checking if the identity is a client
[36m2019-01-29 08:19:54.946 UTC [msp] getCertificationChain -> DEBU 06c[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 08:19:54.947 UTC [common/channelconfig] NewStandardValues -> DEBU 06d[0m Initializing protos for *channelconfig.OrdererProtos
[36m2019-01-29 08:19:54.947 UTC [common/channelconfig] initializeProtosStruct -> DEBU 06e[0m Processing field: ConsensusType
[36m2019-01-29 08:19:54.947 UTC [common/channelconfig] initializeProtosStruct -> DEBU 06f[0m Processing field: BatchSize
[36m2019-01-29 08:19:54.947 UTC [common/channelconfig] initializeProtosStruct -> DEBU 070[0m Processing field: BatchTimeout
[36m2019-01-29 08:19:54.947 UTC [common/channelconfig] initializeProtosStruct -> DEBU 071[0m Processing field: KafkaBrokers
[36m2019-01-29 08:19:54.947 UTC [common/channelconfig] initializeProtosStruct -> DEBU 072[0m Processing field: ChannelRestrictions
[36m2019-01-29 08:19:54.947 UTC [common/channelconfig] initializeProtosStruct -> DEBU 073[0m Processing field: Capabilities
[36m2019-01-29 08:19:54.948 UTC [common/channelconfig] NewStandardValues -> DEBU 074[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:19:54.948 UTC [common/channelconfig] initializeProtosStruct -> DEBU 075[0m Processing field: MSP
[36m2019-01-29 08:19:54.948 UTC [common/channelconfig] validateMSP -> DEBU 076[0m Setting up MSP for org OrdererOrg
[36m2019-01-29 08:19:54.948 UTC [msp] newBccspMsp -> DEBU 077[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:19:54.948 UTC [msp] New -> DEBU 078[0m Creating Cache-MSP instance
[36m2019-01-29 08:19:54.948 UTC [msp] Setup -> DEBU 079[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 08:19:54.949 UTC [msp/identity] newIdentity -> DEBU 07a[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 08:19:54.951 UTC [msp/identity] newIdentity -> DEBU 07b[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 08:19:54.952 UTC [msp] Validate -> DEBU 07c[0m MSP OrdererMSP validating identity
[36m2019-01-29 08:19:54.952 UTC [msp] Setup -> DEBU 07d[0m Setting up the MSP manager (4 msps)
[36m2019-01-29 08:19:54.952 UTC [msp] Setup -> DEBU 07e[0m MSP manager setup complete, setup 4 msps
[36m2019-01-29 08:19:54.952 UTC [policies] NewManagerImpl -> DEBU 07f[0m Proposed new policy Admins for Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 08:19:54.952 UTC [policies] NewManagerImpl -> DEBU 080[0m Proposed new policy Readers for Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 08:19:54.952 UTC [policies] NewManagerImpl -> DEBU 081[0m Proposed new policy Writers for Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 08:19:54.952 UTC [policies] NewManagerImpl -> DEBU 082[0m Proposed new policy Admins for Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 08:19:54.952 UTC [policies] NewManagerImpl -> DEBU 083[0m Proposed new policy Readers for Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 08:19:54.952 UTC [policies] NewManagerImpl -> DEBU 084[0m Proposed new policy Writers for Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 08:19:54.952 UTC [policies] NewManagerImpl -> DEBU 085[0m Proposed new policy Readers for Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 08:19:54.952 UTC [policies] NewManagerImpl -> DEBU 086[0m Proposed new policy Writers for Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 08:19:54.952 UTC [policies] NewManagerImpl -> DEBU 087[0m Proposed new policy Admins for Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 08:19:54.952 UTC [policies] NewManagerImpl -> DEBU 088[0m Proposed new policy Admins for Channel/Consortiums
[36m2019-01-29 08:19:54.953 UTC [policies] NewManagerImpl -> DEBU 089[0m Proposed new policy Readers for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:19:54.953 UTC [policies] NewManagerImpl -> DEBU 08a[0m Proposed new policy Writers for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:19:54.953 UTC [policies] NewManagerImpl -> DEBU 08b[0m Proposed new policy Admins for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:19:54.953 UTC [policies] NewManagerImpl -> DEBU 08c[0m Proposed new policy Admins for Channel/Orderer
[36m2019-01-29 08:19:54.953 UTC [policies] NewManagerImpl -> DEBU 08d[0m Proposed new policy Readers for Channel/Orderer
[36m2019-01-29 08:19:54.953 UTC [policies] NewManagerImpl -> DEBU 08e[0m Proposed new policy Writers for Channel/Orderer
[36m2019-01-29 08:19:54.953 UTC [policies] NewManagerImpl -> DEBU 08f[0m Proposed new policy BlockValidation for Channel/Orderer
[36m2019-01-29 08:19:54.953 UTC [policies] NewManagerImpl -> DEBU 090[0m Proposed new policy Admins for Channel
[36m2019-01-29 08:19:54.953 UTC [policies] GetPolicy -> DEBU 091[0m Returning dummy reject all policy because Readers could not be found in Channel/Consortiums/Readers
[36m2019-01-29 08:19:54.953 UTC [policies] NewManagerImpl -> DEBU 092[0m Proposed new policy Readers for Channel
[36m2019-01-29 08:19:54.953 UTC [policies] GetPolicy -> DEBU 093[0m Returning dummy reject all policy because Writers could not be found in Channel/Consortiums/Writers
[36m2019-01-29 08:19:54.953 UTC [policies] NewManagerImpl -> DEBU 094[0m Proposed new policy Writers for Channel
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 095[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 096[0m Adding to config map: [Group]  /Channel/Consortiums
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 097[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 098[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 099[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/Hospital1MSP/MSP
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 09a[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital1MSP/Readers
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 09b[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital1MSP/Writers
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 09c[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital1MSP/Admins
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 09d[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 09e[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/Hospital2MSP/MSP
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 09f[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital2MSP/Admins
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 0a0[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital2MSP/Readers
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 0a1[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital2MSP/Writers
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 0a2[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 0a3[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/Hospital3MSP/MSP
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 0a4[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital3MSP/Admins
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 0a5[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital3MSP/Readers
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 0a6[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital3MSP/Writers
[36m2019-01-29 08:19:54.953 UTC [common/configtx] addToMap -> DEBU 0a7[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/ChannelCreationPolicy
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0a8[0m Adding to config map: [Policy] /Channel/Consortiums/Admins
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0a9[0m Adding to config map: [Group]  /Channel/Orderer
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0aa[0m Adding to config map: [Group]  /Channel/Orderer/OrdererOrg
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0ab[0m Adding to config map: [Value]  /Channel/Orderer/OrdererOrg/MSP
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0ac[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0ad[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0ae[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Admins
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0af[0m Adding to config map: [Value]  /Channel/Orderer/ConsensusType
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0b0[0m Adding to config map: [Value]  /Channel/Orderer/BatchSize
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0b1[0m Adding to config map: [Value]  /Channel/Orderer/BatchTimeout
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0b2[0m Adding to config map: [Value]  /Channel/Orderer/ChannelRestrictions
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0b3[0m Adding to config map: [Value]  /Channel/Orderer/Capabilities
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0b4[0m Adding to config map: [Value]  /Channel/Orderer/KafkaBrokers
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0b5[0m Adding to config map: [Policy] /Channel/Orderer/Admins
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0b6[0m Adding to config map: [Policy] /Channel/Orderer/Readers
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0b7[0m Adding to config map: [Policy] /Channel/Orderer/Writers
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0b8[0m Adding to config map: [Policy] /Channel/Orderer/BlockValidation
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0b9[0m Adding to config map: [Value]  /Channel/HashingAlgorithm
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0ba[0m Adding to config map: [Value]  /Channel/BlockDataHashingStructure
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0bb[0m Adding to config map: [Value]  /Channel/OrdererAddresses
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0bc[0m Adding to config map: [Value]  /Channel/Capabilities
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0bd[0m Adding to config map: [Policy] /Channel/Admins
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0be[0m Adding to config map: [Policy] /Channel/Readers
[36m2019-01-29 08:19:54.954 UTC [common/configtx] addToMap -> DEBU 0bf[0m Adding to config map: [Policy] /Channel/Writers
[36m2019-01-29 08:19:54.954 UTC [common/channelconfig] LogSanityChecks -> DEBU 0c0[0m As expected, current configuration has policy '/Channel/Readers'
[36m2019-01-29 08:19:54.954 UTC [common/channelconfig] LogSanityChecks -> DEBU 0c1[0m As expected, current configuration has policy '/Channel/Writers'
[36m2019-01-29 08:19:54.954 UTC [policies] Manager -> DEBU 0c2[0m Manager Channel looking up path [Application]
[36m2019-01-29 08:19:54.954 UTC [policies] Manager -> DEBU 0c3[0m Manager Channel has managers Consortiums
[36m2019-01-29 08:19:54.954 UTC [policies] Manager -> DEBU 0c4[0m Manager Channel has managers Orderer
[36m2019-01-29 08:19:54.954 UTC [policies] Manager -> DEBU 0c5[0m Manager Channel looking up path [Orderer]
[36m2019-01-29 08:19:54.954 UTC [policies] Manager -> DEBU 0c6[0m Manager Channel has managers Consortiums
[36m2019-01-29 08:19:54.954 UTC [policies] Manager -> DEBU 0c7[0m Manager Channel has managers Orderer
[36m2019-01-29 08:19:54.954 UTC [policies] Manager -> DEBU 0c8[0m Manager Channel/Orderer looking up path []
[36m2019-01-29 08:19:54.954 UTC [policies] Manager -> DEBU 0c9[0m Manager Channel/Orderer has managers OrdererOrg
[36m2019-01-29 08:19:54.954 UTC [common/channelconfig] LogSanityChecks -> DEBU 0ca[0m As expected, current configuration has policy '/Channel/Orderer/BlockValidation'
[36m2019-01-29 08:19:54.954 UTC [common/capabilities] Supported -> DEBU 0cb[0m Orderer capability V1_1 is supported and is enabled
[36m2019-01-29 08:19:54.954 UTC [common/capabilities] Supported -> DEBU 0cc[0m Channel capability V1_1 is supported and is enabled
[36m2019-01-29 08:19:54.954 UTC [fsblkstorage] Next -> DEBU 0cd[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 08:19:54.954 UTC [fsblkstorage] newBlockfileStream -> DEBU 0ce[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 08:19:54.955 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0cf[0m Remaining bytes=[18495], Going to peek [8] bytes
[36m2019-01-29 08:19:54.955 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0d0[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 08:19:54.955 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0d1[0m blockbytes [18492] read from file [0]
[36m2019-01-29 08:19:54.955 UTC [orderer/commmon/multichannel] newBlockWriter -> DEBU 0d2[0m [channel: testchainid] Creating block writer for tip of chain (blockNumber=0, lastConfigBlockNum=0, lastConfigSeq=0)
2019-01-29 08:19:54.955 UTC [orderer/consensus/kafka] newChain -> INFO 0d3[0m [channel: testchainid] Starting chain with last persisted offset -3 and last recorded block 0
[36m2019-01-29 08:19:54.955 UTC [orderer/commmon/multichannel] newChainSupport -> DEBU 0d4[0m [channel: testchainid] Done creating channel support resources
[36m2019-01-29 08:19:54.955 UTC [orderer/common/msgprocessor] NewSystemChannel -> DEBU 0d5[0m Creating system channel msg processor for channel testchainid
[36m2019-01-29 08:19:54.955 UTC [fsblkstorage] Next -> DEBU 0d6[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 08:19:54.955 UTC [fsblkstorage] newBlockfileStream -> DEBU 0d7[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 08:19:54.955 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0d8[0m Remaining bytes=[18495], Going to peek [8] bytes
[36m2019-01-29 08:19:54.955 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0d9[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 08:19:54.955 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0da[0m blockbytes [18492] read from file [0]
2019-01-29 08:19:54.955 UTC [orderer/commmon/multichannel] NewRegistrar -> INFO 0db[0m Starting system channel 'testchainid' with genesis block hash c84c51e50921c0294405e41d16cb6234273f63b37a240438cc9f32578277d7e3 and orderer type kafka
2019-01-29 08:19:54.955 UTC [orderer/common/server] Start -> INFO 0dc[0m Starting orderer:
 Version: 1.2.0
 Commit SHA: cae2ad4
 Go version: go1.10
 OS/Arch: linux/amd64
 Experimental features: false
2019-01-29 08:19:54.955 UTC [orderer/consensus/kafka] setupProducerForChannel -> INFO 0de[0m [channel: testchainid] Setting up the producer for this channel...
[36m2019-01-29 08:19:54.955 UTC [orderer/consensus/kafka] try -> DEBU 0df[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 08:19:54.955 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 0e0[0m Initializing new client
[36m2019-01-29 08:19:54.955 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 0e1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:19:54.955 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 0e2[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:19:54.955 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0e3[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
2019-01-29 08:19:54.955 UTC [orderer/common/server] Start -> INFO 0dd[0m Beginning to serve requests
[36m2019-01-29 08:19:54.956 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 0e4[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.150:10092: connect: connection refused
[36m2019-01-29 08:19:54.957 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0e5[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.150:10092: connect: connection refused
[36m2019-01-29 08:19:54.957 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 0e6[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:19:54.957 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0e7[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 08:19:54.957 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 0e8[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.149:11092: connect: connection refused
[36m2019-01-29 08:19:54.957 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0e9[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.149:11092: connect: connection refused
[36m2019-01-29 08:19:54.957 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 0ea[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:19:54.957 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0eb[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:19:54.958 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 0ec[0m Connected to broker at kafka0.switch2logic.co.za:9092 (unregistered)
[36m2019-01-29 08:19:54.963 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 0ed[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 08:19:54.963 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 0ee[0m client/brokers registered new broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 08:19:54.963 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 0ef[0m client/brokers registered new broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:19:54.963 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 0f0[0m Successfully initialized new client
[36m2019-01-29 08:19:54.963 UTC [orderer/consensus/kafka] try -> DEBU 0f1[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 08:19:54.963 UTC [orderer/consensus/kafka] startThread -> INFO 0f2[0m [channel: testchainid] Producer set up successfully
2019-01-29 08:19:54.964 UTC [orderer/consensus/kafka] sendConnectMessage -> INFO 0f3[0m [channel: testchainid] About to post the CONNECT message...
[36m2019-01-29 08:19:54.964 UTC [orderer/consensus/kafka] try -> DEBU 0f4[0m [channel: testchainid] Attempting to post the CONNECT message...
[36m2019-01-29 08:19:54.964 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 0f5[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:19:54.964 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU 0f6[0m producer/broker/1 starting up
[36m2019-01-29 08:19:54.964 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU 0f7[0m producer/broker/1 state change to [open] on testchainid/0
[36m2019-01-29 08:19:54.964 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 0f8[0m Connected to broker at kafka1.switch2logic.co.za:9092 (registered as #1)
[36m2019-01-29 08:19:54.970 UTC [orderer/consensus/kafka] try -> DEBU 0f9[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 08:19:54.970 UTC [orderer/consensus/kafka] startThread -> INFO 0fa[0m [channel: testchainid] CONNECT message posted successfully
2019-01-29 08:19:54.970 UTC [orderer/consensus/kafka] setupParentConsumerForChannel -> INFO 0fb[0m [channel: testchainid] Setting up the parent consumer for this channel...
[36m2019-01-29 08:19:54.970 UTC [orderer/consensus/kafka] try -> DEBU 0fc[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 08:19:54.970 UTC [orderer/consensus/kafka/sarama] NewConsumer -> DEBU 0fd[0m Initializing new client
[36m2019-01-29 08:19:54.970 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 0fe[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:19:54.970 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 0ff[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:19:54.970 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 100[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:19:54.971 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 101[0m Connected to broker at kafka0.switch2logic.co.za:9092 (unregistered)
[36m2019-01-29 08:19:54.972 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 102[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 08:19:54.972 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 103[0m client/brokers registered new broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 08:19:54.972 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 104[0m client/brokers registered new broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:19:54.972 UTC [orderer/consensus/kafka/sarama] NewConsumer -> DEBU 105[0m Successfully initialized new client
[36m2019-01-29 08:19:54.972 UTC [orderer/consensus/kafka] try -> DEBU 106[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 08:19:54.972 UTC [orderer/consensus/kafka] startThread -> INFO 107[0m [channel: testchainid] Parent consumer set up successfully
2019-01-29 08:19:54.972 UTC [orderer/consensus/kafka] setupChannelConsumerForChannel -> INFO 108[0m [channel: testchainid] Setting up the channel consumer for this channel (start offset: -2)...
[36m2019-01-29 08:19:54.972 UTC [orderer/consensus/kafka] try -> DEBU 109[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 08:19:54.972 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 10a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:19:54.974 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 10b[0m Connected to broker at kafka1.switch2logic.co.za:9092 (registered as #1)
[36m2019-01-29 08:19:54.976 UTC [orderer/consensus/kafka] try -> DEBU 10c[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 08:19:54.976 UTC [orderer/consensus/kafka] startThread -> INFO 10e[0m [channel: testchainid] Channel consumer set up successfully
2019-01-29 08:19:54.976 UTC [orderer/consensus/kafka] startThread -> INFO 10f[0m [channel: testchainid] Start phase completed successfully
[36m2019-01-29 08:19:54.976 UTC [orderer/consensus/kafka/sarama] subscriptionConsumer -> DEBU 10d[0m consumer/broker/1 added subscription to testchainid/0
[36m2019-01-29 08:19:54.977 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 110[0m [channel: testchainid] Successfully unmarshalled consumed message, offset is 0. Inspecting type...
[36m2019-01-29 08:19:54.977 UTC [orderer/consensus/kafka] processConnect -> DEBU 111[0m [channel: testchainid] It's a connect message - ignoring
[36m2019-01-29 08:19:54.977 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 112[0m [channel: testchainid] Successfully unmarshalled consumed message, offset is 1. Inspecting type...
[36m2019-01-29 08:19:54.977 UTC [orderer/consensus/kafka] processConnect -> DEBU 113[0m [channel: testchainid] It's a connect message - ignoring
[36m2019-01-29 08:19:54.977 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 114[0m [channel: testchainid] Successfully unmarshalled consumed message, offset is 2. Inspecting type...
[36m2019-01-29 08:19:54.977 UTC [orderer/consensus/kafka] processConnect -> DEBU 115[0m [channel: testchainid] It's a connect message - ignoring
[36m2019-01-29 08:29:54.964 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 116[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:29:54.973 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 117[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:35:25.174 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 118[0m [channel: testchainid] Successfully unmarshalled consumed message, offset is 3. Inspecting type...
[36m2019-01-29 08:35:25.174 UTC [orderer/consensus/kafka] processConnect -> DEBU 119[0m [channel: testchainid] It's a connect message - ignoring
[36m2019-01-29 08:39:54.964 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 11a[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:39:54.972 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 11b[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:42:47.100 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 11c[0m [channel: testchainid] Successfully unmarshalled consumed message, offset is 4. Inspecting type...
[36m2019-01-29 08:42:47.100 UTC [orderer/consensus/kafka] processConnect -> DEBU 11d[0m [channel: testchainid] It's a connect message - ignoring
[36m2019-01-29 08:49:54.964 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 11e[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:49:54.973 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 11f[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:50:44.315 UTC [orderer/common/server] Deliver -> DEBU 120[0m Starting new Deliver handler
[36m2019-01-29 08:50:44.315 UTC [common/deliver] Handle -> DEBU 121[0m Starting new deliver loop for 10.0.0.183:55292
[36m2019-01-29 08:50:44.315 UTC [common/deliver] Handle -> DEBU 122[0m Attempting to read seek info message from 10.0.0.183:55292
[36m2019-01-29 08:50:44.325 UTC [orderer/common/server] Broadcast -> DEBU 123[0m Starting new Broadcast handler
[36m2019-01-29 08:50:44.325 UTC [orderer/common/broadcast] Handle -> DEBU 124[0m Starting new broadcast loop for 10.0.0.183:55294
[36m2019-01-29 08:50:44.325 UTC [orderer/common/broadcast] Handle -> DEBU 125[0m [channel: comunitychannel] Broadcast is processing config update message from 10.0.0.183:55294
[36m2019-01-29 08:50:44.325 UTC [orderer/common/msgprocessor] ProcessConfigUpdateMsg -> DEBU 126[0m Processing config update tx with system channel message processor for channel ID comunitychannel
[36m2019-01-29 08:50:44.325 UTC [orderer/common/msgprocessor] ProcessConfigUpdateMsg -> DEBU 127[0m Processing channel create tx for channel comunitychannel on system channel testchainid
[36m2019-01-29 08:50:44.326 UTC [common/channelconfig] NewStandardValues -> DEBU 128[0m Initializing protos for *channelconfig.ChannelProtos
[36m2019-01-29 08:50:44.326 UTC [common/channelconfig] initializeProtosStruct -> DEBU 129[0m Processing field: HashingAlgorithm
[36m2019-01-29 08:50:44.326 UTC [common/channelconfig] initializeProtosStruct -> DEBU 12a[0m Processing field: BlockDataHashingStructure
[36m2019-01-29 08:50:44.326 UTC [common/channelconfig] initializeProtosStruct -> DEBU 12b[0m Processing field: OrdererAddresses
[36m2019-01-29 08:50:44.326 UTC [common/channelconfig] initializeProtosStruct -> DEBU 12c[0m Processing field: Consortium
[36m2019-01-29 08:50:44.326 UTC [common/channelconfig] initializeProtosStruct -> DEBU 12d[0m Processing field: Capabilities
[36m2019-01-29 08:50:44.326 UTC [common/channelconfig] NewStandardValues -> DEBU 12e[0m Initializing protos for *channelconfig.OrdererProtos
[36m2019-01-29 08:50:44.326 UTC [common/channelconfig] initializeProtosStruct -> DEBU 12f[0m Processing field: ConsensusType
[36m2019-01-29 08:50:44.326 UTC [common/channelconfig] initializeProtosStruct -> DEBU 130[0m Processing field: BatchSize
[36m2019-01-29 08:50:44.326 UTC [common/channelconfig] initializeProtosStruct -> DEBU 131[0m Processing field: BatchTimeout
[36m2019-01-29 08:50:44.326 UTC [common/channelconfig] initializeProtosStruct -> DEBU 132[0m Processing field: KafkaBrokers
[36m2019-01-29 08:50:44.326 UTC [common/channelconfig] initializeProtosStruct -> DEBU 133[0m Processing field: ChannelRestrictions
[36m2019-01-29 08:50:44.326 UTC [common/channelconfig] initializeProtosStruct -> DEBU 134[0m Processing field: Capabilities
[36m2019-01-29 08:50:44.326 UTC [common/channelconfig] NewStandardValues -> DEBU 135[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:50:44.326 UTC [common/channelconfig] initializeProtosStruct -> DEBU 136[0m Processing field: MSP
[36m2019-01-29 08:50:44.326 UTC [common/channelconfig] validateMSP -> DEBU 137[0m Setting up MSP for org OrdererOrg
[36m2019-01-29 08:50:44.326 UTC [msp] newBccspMsp -> DEBU 138[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:50:44.326 UTC [msp] New -> DEBU 139[0m Creating Cache-MSP instance
[36m2019-01-29 08:50:44.326 UTC [msp] Setup -> DEBU 13a[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 08:50:44.327 UTC [msp/identity] newIdentity -> DEBU 13b[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.327 UTC [msp/identity] newIdentity -> DEBU 13c[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.327 UTC [msp] Validate -> DEBU 13d[0m MSP OrdererMSP validating identity
[36m2019-01-29 08:50:44.327 UTC [common/channelconfig] NewStandardValues -> DEBU 13e[0m Initializing protos for *channelconfig.ApplicationProtos
[36m2019-01-29 08:50:44.327 UTC [common/channelconfig] initializeProtosStruct -> DEBU 13f[0m Processing field: ACLs
[36m2019-01-29 08:50:44.327 UTC [common/channelconfig] initializeProtosStruct -> DEBU 140[0m Processing field: Capabilities
[36m2019-01-29 08:50:44.327 UTC [common/channelconfig] NewStandardValues -> DEBU 141[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 08:50:44.327 UTC [common/channelconfig] initializeProtosStruct -> DEBU 142[0m Processing field: AnchorPeers
[36m2019-01-29 08:50:44.327 UTC [common/channelconfig] NewStandardValues -> DEBU 143[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:50:44.328 UTC [common/channelconfig] initializeProtosStruct -> DEBU 144[0m Processing field: MSP
[36m2019-01-29 08:50:44.328 UTC [common/channelconfig] Validate -> DEBU 145[0m Anchor peers for org Hospital2MSP are 
[36m2019-01-29 08:50:44.328 UTC [common/channelconfig] validateMSP -> DEBU 146[0m Setting up MSP for org Hospital2MSP
[36m2019-01-29 08:50:44.328 UTC [msp] newBccspMsp -> DEBU 147[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:50:44.328 UTC [msp] New -> DEBU 148[0m Creating Cache-MSP instance
[36m2019-01-29 08:50:44.329 UTC [msp] Setup -> DEBU 149[0m Setting up MSP instance Hospital2MSP
[36m2019-01-29 08:50:44.329 UTC [msp/identity] newIdentity -> DEBU 14a[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdjCCAhygAwIBAgIRALZ3Pffn/xWUtUWPSK6CYVAwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowgYsxCzAJBgNVBAYTAlVTMRMwEQYD
VQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4gRnJhbmNpc2NvMSUwIwYDVQQK
Exxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMSgwJgYDVQQDEx9jYS5ob3Nw
aXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcD
QgAESzVpiz6P4mIb+KwvrttcXJKZR5i137Is4298E7OrkYdAg0tB4rpfKTXzUMwW
vI12cTDujpyaZSiqQ3/zP4fOQqNfMF0wDgYDVR0PAQH/BAQDAgGmMA8GA1UdJQQI
MAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQg8QJqWKPaX24HINwR
S6sINFSwRD+440/ccSnvFVaGF24wCgYIKoZIzj0EAwIDSAAwRQIhAPO2Nng3/nqb
cKPliUqVPVjBMu+D55M90+jexWSOpkjcAiADPx8qv9exZ4BfP3+G49Qzpx0Meblj
53vzUgYFa/0EXw==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.329 UTC [msp/identity] newIdentity -> DEBU 14b[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAJ6XtHMTTW1JzdpD54qCc3wwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA+z1rb9aA1HwHfNLi9V9VZe
bdwT4MXak445zGz4DXzRUsmNeBngzntKb6qpgHQpeFtzbzLx+zwiynRDvfdzORGj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPEC
alij2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQC
IA8A3CCJeCasyy7gYd/M1aHYmwWsecY7UY7cVUGy4XOxAiB0rrmfnGW6JA11PDCu
+5c5Rpn0jF5ddsFn+UIpzBFvmg==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.330 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 14c[0m Checking if identity satisfies role [CLIENT] for Hospital2MSP
[36m2019-01-29 08:50:44.330 UTC [msp] Validate -> DEBU 14d[0m MSP Hospital2MSP validating identity
[36m2019-01-29 08:50:44.330 UTC [msp] getCertificationChain -> DEBU 14e[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 08:50:44.330 UTC [msp] hasOURole -> DEBU 14f[0m MSP Hospital2MSP checking if the identity is a client
[36m2019-01-29 08:50:44.330 UTC [msp] getCertificationChain -> DEBU 150[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 08:50:44.330 UTC [common/channelconfig] NewStandardValues -> DEBU 151[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 08:50:44.330 UTC [common/channelconfig] initializeProtosStruct -> DEBU 152[0m Processing field: AnchorPeers
[36m2019-01-29 08:50:44.331 UTC [common/channelconfig] NewStandardValues -> DEBU 153[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:50:44.331 UTC [common/channelconfig] initializeProtosStruct -> DEBU 154[0m Processing field: MSP
[36m2019-01-29 08:50:44.331 UTC [common/channelconfig] Validate -> DEBU 155[0m Anchor peers for org Hospital3MSP are 
[36m2019-01-29 08:50:44.331 UTC [common/channelconfig] validateMSP -> DEBU 156[0m Setting up MSP for org Hospital3MSP
[36m2019-01-29 08:50:44.331 UTC [msp] newBccspMsp -> DEBU 157[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:50:44.331 UTC [msp] New -> DEBU 158[0m Creating Cache-MSP instance
[36m2019-01-29 08:50:44.331 UTC [msp] Setup -> DEBU 159[0m Setting up MSP instance Hospital3MSP
[36m2019-01-29 08:50:44.331 UTC [msp/identity] newIdentity -> DEBU 15a[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdTCCAhugAwIBAgIQCCXeHGgObIJo8aq4AdvgVjAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMy5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQm1Oo/AEu7ZZToE15Y5ntlaN128Hs/exX3jNH5nO6r4tvyFtBBNEn1ecXxxPtm
dmQ+nPHONorLwDC42SMAKwfro18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCA7vVWFXWpgZBojQ1ZL
xUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEAlOUoPtpfO8V7
vFRCanjU5zZWGO5ajFlPDJjjDTy3UrwCICrUYV32JrR/m3dslCl15dSThHrKtYCC
Ehvyw1+lNqiG
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.331 UTC [msp/identity] newIdentity -> DEBU 15b[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAI8BA/5Jf+5m8ncI+D7aSiMwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwzLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABF1IehIKIjh8tXYZ7rw4QOuc
g/tpBAkJeOt44zr/LepbjhyFW8vIRnkw75i6Wcg+LOoAS+2NFNg2oNjqg//yCHWj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIDu9
VYVdamBkGiNDVkvFS+AEt6Jrn0yK8hFyPGZhqenJMAoGCCqGSM49BAMCA0cAMEQC
IFyc85pwbCAxGNdw4fflsX2I3HrP3h93YAB2zSDC8HtkAiAQxblAZfEnXUiQKrqE
8I10bYJqD6pJz1jb3Pv+4xplTA==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.332 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 15c[0m Checking if identity satisfies role [CLIENT] for Hospital3MSP
[36m2019-01-29 08:50:44.332 UTC [msp] Validate -> DEBU 15d[0m MSP Hospital3MSP validating identity
[36m2019-01-29 08:50:44.332 UTC [msp] getCertificationChain -> DEBU 15e[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 08:50:44.332 UTC [msp] hasOURole -> DEBU 15f[0m MSP Hospital3MSP checking if the identity is a client
[36m2019-01-29 08:50:44.332 UTC [msp] getCertificationChain -> DEBU 160[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 08:50:44.333 UTC [common/channelconfig] NewStandardValues -> DEBU 161[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 08:50:44.333 UTC [common/channelconfig] initializeProtosStruct -> DEBU 162[0m Processing field: AnchorPeers
[36m2019-01-29 08:50:44.333 UTC [common/channelconfig] NewStandardValues -> DEBU 163[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:50:44.333 UTC [common/channelconfig] initializeProtosStruct -> DEBU 164[0m Processing field: MSP
[36m2019-01-29 08:50:44.333 UTC [common/channelconfig] Validate -> DEBU 165[0m Anchor peers for org Hospital1MSP are 
[36m2019-01-29 08:50:44.333 UTC [common/channelconfig] validateMSP -> DEBU 166[0m Setting up MSP for org Hospital1MSP
[36m2019-01-29 08:50:44.333 UTC [msp] newBccspMsp -> DEBU 167[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:50:44.333 UTC [msp] New -> DEBU 168[0m Creating Cache-MSP instance
[36m2019-01-29 08:50:44.333 UTC [msp] Setup -> DEBU 169[0m Setting up MSP instance Hospital1MSP
[36m2019-01-29 08:50:44.333 UTC [msp/identity] newIdentity -> DEBU 16a[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdDCCAhugAwIBAgIQZxH8VsnceZxXgKxlURLgVTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMS5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQX9V4k/h/3pFxByKDscjgQrC0YS/bSbKvzSG2Z2PvltEWp40qB9OaKUN0GQYzo
yK2Etdvyjfd1OFwaTjpGSvv9o18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCBVU7DoP9Msf/nNTNl6
5FxwAXZ82eaGBT7zWYdyh4QbBjAKBggqhkjOPQQDAgNHADBEAiBmXGWQbxCGRp0c
5/y9qJyciB1CDHsknRZDjNzt5f563wIgMy7VPi1R8DpIIIRN9B3mfhDpRa+J+8PA
S4DOJy7rjp4=
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.333 UTC [msp/identity] newIdentity -> DEBU 16b[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.334 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 16c[0m Checking if identity satisfies role [CLIENT] for Hospital1MSP
[36m2019-01-29 08:50:44.334 UTC [msp] Validate -> DEBU 16d[0m MSP Hospital1MSP validating identity
[36m2019-01-29 08:50:44.334 UTC [msp] getCertificationChain -> DEBU 16e[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 08:50:44.334 UTC [msp] hasOURole -> DEBU 16f[0m MSP Hospital1MSP checking if the identity is a client
[36m2019-01-29 08:50:44.334 UTC [msp] getCertificationChain -> DEBU 170[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 08:50:44.335 UTC [msp] Setup -> DEBU 171[0m Setting up the MSP manager (4 msps)
[36m2019-01-29 08:50:44.335 UTC [msp] Setup -> DEBU 172[0m MSP manager setup complete, setup 4 msps
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 173[0m Proposed new policy Admins for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 174[0m Proposed new policy Readers for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 175[0m Proposed new policy Writers for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 176[0m Proposed new policy Admins for Channel/Orderer
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 177[0m Proposed new policy Readers for Channel/Orderer
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 178[0m Proposed new policy Writers for Channel/Orderer
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 179[0m Proposed new policy BlockValidation for Channel/Orderer
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 17a[0m Proposed new policy Admins for Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 17b[0m Proposed new policy Readers for Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 17c[0m Proposed new policy Writers for Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 17d[0m Proposed new policy Admins for Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 17e[0m Proposed new policy Readers for Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 17f[0m Proposed new policy Writers for Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 180[0m Proposed new policy Writers for Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 181[0m Proposed new policy Admins for Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 182[0m Proposed new policy Readers for Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 183[0m Proposed new policy ChannelCreationPolicy for Channel/Application
[36m2019-01-29 08:50:44.335 UTC [policies] GetPolicy -> DEBU 184[0m Returning dummy reject all policy because Admins could not be found in Channel/Application/Admins
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 185[0m Proposed new policy Admins for Channel
[36m2019-01-29 08:50:44.335 UTC [policies] GetPolicy -> DEBU 186[0m Returning dummy reject all policy because Readers could not be found in Channel/Application/Readers
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 187[0m Proposed new policy Readers for Channel
[36m2019-01-29 08:50:44.335 UTC [policies] GetPolicy -> DEBU 188[0m Returning dummy reject all policy because Writers could not be found in Channel/Application/Writers
[36m2019-01-29 08:50:44.335 UTC [policies] NewManagerImpl -> DEBU 189[0m Proposed new policy Writers for Channel
[36m2019-01-29 08:50:44.335 UTC [common/configtx] addToMap -> DEBU 18a[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 08:50:44.335 UTC [common/configtx] addToMap -> DEBU 18b[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 08:50:44.335 UTC [common/configtx] addToMap -> DEBU 18c[0m Adding to config map: [Group]  /Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.335 UTC [common/configtx] addToMap -> DEBU 18d[0m Adding to config map: [Value]  /Channel/Application/Hospital3MSP/MSP
[36m2019-01-29 08:50:44.335 UTC [common/configtx] addToMap -> DEBU 18e[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Admins
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 18f[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 190[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Writers
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 191[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 192[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/MSP
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 193[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 194[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 195[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 196[0m Adding to config map: [Group]  /Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 197[0m Adding to config map: [Value]  /Channel/Application/Hospital2MSP/MSP
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 198[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 199[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Writers
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 19a[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Admins
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 19b[0m Adding to config map: [Policy] /Channel/Application/ChannelCreationPolicy
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 19c[0m Adding to config map: [Group]  /Channel/Orderer
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 19d[0m Adding to config map: [Group]  /Channel/Orderer/OrdererOrg
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 19e[0m Adding to config map: [Value]  /Channel/Orderer/OrdererOrg/MSP
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 19f[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 08:50:44.336 UTC [common/configtx] addToMap -> DEBU 1a0[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 08:50:44.337 UTC [common/configtx] addToMap -> DEBU 1a1[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Admins
[36m2019-01-29 08:50:44.337 UTC [common/configtx] addToMap -> DEBU 1a2[0m Adding to config map: [Value]  /Channel/Orderer/ChannelRestrictions
[36m2019-01-29 08:50:44.337 UTC [common/configtx] addToMap -> DEBU 1a3[0m Adding to config map: [Value]  /Channel/Orderer/Capabilities
[36m2019-01-29 08:50:44.337 UTC [common/configtx] addToMap -> DEBU 1a4[0m Adding to config map: [Value]  /Channel/Orderer/KafkaBrokers
[36m2019-01-29 08:50:44.337 UTC [common/configtx] addToMap -> DEBU 1a5[0m Adding to config map: [Value]  /Channel/Orderer/ConsensusType
[36m2019-01-29 08:50:44.337 UTC [common/configtx] addToMap -> DEBU 1a6[0m Adding to config map: [Value]  /Channel/Orderer/BatchSize
[36m2019-01-29 08:50:44.337 UTC [common/configtx] addToMap -> DEBU 1a7[0m Adding to config map: [Value]  /Channel/Orderer/BatchTimeout
[36m2019-01-29 08:50:44.337 UTC [common/configtx] addToMap -> DEBU 1a8[0m Adding to config map: [Policy] /Channel/Orderer/Admins
[36m2019-01-29 08:50:44.337 UTC [common/configtx] addToMap -> DEBU 1a9[0m Adding to config map: [Policy] /Channel/Orderer/Readers
[36m2019-01-29 08:50:44.337 UTC [common/configtx] addToMap -> DEBU 1aa[0m Adding to config map: [Policy] /Channel/Orderer/Writers
[36m2019-01-29 08:50:44.337 UTC [common/configtx] addToMap -> DEBU 1ab[0m Adding to config map: [Policy] /Channel/Orderer/BlockValidation
[36m2019-01-29 08:50:44.337 UTC [common/configtx] addToMap -> DEBU 1ac[0m Adding to config map: [Value]  /Channel/Capabilities
[36m2019-01-29 08:50:44.337 UTC [common/configtx] addToMap -> DEBU 1ad[0m Adding to config map: [Value]  /Channel/Consortium
[36m2019-01-29 08:50:44.337 UTC [common/configtx] addToMap -> DEBU 1ae[0m Adding to config map: [Value]  /Channel/HashingAlgorithm
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1af[0m Adding to config map: [Value]  /Channel/BlockDataHashingStructure
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1b0[0m Adding to config map: [Value]  /Channel/OrdererAddresses
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1b1[0m Adding to config map: [Policy] /Channel/Admins
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1b2[0m Adding to config map: [Policy] /Channel/Readers
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1b3[0m Adding to config map: [Policy] /Channel/Writers
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1b4[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1b5[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1b6[0m Adding to config map: [Group]  /Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1b7[0m Adding to config map: [Group]  /Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1b8[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1b9[0m Adding to config map: [Value]  /Channel/Consortium
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1ba[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1bb[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1bc[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1bd[0m Adding to config map: [Group]  /Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1be[0m Adding to config map: [Group]  /Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1bf[0m Adding to config map: [Value]  /Channel/Application/Capabilities
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1c0[0m Adding to config map: [Policy] /Channel/Application/Readers
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1c1[0m Adding to config map: [Policy] /Channel/Application/Writers
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1c2[0m Adding to config map: [Policy] /Channel/Application/Admins
[36m2019-01-29 08:50:44.338 UTC [common/configtx] addToMap -> DEBU 1c3[0m Adding to config map: [Value]  /Channel/Consortium
[36m2019-01-29 08:50:44.339 UTC [common/configtx] verifyDeltaSet -> DEBU 1c4[0m Processing change to key: [Policy] /Channel/Application/Writers
[36m2019-01-29 08:50:44.339 UTC [common/configtx] verifyDeltaSet -> DEBU 1c5[0m Processing change to key: [Policy] /Channel/Application/Admins
[36m2019-01-29 08:50:44.339 UTC [common/configtx] verifyDeltaSet -> DEBU 1c6[0m Processing change to key: [Policy] /Channel/Application/Readers
[36m2019-01-29 08:50:44.339 UTC [common/configtx] verifyDeltaSet -> DEBU 1c7[0m Processing change to key: [Group]  /Channel/Application
[36m2019-01-29 08:50:44.339 UTC [common/configtx] policyForItem -> DEBU 1c8[0m Getting policy for item Application with mod_policy ChannelCreationPolicy
[36m2019-01-29 08:50:44.339 UTC [policies] Manager -> DEBU 1c9[0m Manager Channel looking up path []
[36m2019-01-29 08:50:44.339 UTC [policies] Manager -> DEBU 1ca[0m Manager Channel has managers Orderer
[36m2019-01-29 08:50:44.339 UTC [policies] Manager -> DEBU 1cb[0m Manager Channel has managers Application
[36m2019-01-29 08:50:44.339 UTC [policies] Manager -> DEBU 1cc[0m Manager Channel looking up path [Application]
[36m2019-01-29 08:50:44.339 UTC [policies] Manager -> DEBU 1cd[0m Manager Channel has managers Application
[36m2019-01-29 08:50:44.339 UTC [policies] Manager -> DEBU 1ce[0m Manager Channel has managers Orderer
[36m2019-01-29 08:50:44.339 UTC [policies] Manager -> DEBU 1cf[0m Manager Channel/Application looking up path []
[36m2019-01-29 08:50:44.339 UTC [policies] Manager -> DEBU 1d0[0m Manager Channel/Application has managers Hospital1MSP
[36m2019-01-29 08:50:44.339 UTC [policies] Manager -> DEBU 1d1[0m Manager Channel/Application has managers Hospital2MSP
[36m2019-01-29 08:50:44.339 UTC [policies] Manager -> DEBU 1d2[0m Manager Channel/Application has managers Hospital3MSP
[36m2019-01-29 08:50:44.339 UTC [policies] Evaluate -> DEBU 1d3[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/ChannelCreationPolicy ==
[36m2019-01-29 08:50:44.339 UTC [policies] Evaluate -> DEBU 1d4[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 08:50:44.339 UTC [policies] Evaluate -> DEBU 1d5[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Admins ==
2019-01-29 08:50:44.339 UTC [msp] DeserializeIdentity -> INFO 1d6[0m Obtaining identity
[36m2019-01-29 08:50:44.339 UTC [msp/identity] newIdentity -> DEBU 1d7[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.340 UTC [cauthdsl] func1 -> DEBU 1d8[0m 0xc4200bcd40 gate 1548751844340135281 evaluation starts
[36m2019-01-29 08:50:44.340 UTC [cauthdsl] func2 -> DEBU 1d9[0m 0xc4200bcd40 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 08:50:44.340 UTC [cauthdsl] func2 -> DEBU 1da[0m 0xc4200bcd40 processing identity 0 with bytes of 0a0c486f73706974616c314d535012df062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943547a4343416657674177494241674951627a7870534b69754a437a4469446c5a702b58625744414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42344d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a45504d4130474131554543784d470a593278705a5735304d5373774b5159445651514444434a425a473170626b426f62334e7761585268624445756333647064474e6f4d6d78765a326c6a4c6d4e760a4c6e70684d466b77457759484b6f5a497a6a3043415159494b6f5a497a6a304441516344516741456e7041446c382b6155426356577636713349674d513479670a49536177513077616d486d7053762b2f62314235686634563248796f50726177497234352b6f342f517a2b2b57617a5a6c795937506b6f54323772374b614e4e0a4d45737744675944565230504151482f42415144416765414d41774741315564457745422f7751434d4141774b7759445652306a42435177496f416756564f770a36442f544c482f357a557a5a6575526363414632664e6e6d6867552b38316d48636f654547775977436759494b6f5a497a6a30454177494453414177525149680a414c6176644d6d3969367853736e57646c6c694d6831307336646d474261584f30437861436f774c4a494d744169424564757564474a796d444445317a6269580a59775838336359575a4855635956384958655a5046796d432b773d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 08:50:44.340 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 1db[0m Checking if identity satisfies ADMIN role for Hospital1MSP
[36m2019-01-29 08:50:44.340 UTC [cauthdsl] func2 -> DEBU 1dc[0m 0xc4200bcd40 principal matched by identity 0
[36m2019-01-29 08:50:44.340 UTC [msp/identity] Verify -> DEBU 1dd[0m Verify: digest = 00000000  c2 e2 96 3f 6f 2c 77 a0  44 fe 58 ee 5d 21 66 77  |...?o,w.D.X.]!fw|
00000010  18 c4 17 16 70 28 fd c9  16 4f 47 d4 52 bc 83 81  |....p(...OG.R...|
[36m2019-01-29 08:50:44.340 UTC [msp/identity] Verify -> DEBU 1de[0m Verify: sig = 00000000  30 45 02 21 00 ee 1e f0  59 8a cb ad 55 c3 a1 a2  |0E.!....Y...U...|
00000010  45 79 46 94 cc 58 e8 b3  b9 30 ae 95 03 2d 88 9d  |EyF..X...0...-..|
00000020  a2 af fc 01 78 02 20 21  9d 14 ce fe 7e 7e 2e 01  |....x. !....~~..|
00000030  f4 76 e7 f5 49 d2 ef c1  43 ab 74 97 08 19 ac 6c  |.v..I...C.t....l|
00000040  04 e1 b6 2e 43 f5 70                              |....C.p|
[36m2019-01-29 08:50:44.340 UTC [cauthdsl] func2 -> DEBU 1df[0m 0xc4200bcd40 principal evaluation succeeds for identity 0
[36m2019-01-29 08:50:44.340 UTC [cauthdsl] func1 -> DEBU 1e0[0m 0xc4200bcd40 gate 1548751844340135281 evaluation succeeds
[36m2019-01-29 08:50:44.340 UTC [policies] Evaluate -> DEBU 1e1[0m Signature set satisfies policy /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 08:50:44.340 UTC [policies] Evaluate -> DEBU 1e2[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 08:50:44.340 UTC [policies] Evaluate -> DEBU 1e3[0m Signature set satisfies policy /Channel/Application/ChannelCreationPolicy
[36m2019-01-29 08:50:44.340 UTC [policies] Evaluate -> DEBU 1e4[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/ChannelCreationPolicy
[36m2019-01-29 08:50:44.340 UTC [common/configtx] verifyDeltaSet -> DEBU 1e5[0m Processing change to key: [Value]  /Channel/Application/Capabilities
[36m2019-01-29 08:50:44.340 UTC [common/configtx] recurseConfigMap -> DEBU 1e6[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\016\022\014\n\nOrdererMSP" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.340 UTC [common/configtx] recurseConfigMap -> DEBU 1e7[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\016\022\014\n\nOrdererMSP" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.340 UTC [common/configtx] recurseConfigMap -> DEBU 1e8[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\nOrdererMSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1e9[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1ea[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1eb[0m Setting policy for key BlockValidation to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1ec[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1ed[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\022\022\020\n\014Hospital3MSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1ee[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital3MSP" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1ef[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital3MSP" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1f0[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\022\022\020\n\014Hospital1MSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1f1[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital1MSP" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1f2[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital1MSP" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1f3[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital2MSP" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1f4[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital2MSP" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1f5[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\022\022\020\n\014Hospital2MSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1f6[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1f7[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1f8[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1f9[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1fa[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [common/configtx] recurseConfigMap -> DEBU 1fb[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.341 UTC [msp] GetDefaultSigningIdentity -> DEBU 1fc[0m Obtaining default signing identity
[36m2019-01-29 08:50:44.342 UTC [msp] GetDefaultSigningIdentity -> DEBU 1fd[0m Obtaining default signing identity
[36m2019-01-29 08:50:44.342 UTC [msp/identity] Sign -> DEBU 1fe[0m Sign: plaintext: 0AFA060A1B08011A0608E4A7C0E20522...8AEB9D844884FFEDC8137EA3EABC1CF1 
[36m2019-01-29 08:50:44.342 UTC [msp/identity] Sign -> DEBU 1ff[0m Sign: digest: E8B28CC2983F8B8D2857093D6D53852355CE11489D911637B447E554201FDE7A 
[36m2019-01-29 08:50:44.342 UTC [msp] GetDefaultSigningIdentity -> DEBU 200[0m Obtaining default signing identity
[36m2019-01-29 08:50:44.342 UTC [msp] GetDefaultSigningIdentity -> DEBU 201[0m Obtaining default signing identity
[36m2019-01-29 08:50:44.342 UTC [msp/identity] Sign -> DEBU 202[0m Sign: plaintext: 0AF6060A1708041A0608E4A7C0E20522...4BB7D1558FE650B032D440CC2A12EE3B 
[36m2019-01-29 08:50:44.342 UTC [msp/identity] Sign -> DEBU 203[0m Sign: digest: 2A23472883EEE5BB2685CEF51D19E66421E29A98E369EC8612236CCFFC8CF4CC 
[36m2019-01-29 08:50:44.342 UTC [policies] Evaluate -> DEBU 204[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Writers ==
[36m2019-01-29 08:50:44.343 UTC [policies] Evaluate -> DEBU 205[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 08:50:44.343 UTC [policies] Evaluate -> DEBU 206[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Orderer/Writers ==
[36m2019-01-29 08:50:44.343 UTC [policies] Evaluate -> DEBU 207[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 08:50:44.343 UTC [policies] Evaluate -> DEBU 208[0m == Evaluating *cauthdsl.policy Policy /Channel/Orderer/OrdererOrg/Writers ==
2019-01-29 08:50:44.343 UTC [msp] DeserializeIdentity -> INFO 209[0m Obtaining identity
[36m2019-01-29 08:50:44.343 UTC [msp/identity] newIdentity -> DEBU 20a[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICLDCCAdKgAwIBAgIQIoN1EApgEjYu/O7unXVqYzAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBqMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEuMCwGA1UEAxMlb3JkZXJlcjAuaG9zcGl0YWwxLnN3
aXRjaDJsb2dpYy5jby56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABH2/gH3f
8uhcO2CKglQg53cAJL79dENZNdRWfaRikU5j3X60lh9Bx5YpznQ5l0qMaCK1Ogec
Nw5LGgie9ESxrHyjTTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsG
A1UdIwQkMCKAIBiGNntD5tBlG9C8QNsdXde3ItTRRa+p/+3yJwRCsbJxMAoGCCqG
SM49BAMCA0gAMEUCIQCBtzCy6LE3S8b/XG08Qcs01A8QjmCwqmZJQrDvRA8ZhgIg
aAwP9tYtEZs1DBv2bH7HwYIwmi8nlgD3+Xp6CWwvAkg=
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.343 UTC [cauthdsl] func1 -> DEBU 20b[0m 0xc4200bce88 gate 1548751844343588782 evaluation starts
[36m2019-01-29 08:50:44.343 UTC [cauthdsl] func2 -> DEBU 20c[0m 0xc4200bce88 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 08:50:44.343 UTC [cauthdsl] func2 -> DEBU 20d[0m 0xc4200bce88 processing identity 0 with bytes of 0a0a4f7264657265724d535012ae062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949434c44434341644b674177494241674951496f4e3145417067456a59752f4f37756e585671597a414b42676771686b6a4f50515144416a42334d5173770a435159445651514745774a56557a45544d4245474131554543424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a790a5957356a61584e6a627a45624d426b474131554543684d536333647064474e6f4d6d78765a326c6a4c6d4e764c6e70684d52347748415944565151444578566a0a5953357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b774d5449354d4463304d5455305768634e4d6a6b774d5449324d4463300a4d545530576a42714d517377435159445651514745774a56557a45544d4245474131554543424d4b5132467361575a76636d3570595445574d425147413155450a42784d4e5532467549455a795957356a61584e6a627a45754d4377474131554541784d6c62334a6b5a584a6c636a41756147397a63476c30595777784c6e4e330a6158526a61444a73623264705979356a627935365954425a4d424d4742797147534d34394167454743437147534d343941774548413049414248322f674833660a387568634f32434b676c5167353363414a4c373964454e5a4e645257666152696b55356a335836306c683942783559707a6e51356c30714d61434b314f6765630a4e77354c47676965394553787248796a5454424c4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d4373470a413155644977516b4d434b41494269474e6e74443574426c47394338514e7364586465334974545252612b702f2b33794a77524373624a784d416f47434371470a534d343942414d43413067414d45554349514342747a4379364c45335338622f5847303851637330314138516a6d4377716d5a4a517244765241385a686749670a6141775039745974455a73314442763262483748775949776d69386e6c6744332b58703643577776416b673d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 08:50:44.343 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 20e[0m Checking if identity satisfies MEMBER role for OrdererMSP
[36m2019-01-29 08:50:44.343 UTC [msp] Validate -> DEBU 20f[0m MSP OrdererMSP validating identity
[36m2019-01-29 08:50:44.344 UTC [cauthdsl] func2 -> DEBU 210[0m 0xc4200bce88 principal matched by identity 0
[36m2019-01-29 08:50:44.395 UTC [msp/identity] Verify -> DEBU 211[0m Verify: digest = 00000000  2a 23 47 28 83 ee e5 bb  26 85 ce f5 1d 19 e6 64  |*#G(....&......d|
00000010  21 e2 9a 98 e3 69 ec 86  12 23 6c cf fc 8c f4 cc  |!....i...#l.....|
[36m2019-01-29 08:50:44.395 UTC [msp/identity] Verify -> DEBU 212[0m Verify: sig = 00000000  30 44 02 20 52 c8 d7 88  71 94 41 ab cc 2c 97 99  |0D. R...q.A..,..|
00000010  8d f5 73 75 0b 08 4d fb  e5 a4 3f e3 ef b2 66 df  |..su..M...?...f.|
00000020  7d 40 e4 ce 02 20 5c a7  2c 88 b0 08 e7 c8 db 6f  |}@... \.,......o|
00000030  d9 98 96 82 14 7f 3b a2  8d c7 97 54 11 33 b9 33  |......;....T.3.3|
00000040  37 bc db e5 82 4a                                 |7....J|
[36m2019-01-29 08:50:44.396 UTC [cauthdsl] func2 -> DEBU 213[0m 0xc4200bce88 principal evaluation succeeds for identity 0
[36m2019-01-29 08:50:44.396 UTC [cauthdsl] func1 -> DEBU 214[0m 0xc4200bce88 gate 1548751844343588782 evaluation succeeds
[36m2019-01-29 08:50:44.396 UTC [policies] Evaluate -> DEBU 215[0m Signature set satisfies policy /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 08:50:44.396 UTC [policies] Evaluate -> DEBU 216[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 08:50:44.396 UTC [policies] Evaluate -> DEBU 217[0m Signature set satisfies policy /Channel/Orderer/Writers
[36m2019-01-29 08:50:44.396 UTC [policies] Evaluate -> DEBU 218[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Orderer/Writers
[36m2019-01-29 08:50:44.396 UTC [policies] Evaluate -> DEBU 219[0m Signature set satisfies policy /Channel/Writers
[36m2019-01-29 08:50:44.396 UTC [policies] Evaluate -> DEBU 21a[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Writers
[36m2019-01-29 08:50:44.396 UTC [common/channelconfig] NewStandardValues -> DEBU 21b[0m Initializing protos for *channelconfig.ChannelProtos
[36m2019-01-29 08:50:44.396 UTC [common/channelconfig] initializeProtosStruct -> DEBU 21c[0m Processing field: HashingAlgorithm
[36m2019-01-29 08:50:44.396 UTC [common/channelconfig] initializeProtosStruct -> DEBU 21d[0m Processing field: BlockDataHashingStructure
[36m2019-01-29 08:50:44.396 UTC [common/channelconfig] initializeProtosStruct -> DEBU 21e[0m Processing field: OrdererAddresses
[36m2019-01-29 08:50:44.396 UTC [common/channelconfig] initializeProtosStruct -> DEBU 21f[0m Processing field: Consortium
[36m2019-01-29 08:50:44.396 UTC [common/channelconfig] initializeProtosStruct -> DEBU 220[0m Processing field: Capabilities
[36m2019-01-29 08:50:44.396 UTC [common/channelconfig] NewStandardValues -> DEBU 221[0m Initializing protos for *channelconfig.OrdererProtos
[36m2019-01-29 08:50:44.396 UTC [common/channelconfig] initializeProtosStruct -> DEBU 222[0m Processing field: ConsensusType
[36m2019-01-29 08:50:44.396 UTC [common/channelconfig] initializeProtosStruct -> DEBU 223[0m Processing field: BatchSize
[36m2019-01-29 08:50:44.396 UTC [common/channelconfig] initializeProtosStruct -> DEBU 224[0m Processing field: BatchTimeout
[36m2019-01-29 08:50:44.396 UTC [common/channelconfig] initializeProtosStruct -> DEBU 225[0m Processing field: KafkaBrokers
[36m2019-01-29 08:50:44.396 UTC [common/channelconfig] initializeProtosStruct -> DEBU 226[0m Processing field: ChannelRestrictions
[36m2019-01-29 08:50:44.396 UTC [common/channelconfig] initializeProtosStruct -> DEBU 227[0m Processing field: Capabilities
[36m2019-01-29 08:50:44.397 UTC [common/channelconfig] NewStandardValues -> DEBU 228[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:50:44.397 UTC [common/channelconfig] initializeProtosStruct -> DEBU 229[0m Processing field: MSP
[36m2019-01-29 08:50:44.397 UTC [common/channelconfig] validateMSP -> DEBU 22a[0m Setting up MSP for org OrdererOrg
[36m2019-01-29 08:50:44.397 UTC [msp] newBccspMsp -> DEBU 22b[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:50:44.397 UTC [msp] New -> DEBU 22c[0m Creating Cache-MSP instance
[36m2019-01-29 08:50:44.397 UTC [msp] Setup -> DEBU 22d[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 08:50:44.397 UTC [msp/identity] newIdentity -> DEBU 22e[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.397 UTC [msp/identity] newIdentity -> DEBU 22f[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.397 UTC [msp] Validate -> DEBU 230[0m MSP OrdererMSP validating identity
[36m2019-01-29 08:50:44.398 UTC [common/channelconfig] NewStandardValues -> DEBU 231[0m Initializing protos for *channelconfig.ApplicationProtos
[36m2019-01-29 08:50:44.398 UTC [common/channelconfig] initializeProtosStruct -> DEBU 232[0m Processing field: ACLs
[36m2019-01-29 08:50:44.398 UTC [common/channelconfig] initializeProtosStruct -> DEBU 233[0m Processing field: Capabilities
[36m2019-01-29 08:50:44.398 UTC [common/channelconfig] NewStandardValues -> DEBU 234[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 08:50:44.398 UTC [common/channelconfig] initializeProtosStruct -> DEBU 235[0m Processing field: AnchorPeers
[36m2019-01-29 08:50:44.398 UTC [common/channelconfig] NewStandardValues -> DEBU 236[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:50:44.398 UTC [common/channelconfig] initializeProtosStruct -> DEBU 237[0m Processing field: MSP
[36m2019-01-29 08:50:44.398 UTC [common/channelconfig] Validate -> DEBU 238[0m Anchor peers for org Hospital2MSP are 
[36m2019-01-29 08:50:44.398 UTC [common/channelconfig] validateMSP -> DEBU 239[0m Setting up MSP for org Hospital2MSP
[36m2019-01-29 08:50:44.398 UTC [msp] newBccspMsp -> DEBU 23a[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:50:44.398 UTC [msp] New -> DEBU 23b[0m Creating Cache-MSP instance
[36m2019-01-29 08:50:44.398 UTC [msp] Setup -> DEBU 23c[0m Setting up MSP instance Hospital2MSP
[36m2019-01-29 08:50:44.398 UTC [msp/identity] newIdentity -> DEBU 23d[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdjCCAhygAwIBAgIRALZ3Pffn/xWUtUWPSK6CYVAwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowgYsxCzAJBgNVBAYTAlVTMRMwEQYD
VQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4gRnJhbmNpc2NvMSUwIwYDVQQK
Exxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMSgwJgYDVQQDEx9jYS5ob3Nw
aXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcD
QgAESzVpiz6P4mIb+KwvrttcXJKZR5i137Is4298E7OrkYdAg0tB4rpfKTXzUMwW
vI12cTDujpyaZSiqQ3/zP4fOQqNfMF0wDgYDVR0PAQH/BAQDAgGmMA8GA1UdJQQI
MAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQg8QJqWKPaX24HINwR
S6sINFSwRD+440/ccSnvFVaGF24wCgYIKoZIzj0EAwIDSAAwRQIhAPO2Nng3/nqb
cKPliUqVPVjBMu+D55M90+jexWSOpkjcAiADPx8qv9exZ4BfP3+G49Qzpx0Meblj
53vzUgYFa/0EXw==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.399 UTC [msp/identity] newIdentity -> DEBU 23e[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAJ6XtHMTTW1JzdpD54qCc3wwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA+z1rb9aA1HwHfNLi9V9VZe
bdwT4MXak445zGz4DXzRUsmNeBngzntKb6qpgHQpeFtzbzLx+zwiynRDvfdzORGj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPEC
alij2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQC
IA8A3CCJeCasyy7gYd/M1aHYmwWsecY7UY7cVUGy4XOxAiB0rrmfnGW6JA11PDCu
+5c5Rpn0jF5ddsFn+UIpzBFvmg==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.400 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 23f[0m Checking if identity satisfies role [CLIENT] for Hospital2MSP
[36m2019-01-29 08:50:44.400 UTC [msp] Validate -> DEBU 240[0m MSP Hospital2MSP validating identity
[36m2019-01-29 08:50:44.400 UTC [msp] getCertificationChain -> DEBU 241[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 08:50:44.400 UTC [msp] hasOURole -> DEBU 242[0m MSP Hospital2MSP checking if the identity is a client
[36m2019-01-29 08:50:44.400 UTC [msp] getCertificationChain -> DEBU 243[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 08:50:44.400 UTC [common/channelconfig] NewStandardValues -> DEBU 244[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 08:50:44.400 UTC [common/channelconfig] initializeProtosStruct -> DEBU 245[0m Processing field: AnchorPeers
[36m2019-01-29 08:50:44.400 UTC [common/channelconfig] NewStandardValues -> DEBU 246[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:50:44.401 UTC [common/channelconfig] initializeProtosStruct -> DEBU 247[0m Processing field: MSP
[36m2019-01-29 08:50:44.401 UTC [common/channelconfig] Validate -> DEBU 248[0m Anchor peers for org Hospital3MSP are 
[36m2019-01-29 08:50:44.401 UTC [common/channelconfig] validateMSP -> DEBU 249[0m Setting up MSP for org Hospital3MSP
[36m2019-01-29 08:50:44.401 UTC [msp] newBccspMsp -> DEBU 24a[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:50:44.401 UTC [msp] New -> DEBU 24b[0m Creating Cache-MSP instance
[36m2019-01-29 08:50:44.401 UTC [msp] Setup -> DEBU 24c[0m Setting up MSP instance Hospital3MSP
[36m2019-01-29 08:50:44.401 UTC [msp/identity] newIdentity -> DEBU 24d[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdTCCAhugAwIBAgIQCCXeHGgObIJo8aq4AdvgVjAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMy5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQm1Oo/AEu7ZZToE15Y5ntlaN128Hs/exX3jNH5nO6r4tvyFtBBNEn1ecXxxPtm
dmQ+nPHONorLwDC42SMAKwfro18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCA7vVWFXWpgZBojQ1ZL
xUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEAlOUoPtpfO8V7
vFRCanjU5zZWGO5ajFlPDJjjDTy3UrwCICrUYV32JrR/m3dslCl15dSThHrKtYCC
Ehvyw1+lNqiG
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.402 UTC [msp/identity] newIdentity -> DEBU 24e[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAI8BA/5Jf+5m8ncI+D7aSiMwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwzLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABF1IehIKIjh8tXYZ7rw4QOuc
g/tpBAkJeOt44zr/LepbjhyFW8vIRnkw75i6Wcg+LOoAS+2NFNg2oNjqg//yCHWj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIDu9
VYVdamBkGiNDVkvFS+AEt6Jrn0yK8hFyPGZhqenJMAoGCCqGSM49BAMCA0cAMEQC
IFyc85pwbCAxGNdw4fflsX2I3HrP3h93YAB2zSDC8HtkAiAQxblAZfEnXUiQKrqE
8I10bYJqD6pJz1jb3Pv+4xplTA==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.403 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 24f[0m Checking if identity satisfies role [CLIENT] for Hospital3MSP
[36m2019-01-29 08:50:44.403 UTC [msp] Validate -> DEBU 250[0m MSP Hospital3MSP validating identity
[36m2019-01-29 08:50:44.403 UTC [msp] getCertificationChain -> DEBU 251[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 08:50:44.403 UTC [msp] hasOURole -> DEBU 252[0m MSP Hospital3MSP checking if the identity is a client
[36m2019-01-29 08:50:44.403 UTC [msp] getCertificationChain -> DEBU 253[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 08:50:44.403 UTC [common/channelconfig] NewStandardValues -> DEBU 254[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 08:50:44.403 UTC [common/channelconfig] initializeProtosStruct -> DEBU 255[0m Processing field: AnchorPeers
[36m2019-01-29 08:50:44.403 UTC [common/channelconfig] NewStandardValues -> DEBU 256[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:50:44.403 UTC [common/channelconfig] initializeProtosStruct -> DEBU 257[0m Processing field: MSP
[36m2019-01-29 08:50:44.403 UTC [common/channelconfig] Validate -> DEBU 258[0m Anchor peers for org Hospital1MSP are 
[36m2019-01-29 08:50:44.403 UTC [common/channelconfig] validateMSP -> DEBU 259[0m Setting up MSP for org Hospital1MSP
[36m2019-01-29 08:50:44.403 UTC [msp] newBccspMsp -> DEBU 25a[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:50:44.403 UTC [msp] New -> DEBU 25b[0m Creating Cache-MSP instance
[36m2019-01-29 08:50:44.403 UTC [msp] Setup -> DEBU 25c[0m Setting up MSP instance Hospital1MSP
[36m2019-01-29 08:50:44.404 UTC [msp/identity] newIdentity -> DEBU 25d[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdDCCAhugAwIBAgIQZxH8VsnceZxXgKxlURLgVTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMS5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQX9V4k/h/3pFxByKDscjgQrC0YS/bSbKvzSG2Z2PvltEWp40qB9OaKUN0GQYzo
yK2Etdvyjfd1OFwaTjpGSvv9o18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCBVU7DoP9Msf/nNTNl6
5FxwAXZ82eaGBT7zWYdyh4QbBjAKBggqhkjOPQQDAgNHADBEAiBmXGWQbxCGRp0c
5/y9qJyciB1CDHsknRZDjNzt5f563wIgMy7VPi1R8DpIIIRN9B3mfhDpRa+J+8PA
S4DOJy7rjp4=
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.404 UTC [msp/identity] newIdentity -> DEBU 25e[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.404 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 25f[0m Checking if identity satisfies role [CLIENT] for Hospital1MSP
[36m2019-01-29 08:50:44.404 UTC [msp] Validate -> DEBU 260[0m MSP Hospital1MSP validating identity
[36m2019-01-29 08:50:44.405 UTC [msp] getCertificationChain -> DEBU 261[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 08:50:44.405 UTC [msp] hasOURole -> DEBU 262[0m MSP Hospital1MSP checking if the identity is a client
[36m2019-01-29 08:50:44.405 UTC [msp] getCertificationChain -> DEBU 263[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 08:50:44.405 UTC [msp] Setup -> DEBU 264[0m Setting up the MSP manager (4 msps)
[36m2019-01-29 08:50:44.405 UTC [msp] Setup -> DEBU 265[0m MSP manager setup complete, setup 4 msps
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 266[0m Proposed new policy Writers for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 267[0m Proposed new policy Admins for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 268[0m Proposed new policy Readers for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 269[0m Proposed new policy Readers for Channel/Orderer
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 26a[0m Proposed new policy Writers for Channel/Orderer
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 26b[0m Proposed new policy BlockValidation for Channel/Orderer
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 26c[0m Proposed new policy Admins for Channel/Orderer
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 26d[0m Proposed new policy Admins for Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 26e[0m Proposed new policy Readers for Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 26f[0m Proposed new policy Writers for Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 270[0m Proposed new policy Writers for Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 271[0m Proposed new policy Admins for Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 272[0m Proposed new policy Readers for Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 273[0m Proposed new policy Admins for Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 274[0m Proposed new policy Readers for Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 275[0m Proposed new policy Writers for Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 276[0m Proposed new policy ChannelCreationPolicy for Channel/Application
[36m2019-01-29 08:50:44.405 UTC [policies] GetPolicy -> DEBU 277[0m Returning dummy reject all policy because Readers could not be found in Channel/Application/Readers
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 278[0m Proposed new policy Readers for Channel
[36m2019-01-29 08:50:44.405 UTC [policies] GetPolicy -> DEBU 279[0m Returning dummy reject all policy because Writers could not be found in Channel/Application/Writers
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 27a[0m Proposed new policy Writers for Channel
[36m2019-01-29 08:50:44.405 UTC [policies] GetPolicy -> DEBU 27b[0m Returning dummy reject all policy because Admins could not be found in Channel/Application/Admins
[36m2019-01-29 08:50:44.405 UTC [policies] NewManagerImpl -> DEBU 27c[0m Proposed new policy Admins for Channel
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 27d[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 27e[0m Adding to config map: [Group]  /Channel/Orderer
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 27f[0m Adding to config map: [Group]  /Channel/Orderer/OrdererOrg
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 280[0m Adding to config map: [Value]  /Channel/Orderer/OrdererOrg/MSP
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 281[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 282[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Admins
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 283[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 284[0m Adding to config map: [Value]  /Channel/Orderer/BatchSize
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 285[0m Adding to config map: [Value]  /Channel/Orderer/BatchTimeout
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 286[0m Adding to config map: [Value]  /Channel/Orderer/ChannelRestrictions
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 287[0m Adding to config map: [Value]  /Channel/Orderer/Capabilities
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 288[0m Adding to config map: [Value]  /Channel/Orderer/KafkaBrokers
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 289[0m Adding to config map: [Value]  /Channel/Orderer/ConsensusType
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 28a[0m Adding to config map: [Policy] /Channel/Orderer/Readers
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 28b[0m Adding to config map: [Policy] /Channel/Orderer/Writers
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 28c[0m Adding to config map: [Policy] /Channel/Orderer/BlockValidation
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 28d[0m Adding to config map: [Policy] /Channel/Orderer/Admins
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 28e[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 28f[0m Adding to config map: [Group]  /Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 290[0m Adding to config map: [Value]  /Channel/Application/Hospital3MSP/MSP
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 291[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Admins
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 292[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 293[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Writers
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 294[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 295[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/MSP
[36m2019-01-29 08:50:44.406 UTC [common/configtx] addToMap -> DEBU 296[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 297[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 298[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 299[0m Adding to config map: [Group]  /Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 29a[0m Adding to config map: [Value]  /Channel/Application/Hospital2MSP/MSP
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 29b[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Admins
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 29c[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 29d[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Writers
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 29e[0m Adding to config map: [Policy] /Channel/Application/ChannelCreationPolicy
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 29f[0m Adding to config map: [Value]  /Channel/HashingAlgorithm
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2a0[0m Adding to config map: [Value]  /Channel/BlockDataHashingStructure
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2a1[0m Adding to config map: [Value]  /Channel/OrdererAddresses
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2a2[0m Adding to config map: [Value]  /Channel/Capabilities
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2a3[0m Adding to config map: [Value]  /Channel/Consortium
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2a4[0m Adding to config map: [Policy] /Channel/Readers
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2a5[0m Adding to config map: [Policy] /Channel/Writers
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2a6[0m Adding to config map: [Policy] /Channel/Admins
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2a7[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2a8[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2a9[0m Adding to config map: [Group]  /Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2aa[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2ab[0m Adding to config map: [Group]  /Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2ac[0m Adding to config map: [Value]  /Channel/Consortium
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2ad[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2ae[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2af[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2b0[0m Adding to config map: [Group]  /Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2b1[0m Adding to config map: [Group]  /Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2b2[0m Adding to config map: [Value]  /Channel/Application/Capabilities
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2b3[0m Adding to config map: [Policy] /Channel/Application/Writers
[36m2019-01-29 08:50:44.407 UTC [common/configtx] addToMap -> DEBU 2b4[0m Adding to config map: [Policy] /Channel/Application/Admins
[36m2019-01-29 08:50:44.408 UTC [common/configtx] addToMap -> DEBU 2b5[0m Adding to config map: [Policy] /Channel/Application/Readers
[36m2019-01-29 08:50:44.408 UTC [common/configtx] addToMap -> DEBU 2b6[0m Adding to config map: [Value]  /Channel/Consortium
[36m2019-01-29 08:50:44.408 UTC [common/configtx] verifyDeltaSet -> DEBU 2b7[0m Processing change to key: [Value]  /Channel/Application/Capabilities
[36m2019-01-29 08:50:44.408 UTC [common/configtx] verifyDeltaSet -> DEBU 2b8[0m Processing change to key: [Policy] /Channel/Application/Readers
[36m2019-01-29 08:50:44.408 UTC [common/configtx] verifyDeltaSet -> DEBU 2b9[0m Processing change to key: [Policy] /Channel/Application/Admins
[36m2019-01-29 08:50:44.408 UTC [common/configtx] verifyDeltaSet -> DEBU 2ba[0m Processing change to key: [Group]  /Channel/Application
[36m2019-01-29 08:50:44.408 UTC [common/configtx] policyForItem -> DEBU 2bb[0m Getting policy for item Application with mod_policy ChannelCreationPolicy
[36m2019-01-29 08:50:44.408 UTC [policies] Manager -> DEBU 2bc[0m Manager Channel looking up path []
[36m2019-01-29 08:50:44.408 UTC [policies] Manager -> DEBU 2bd[0m Manager Channel has managers Orderer
[36m2019-01-29 08:50:44.408 UTC [policies] Manager -> DEBU 2be[0m Manager Channel has managers Application
[36m2019-01-29 08:50:44.408 UTC [policies] Manager -> DEBU 2bf[0m Manager Channel looking up path [Application]
[36m2019-01-29 08:50:44.408 UTC [policies] Manager -> DEBU 2c0[0m Manager Channel has managers Orderer
[36m2019-01-29 08:50:44.408 UTC [policies] Manager -> DEBU 2c1[0m Manager Channel has managers Application
[36m2019-01-29 08:50:44.408 UTC [policies] Manager -> DEBU 2c2[0m Manager Channel/Application looking up path []
[36m2019-01-29 08:50:44.408 UTC [policies] Manager -> DEBU 2c3[0m Manager Channel/Application has managers Hospital2MSP
[36m2019-01-29 08:50:44.408 UTC [policies] Manager -> DEBU 2c4[0m Manager Channel/Application has managers Hospital3MSP
[36m2019-01-29 08:50:44.408 UTC [policies] Manager -> DEBU 2c5[0m Manager Channel/Application has managers Hospital1MSP
[36m2019-01-29 08:50:44.408 UTC [policies] Evaluate -> DEBU 2c6[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/ChannelCreationPolicy ==
[36m2019-01-29 08:50:44.408 UTC [policies] Evaluate -> DEBU 2c7[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 08:50:44.408 UTC [policies] Evaluate -> DEBU 2c8[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Admins ==
2019-01-29 08:50:44.408 UTC [msp] DeserializeIdentity -> INFO 2c9[0m Obtaining identity
[36m2019-01-29 08:50:44.408 UTC [msp/identity] newIdentity -> DEBU 2ca[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.408 UTC [cauthdsl] func1 -> DEBU 2cb[0m 0xc4200bca18 gate 1548751844408873598 evaluation starts
[36m2019-01-29 08:50:44.408 UTC [cauthdsl] func2 -> DEBU 2cc[0m 0xc4200bca18 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 08:50:44.408 UTC [cauthdsl] func2 -> DEBU 2cd[0m 0xc4200bca18 processing identity 0 with bytes of 0a0c486f73706974616c314d535012df062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943547a4343416657674177494241674951627a7870534b69754a437a4469446c5a702b58625744414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42344d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a45504d4130474131554543784d470a593278705a5735304d5373774b5159445651514444434a425a473170626b426f62334e7761585268624445756333647064474e6f4d6d78765a326c6a4c6d4e760a4c6e70684d466b77457759484b6f5a497a6a3043415159494b6f5a497a6a304441516344516741456e7041446c382b6155426356577636713349674d513479670a49536177513077616d486d7053762b2f62314235686634563248796f50726177497234352b6f342f517a2b2b57617a5a6c795937506b6f54323772374b614e4e0a4d45737744675944565230504151482f42415144416765414d41774741315564457745422f7751434d4141774b7759445652306a42435177496f416756564f770a36442f544c482f357a557a5a6575526363414632664e6e6d6867552b38316d48636f654547775977436759494b6f5a497a6a30454177494453414177525149680a414c6176644d6d3969367853736e57646c6c694d6831307336646d474261584f30437861436f774c4a494d744169424564757564474a796d444445317a6269580a59775838336359575a4855635956384958655a5046796d432b773d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 08:50:44.408 UTC [cauthdsl] func2 -> DEBU 2ce[0m 0xc4200bca18 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital2MSP, got Hospital1MSP)
[36m2019-01-29 08:50:44.409 UTC [cauthdsl] func2 -> DEBU 2cf[0m 0xc4200bca18 principal evaluation fails
[36m2019-01-29 08:50:44.409 UTC [cauthdsl] func1 -> DEBU 2d0[0m 0xc4200bca18 gate 1548751844408873598 evaluation fails
[36m2019-01-29 08:50:44.409 UTC [policies] Evaluate -> DEBU 2d1[0m Signature set did not satisfy policy /Channel/Application/Hospital2MSP/Admins
[36m2019-01-29 08:50:44.409 UTC [policies] Evaluate -> DEBU 2d2[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Admins
[36m2019-01-29 08:50:44.409 UTC [policies] Evaluate -> DEBU 2d3[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Admins ==
[36m2019-01-29 08:50:44.409 UTC [cauthdsl] func1 -> DEBU 2d4[0m 0xc4200bca30 gate 1548751844409084439 evaluation starts
[36m2019-01-29 08:50:44.409 UTC [cauthdsl] func2 -> DEBU 2d5[0m 0xc4200bca30 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 08:50:44.409 UTC [cauthdsl] func2 -> DEBU 2d6[0m 0xc4200bca30 processing identity 0 with bytes of 0a0c486f73706974616c314d535012df062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943547a4343416657674177494241674951627a7870534b69754a437a4469446c5a702b58625744414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42344d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a45504d4130474131554543784d470a593278705a5735304d5373774b5159445651514444434a425a473170626b426f62334e7761585268624445756333647064474e6f4d6d78765a326c6a4c6d4e760a4c6e70684d466b77457759484b6f5a497a6a3043415159494b6f5a497a6a304441516344516741456e7041446c382b6155426356577636713349674d513479670a49536177513077616d486d7053762b2f62314235686634563248796f50726177497234352b6f342f517a2b2b57617a5a6c795937506b6f54323772374b614e4e0a4d45737744675944565230504151482f42415144416765414d41774741315564457745422f7751434d4141774b7759445652306a42435177496f416756564f770a36442f544c482f357a557a5a6575526363414632664e6e6d6867552b38316d48636f654547775977436759494b6f5a497a6a30454177494453414177525149680a414c6176644d6d3969367853736e57646c6c694d6831307336646d474261584f30437861436f774c4a494d744169424564757564474a796d444445317a6269580a59775838336359575a4855635956384958655a5046796d432b773d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 08:50:44.409 UTC [cauthdsl] func2 -> DEBU 2d7[0m 0xc4200bca30 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital3MSP, got Hospital1MSP)
[36m2019-01-29 08:50:44.409 UTC [cauthdsl] func2 -> DEBU 2d8[0m 0xc4200bca30 principal evaluation fails
[36m2019-01-29 08:50:44.409 UTC [cauthdsl] func1 -> DEBU 2d9[0m 0xc4200bca30 gate 1548751844409084439 evaluation fails
[36m2019-01-29 08:50:44.409 UTC [policies] Evaluate -> DEBU 2da[0m Signature set did not satisfy policy /Channel/Application/Hospital3MSP/Admins
[36m2019-01-29 08:50:44.409 UTC [policies] Evaluate -> DEBU 2db[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Admins
[36m2019-01-29 08:50:44.409 UTC [policies] Evaluate -> DEBU 2dc[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Admins ==
[36m2019-01-29 08:50:44.409 UTC [cauthdsl] func1 -> DEBU 2dd[0m 0xc4200bca40 gate 1548751844409298172 evaluation starts
[36m2019-01-29 08:50:44.409 UTC [cauthdsl] func2 -> DEBU 2de[0m 0xc4200bca40 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 08:50:44.409 UTC [cauthdsl] func2 -> DEBU 2df[0m 0xc4200bca40 processing identity 0 with bytes of 0a0c486f73706974616c314d535012df062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943547a4343416657674177494241674951627a7870534b69754a437a4469446c5a702b58625744414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42344d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a45504d4130474131554543784d470a593278705a5735304d5373774b5159445651514444434a425a473170626b426f62334e7761585268624445756333647064474e6f4d6d78765a326c6a4c6d4e760a4c6e70684d466b77457759484b6f5a497a6a3043415159494b6f5a497a6a304441516344516741456e7041446c382b6155426356577636713349674d513479670a49536177513077616d486d7053762b2f62314235686634563248796f50726177497234352b6f342f517a2b2b57617a5a6c795937506b6f54323772374b614e4e0a4d45737744675944565230504151482f42415144416765414d41774741315564457745422f7751434d4141774b7759445652306a42435177496f416756564f770a36442f544c482f357a557a5a6575526363414632664e6e6d6867552b38316d48636f654547775977436759494b6f5a497a6a30454177494453414177525149680a414c6176644d6d3969367853736e57646c6c694d6831307336646d474261584f30437861436f774c4a494d744169424564757564474a796d444445317a6269580a59775838336359575a4855635956384958655a5046796d432b773d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 08:50:44.409 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 2e0[0m Checking if identity satisfies ADMIN role for Hospital1MSP
[36m2019-01-29 08:50:44.409 UTC [cauthdsl] func2 -> DEBU 2e1[0m 0xc4200bca40 principal matched by identity 0
[36m2019-01-29 08:50:44.409 UTC [msp/identity] Verify -> DEBU 2e2[0m Verify: digest = 00000000  c2 e2 96 3f 6f 2c 77 a0  44 fe 58 ee 5d 21 66 77  |...?o,w.D.X.]!fw|
00000010  18 c4 17 16 70 28 fd c9  16 4f 47 d4 52 bc 83 81  |....p(...OG.R...|
[36m2019-01-29 08:50:44.409 UTC [msp/identity] Verify -> DEBU 2e3[0m Verify: sig = 00000000  30 45 02 21 00 ee 1e f0  59 8a cb ad 55 c3 a1 a2  |0E.!....Y...U...|
00000010  45 79 46 94 cc 58 e8 b3  b9 30 ae 95 03 2d 88 9d  |EyF..X...0...-..|
00000020  a2 af fc 01 78 02 20 21  9d 14 ce fe 7e 7e 2e 01  |....x. !....~~..|
00000030  f4 76 e7 f5 49 d2 ef c1  43 ab 74 97 08 19 ac 6c  |.v..I...C.t....l|
00000040  04 e1 b6 2e 43 f5 70                              |....C.p|
[36m2019-01-29 08:50:44.409 UTC [cauthdsl] func2 -> DEBU 2e4[0m 0xc4200bca40 principal evaluation succeeds for identity 0
[36m2019-01-29 08:50:44.409 UTC [cauthdsl] func1 -> DEBU 2e5[0m 0xc4200bca40 gate 1548751844409298172 evaluation succeeds
[36m2019-01-29 08:50:44.409 UTC [policies] Evaluate -> DEBU 2e6[0m Signature set satisfies policy /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 08:50:44.409 UTC [policies] Evaluate -> DEBU 2e7[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 08:50:44.409 UTC [policies] Evaluate -> DEBU 2e8[0m Signature set satisfies policy /Channel/Application/ChannelCreationPolicy
[36m2019-01-29 08:50:44.409 UTC [policies] Evaluate -> DEBU 2e9[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/ChannelCreationPolicy
[36m2019-01-29 08:50:44.409 UTC [common/configtx] verifyDeltaSet -> DEBU 2ea[0m Processing change to key: [Policy] /Channel/Application/Writers
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2eb[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\nOrdererMSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2ec[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\016\022\014\n\nOrdererMSP" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2ed[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\016\022\014\n\nOrdererMSP" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2ee[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2ef[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2f0[0m Setting policy for key BlockValidation to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2f1[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2f2[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital2MSP" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2f3[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\022\022\020\n\014Hospital2MSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2f4[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital2MSP" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2f5[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\022\022\020\n\014Hospital3MSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2f6[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital3MSP" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2f7[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital3MSP" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2f8[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital1MSP" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2f9[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\022\022\020\n\014Hospital1MSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2fa[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital1MSP" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2fb[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2fc[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.410 UTC [common/configtx] recurseConfigMap -> DEBU 2fd[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.411 UTC [common/configtx] recurseConfigMap -> DEBU 2fe[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.411 UTC [common/configtx] recurseConfigMap -> DEBU 2ff[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.411 UTC [common/configtx] recurseConfigMap -> DEBU 300[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 08:50:44.411 UTC [common/channelconfig] NewStandardValues -> DEBU 301[0m Initializing protos for *channelconfig.ChannelProtos
[36m2019-01-29 08:50:44.411 UTC [common/channelconfig] initializeProtosStruct -> DEBU 302[0m Processing field: HashingAlgorithm
[36m2019-01-29 08:50:44.411 UTC [common/channelconfig] initializeProtosStruct -> DEBU 303[0m Processing field: BlockDataHashingStructure
[36m2019-01-29 08:50:44.411 UTC [common/channelconfig] initializeProtosStruct -> DEBU 304[0m Processing field: OrdererAddresses
[36m2019-01-29 08:50:44.411 UTC [common/channelconfig] initializeProtosStruct -> DEBU 305[0m Processing field: Consortium
[36m2019-01-29 08:50:44.411 UTC [common/channelconfig] initializeProtosStruct -> DEBU 306[0m Processing field: Capabilities
[36m2019-01-29 08:50:44.411 UTC [common/channelconfig] NewStandardValues -> DEBU 307[0m Initializing protos for *channelconfig.OrdererProtos
[36m2019-01-29 08:50:44.411 UTC [common/channelconfig] initializeProtosStruct -> DEBU 308[0m Processing field: ConsensusType
[36m2019-01-29 08:50:44.411 UTC [common/channelconfig] initializeProtosStruct -> DEBU 309[0m Processing field: BatchSize
[36m2019-01-29 08:50:44.411 UTC [common/channelconfig] initializeProtosStruct -> DEBU 30a[0m Processing field: BatchTimeout
[36m2019-01-29 08:50:44.411 UTC [common/channelconfig] initializeProtosStruct -> DEBU 30b[0m Processing field: KafkaBrokers
[36m2019-01-29 08:50:44.411 UTC [common/channelconfig] initializeProtosStruct -> DEBU 30c[0m Processing field: ChannelRestrictions
[36m2019-01-29 08:50:44.411 UTC [common/channelconfig] initializeProtosStruct -> DEBU 30d[0m Processing field: Capabilities
[36m2019-01-29 08:50:44.411 UTC [common/channelconfig] NewStandardValues -> DEBU 30e[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:50:44.411 UTC [common/channelconfig] initializeProtosStruct -> DEBU 30f[0m Processing field: MSP
[36m2019-01-29 08:50:44.411 UTC [common/channelconfig] validateMSP -> DEBU 310[0m Setting up MSP for org OrdererOrg
[36m2019-01-29 08:50:44.411 UTC [msp] newBccspMsp -> DEBU 311[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:50:44.411 UTC [msp] New -> DEBU 312[0m Creating Cache-MSP instance
[36m2019-01-29 08:50:44.411 UTC [msp] Setup -> DEBU 313[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 08:50:44.412 UTC [msp/identity] newIdentity -> DEBU 314[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.412 UTC [msp/identity] newIdentity -> DEBU 315[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.412 UTC [msp] Validate -> DEBU 316[0m MSP OrdererMSP validating identity
[36m2019-01-29 08:50:44.413 UTC [common/channelconfig] NewStandardValues -> DEBU 317[0m Initializing protos for *channelconfig.ApplicationProtos
[36m2019-01-29 08:50:44.413 UTC [common/channelconfig] initializeProtosStruct -> DEBU 318[0m Processing field: ACLs
[36m2019-01-29 08:50:44.413 UTC [common/channelconfig] initializeProtosStruct -> DEBU 319[0m Processing field: Capabilities
[36m2019-01-29 08:50:44.413 UTC [common/channelconfig] NewStandardValues -> DEBU 31a[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 08:50:44.413 UTC [common/channelconfig] initializeProtosStruct -> DEBU 31b[0m Processing field: AnchorPeers
[36m2019-01-29 08:50:44.413 UTC [common/channelconfig] NewStandardValues -> DEBU 31c[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:50:44.413 UTC [common/channelconfig] initializeProtosStruct -> DEBU 31d[0m Processing field: MSP
[36m2019-01-29 08:50:44.413 UTC [common/channelconfig] Validate -> DEBU 31e[0m Anchor peers for org Hospital1MSP are 
[36m2019-01-29 08:50:44.413 UTC [common/channelconfig] validateMSP -> DEBU 31f[0m Setting up MSP for org Hospital1MSP
[36m2019-01-29 08:50:44.413 UTC [msp] newBccspMsp -> DEBU 320[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:50:44.413 UTC [msp] New -> DEBU 321[0m Creating Cache-MSP instance
[36m2019-01-29 08:50:44.413 UTC [msp] Setup -> DEBU 322[0m Setting up MSP instance Hospital1MSP
[36m2019-01-29 08:50:44.413 UTC [msp/identity] newIdentity -> DEBU 323[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdDCCAhugAwIBAgIQZxH8VsnceZxXgKxlURLgVTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMS5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQX9V4k/h/3pFxByKDscjgQrC0YS/bSbKvzSG2Z2PvltEWp40qB9OaKUN0GQYzo
yK2Etdvyjfd1OFwaTjpGSvv9o18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCBVU7DoP9Msf/nNTNl6
5FxwAXZ82eaGBT7zWYdyh4QbBjAKBggqhkjOPQQDAgNHADBEAiBmXGWQbxCGRp0c
5/y9qJyciB1CDHsknRZDjNzt5f563wIgMy7VPi1R8DpIIIRN9B3mfhDpRa+J+8PA
S4DOJy7rjp4=
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.413 UTC [msp/identity] newIdentity -> DEBU 324[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.414 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 325[0m Checking if identity satisfies role [CLIENT] for Hospital1MSP
[36m2019-01-29 08:50:44.414 UTC [msp] Validate -> DEBU 326[0m MSP Hospital1MSP validating identity
[36m2019-01-29 08:50:44.414 UTC [msp] getCertificationChain -> DEBU 327[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 08:50:44.414 UTC [msp] hasOURole -> DEBU 328[0m MSP Hospital1MSP checking if the identity is a client
[36m2019-01-29 08:50:44.414 UTC [msp] getCertificationChain -> DEBU 329[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 08:50:44.415 UTC [common/channelconfig] NewStandardValues -> DEBU 32a[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 08:50:44.415 UTC [common/channelconfig] initializeProtosStruct -> DEBU 32b[0m Processing field: AnchorPeers
[36m2019-01-29 08:50:44.415 UTC [common/channelconfig] NewStandardValues -> DEBU 32c[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:50:44.415 UTC [common/channelconfig] initializeProtosStruct -> DEBU 32d[0m Processing field: MSP
[36m2019-01-29 08:50:44.415 UTC [common/channelconfig] Validate -> DEBU 32e[0m Anchor peers for org Hospital2MSP are 
[36m2019-01-29 08:50:44.415 UTC [common/channelconfig] validateMSP -> DEBU 32f[0m Setting up MSP for org Hospital2MSP
[36m2019-01-29 08:50:44.415 UTC [msp] newBccspMsp -> DEBU 330[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:50:44.415 UTC [msp] New -> DEBU 331[0m Creating Cache-MSP instance
[36m2019-01-29 08:50:44.415 UTC [msp] Setup -> DEBU 332[0m Setting up MSP instance Hospital2MSP
[36m2019-01-29 08:50:44.415 UTC [msp/identity] newIdentity -> DEBU 333[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdjCCAhygAwIBAgIRALZ3Pffn/xWUtUWPSK6CYVAwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowgYsxCzAJBgNVBAYTAlVTMRMwEQYD
VQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4gRnJhbmNpc2NvMSUwIwYDVQQK
Exxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMSgwJgYDVQQDEx9jYS5ob3Nw
aXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcD
QgAESzVpiz6P4mIb+KwvrttcXJKZR5i137Is4298E7OrkYdAg0tB4rpfKTXzUMwW
vI12cTDujpyaZSiqQ3/zP4fOQqNfMF0wDgYDVR0PAQH/BAQDAgGmMA8GA1UdJQQI
MAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQg8QJqWKPaX24HINwR
S6sINFSwRD+440/ccSnvFVaGF24wCgYIKoZIzj0EAwIDSAAwRQIhAPO2Nng3/nqb
cKPliUqVPVjBMu+D55M90+jexWSOpkjcAiADPx8qv9exZ4BfP3+G49Qzpx0Meblj
53vzUgYFa/0EXw==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.415 UTC [msp/identity] newIdentity -> DEBU 334[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAJ6XtHMTTW1JzdpD54qCc3wwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA+z1rb9aA1HwHfNLi9V9VZe
bdwT4MXak445zGz4DXzRUsmNeBngzntKb6qpgHQpeFtzbzLx+zwiynRDvfdzORGj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPEC
alij2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQC
IA8A3CCJeCasyy7gYd/M1aHYmwWsecY7UY7cVUGy4XOxAiB0rrmfnGW6JA11PDCu
+5c5Rpn0jF5ddsFn+UIpzBFvmg==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.416 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 335[0m Checking if identity satisfies role [CLIENT] for Hospital2MSP
[36m2019-01-29 08:50:44.416 UTC [msp] Validate -> DEBU 336[0m MSP Hospital2MSP validating identity
[36m2019-01-29 08:50:44.416 UTC [msp] getCertificationChain -> DEBU 337[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 08:50:44.416 UTC [msp] hasOURole -> DEBU 338[0m MSP Hospital2MSP checking if the identity is a client
[36m2019-01-29 08:50:44.416 UTC [msp] getCertificationChain -> DEBU 339[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 08:50:44.416 UTC [common/channelconfig] NewStandardValues -> DEBU 33a[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 08:50:44.416 UTC [common/channelconfig] initializeProtosStruct -> DEBU 33b[0m Processing field: AnchorPeers
[36m2019-01-29 08:50:44.416 UTC [common/channelconfig] NewStandardValues -> DEBU 33c[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:50:44.417 UTC [common/channelconfig] initializeProtosStruct -> DEBU 33d[0m Processing field: MSP
[36m2019-01-29 08:50:44.417 UTC [common/channelconfig] Validate -> DEBU 33e[0m Anchor peers for org Hospital3MSP are 
[36m2019-01-29 08:50:44.417 UTC [common/channelconfig] validateMSP -> DEBU 33f[0m Setting up MSP for org Hospital3MSP
[36m2019-01-29 08:50:44.417 UTC [msp] newBccspMsp -> DEBU 340[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:50:44.417 UTC [msp] New -> DEBU 341[0m Creating Cache-MSP instance
[36m2019-01-29 08:50:44.417 UTC [msp] Setup -> DEBU 342[0m Setting up MSP instance Hospital3MSP
[36m2019-01-29 08:50:44.417 UTC [msp/identity] newIdentity -> DEBU 343[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdTCCAhugAwIBAgIQCCXeHGgObIJo8aq4AdvgVjAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMy5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQm1Oo/AEu7ZZToE15Y5ntlaN128Hs/exX3jNH5nO6r4tvyFtBBNEn1ecXxxPtm
dmQ+nPHONorLwDC42SMAKwfro18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCA7vVWFXWpgZBojQ1ZL
xUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEAlOUoPtpfO8V7
vFRCanjU5zZWGO5ajFlPDJjjDTy3UrwCICrUYV32JrR/m3dslCl15dSThHrKtYCC
Ehvyw1+lNqiG
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.417 UTC [msp/identity] newIdentity -> DEBU 344[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAI8BA/5Jf+5m8ncI+D7aSiMwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwzLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABF1IehIKIjh8tXYZ7rw4QOuc
g/tpBAkJeOt44zr/LepbjhyFW8vIRnkw75i6Wcg+LOoAS+2NFNg2oNjqg//yCHWj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIDu9
VYVdamBkGiNDVkvFS+AEt6Jrn0yK8hFyPGZhqenJMAoGCCqGSM49BAMCA0cAMEQC
IFyc85pwbCAxGNdw4fflsX2I3HrP3h93YAB2zSDC8HtkAiAQxblAZfEnXUiQKrqE
8I10bYJqD6pJz1jb3Pv+4xplTA==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.417 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 345[0m Checking if identity satisfies role [CLIENT] for Hospital3MSP
[36m2019-01-29 08:50:44.417 UTC [msp] Validate -> DEBU 346[0m MSP Hospital3MSP validating identity
[36m2019-01-29 08:50:44.418 UTC [msp] getCertificationChain -> DEBU 347[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 08:50:44.418 UTC [msp] hasOURole -> DEBU 348[0m MSP Hospital3MSP checking if the identity is a client
[36m2019-01-29 08:50:44.418 UTC [msp] getCertificationChain -> DEBU 349[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 08:50:44.418 UTC [msp] Setup -> DEBU 34a[0m Setting up the MSP manager (4 msps)
[36m2019-01-29 08:50:44.418 UTC [msp] Setup -> DEBU 34b[0m MSP manager setup complete, setup 4 msps
[36m2019-01-29 08:50:44.418 UTC [policies] NewManagerImpl -> DEBU 34c[0m Proposed new policy Readers for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:50:44.418 UTC [policies] NewManagerImpl -> DEBU 34d[0m Proposed new policy Writers for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:50:44.418 UTC [policies] NewManagerImpl -> DEBU 34e[0m Proposed new policy Admins for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:50:44.418 UTC [policies] NewManagerImpl -> DEBU 34f[0m Proposed new policy Readers for Channel/Orderer
[36m2019-01-29 08:50:44.418 UTC [policies] NewManagerImpl -> DEBU 350[0m Proposed new policy Writers for Channel/Orderer
[36m2019-01-29 08:50:44.418 UTC [policies] NewManagerImpl -> DEBU 351[0m Proposed new policy BlockValidation for Channel/Orderer
[36m2019-01-29 08:50:44.418 UTC [policies] NewManagerImpl -> DEBU 352[0m Proposed new policy Admins for Channel/Orderer
[36m2019-01-29 08:50:44.418 UTC [policies] NewManagerImpl -> DEBU 353[0m Proposed new policy Writers for Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.418 UTC [policies] NewManagerImpl -> DEBU 354[0m Proposed new policy Admins for Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.418 UTC [policies] NewManagerImpl -> DEBU 355[0m Proposed new policy Readers for Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.418 UTC [policies] NewManagerImpl -> DEBU 356[0m Proposed new policy Readers for Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.418 UTC [policies] NewManagerImpl -> DEBU 357[0m Proposed new policy Writers for Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.418 UTC [policies] NewManagerImpl -> DEBU 358[0m Proposed new policy Admins for Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.418 UTC [policies] NewManagerImpl -> DEBU 359[0m Proposed new policy Writers for Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.418 UTC [policies] NewManagerImpl -> DEBU 35a[0m Proposed new policy Admins for Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.418 UTC [policies] NewManagerImpl -> DEBU 35b[0m Proposed new policy Readers for Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.419 UTC [policies] NewManagerImpl -> DEBU 35c[0m Proposed new policy Readers for Channel/Application
[36m2019-01-29 08:50:44.419 UTC [policies] NewManagerImpl -> DEBU 35d[0m Proposed new policy Writers for Channel/Application
[36m2019-01-29 08:50:44.419 UTC [policies] NewManagerImpl -> DEBU 35e[0m Proposed new policy Admins for Channel/Application
[36m2019-01-29 08:50:44.419 UTC [policies] NewManagerImpl -> DEBU 35f[0m Proposed new policy Admins for Channel
[36m2019-01-29 08:50:44.419 UTC [policies] NewManagerImpl -> DEBU 360[0m Proposed new policy Readers for Channel
[36m2019-01-29 08:50:44.419 UTC [policies] NewManagerImpl -> DEBU 361[0m Proposed new policy Writers for Channel
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 362[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 363[0m Adding to config map: [Group]  /Channel/Orderer
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 364[0m Adding to config map: [Group]  /Channel/Orderer/OrdererOrg
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 365[0m Adding to config map: [Value]  /Channel/Orderer/OrdererOrg/MSP
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 366[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 367[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Admins
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 368[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 369[0m Adding to config map: [Value]  /Channel/Orderer/BatchSize
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 36a[0m Adding to config map: [Value]  /Channel/Orderer/BatchTimeout
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 36b[0m Adding to config map: [Value]  /Channel/Orderer/ChannelRestrictions
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 36c[0m Adding to config map: [Value]  /Channel/Orderer/Capabilities
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 36d[0m Adding to config map: [Value]  /Channel/Orderer/KafkaBrokers
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 36e[0m Adding to config map: [Value]  /Channel/Orderer/ConsensusType
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 36f[0m Adding to config map: [Policy] /Channel/Orderer/Writers
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 370[0m Adding to config map: [Policy] /Channel/Orderer/BlockValidation
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 371[0m Adding to config map: [Policy] /Channel/Orderer/Admins
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 372[0m Adding to config map: [Policy] /Channel/Orderer/Readers
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 373[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 374[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 375[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/MSP
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 376[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 377[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 378[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 379[0m Adding to config map: [Group]  /Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.419 UTC [common/configtx] addToMap -> DEBU 37a[0m Adding to config map: [Value]  /Channel/Application/Hospital2MSP/MSP
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 37b[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Admins
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 37c[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 37d[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Writers
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 37e[0m Adding to config map: [Group]  /Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 37f[0m Adding to config map: [Value]  /Channel/Application/Hospital3MSP/MSP
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 380[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Writers
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 381[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Admins
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 382[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 383[0m Adding to config map: [Value]  /Channel/Application/Capabilities
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 384[0m Adding to config map: [Policy] /Channel/Application/Admins
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 385[0m Adding to config map: [Policy] /Channel/Application/Readers
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 386[0m Adding to config map: [Policy] /Channel/Application/Writers
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 387[0m Adding to config map: [Value]  /Channel/HashingAlgorithm
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 388[0m Adding to config map: [Value]  /Channel/BlockDataHashingStructure
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 389[0m Adding to config map: [Value]  /Channel/OrdererAddresses
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 38a[0m Adding to config map: [Value]  /Channel/Capabilities
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 38b[0m Adding to config map: [Value]  /Channel/Consortium
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 38c[0m Adding to config map: [Policy] /Channel/Admins
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 38d[0m Adding to config map: [Policy] /Channel/Readers
[36m2019-01-29 08:50:44.420 UTC [common/configtx] addToMap -> DEBU 38e[0m Adding to config map: [Policy] /Channel/Writers
[36m2019-01-29 08:50:44.420 UTC [common/capabilities] Supported -> DEBU 38f[0m Orderer capability V1_1 is supported and is enabled
[36m2019-01-29 08:50:44.420 UTC [common/capabilities] Supported -> DEBU 390[0m Channel capability V1_1 is supported and is enabled
[36m2019-01-29 08:50:44.420 UTC [orderer/consensus/kafka] enqueue -> DEBU 391[0m [channel: testchainid] Enqueueing envelope...
[36m2019-01-29 08:50:44.421 UTC [orderer/consensus/kafka/sarama] handleResponse -> DEBU 392[0m producer/broker/1 state change to [closing] because EOF
[36m2019-01-29 08:50:44.421 UTC [orderer/consensus/kafka/sarama] handleError -> DEBU 393[0m Closed connection to broker kafka1.switch2logic.co.za:9092
[36m2019-01-29 08:50:44.421 UTC [orderer/consensus/kafka/sarama] dispatch -> DEBU 394[0m producer/leader/testchainid/0 state change to [retrying-1]
[36m2019-01-29 08:50:44.421 UTC [orderer/consensus/kafka/sarama] dispatch -> DEBU 395[0m producer/leader/testchainid/0 abandoning broker 1
[36m2019-01-29 08:50:44.421 UTC [orderer/consensus/kafka/sarama] run -> DEBU 396[0m producer/broker/1 shut down
[36m2019-01-29 08:50:44.522 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 397[0m client/metadata fetching metadata for [testchainid] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:50:44.524 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 398[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:50:44.524 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU 399[0m producer/broker/1 starting up
[36m2019-01-29 08:50:44.524 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU 39a[0m producer/broker/1 state change to [open] on testchainid/0
[36m2019-01-29 08:50:44.524 UTC [orderer/consensus/kafka/sarama] dispatch)-fm -> DEBU 39b[0m producer/leader/testchainid/0 selected broker 1
[36m2019-01-29 08:50:44.524 UTC [orderer/consensus/kafka/sarama] dispatch -> DEBU 39c[0m producer/leader/testchainid/0 state change to [flushing-1]
[36m2019-01-29 08:50:44.524 UTC [orderer/consensus/kafka/sarama] dispatch -> DEBU 39d[0m producer/leader/testchainid/0 state change to [normal]
[36m2019-01-29 08:50:44.525 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 39e[0m Connected to broker at kafka1.switch2logic.co.za:9092 (registered as #1)
[36m2019-01-29 08:50:44.533 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 39f[0m [channel: testchainid] Successfully unmarshalled consumed message, offset is 5. Inspecting type...
[36m2019-01-29 08:50:44.533 UTC [orderer/consensus/kafka] processRegular -> DEBU 3a0[0m [channel: testchainid] Processing regular Kafka message of type CONFIG
[36m2019-01-29 08:50:44.533 UTC [orderer/consensus/kafka] func2 -> DEBU 3a1[0m [channel: testchainid] Received config message
[36m2019-01-29 08:50:44.533 UTC [orderer/consensus/kafka] func2 -> DEBU 3a2[0m [channel: testchainid] Creating isolated block for config message
[36m2019-01-29 08:50:44.534 UTC [common/channelconfig] NewStandardValues -> DEBU 3a3[0m Initializing protos for *channelconfig.ChannelProtos
[36m2019-01-29 08:50:44.534 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3a4[0m Processing field: HashingAlgorithm
[36m2019-01-29 08:50:44.534 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3a5[0m Processing field: BlockDataHashingStructure
[36m2019-01-29 08:50:44.534 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3a6[0m Processing field: OrdererAddresses
[36m2019-01-29 08:50:44.534 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3a7[0m Processing field: Consortium
[36m2019-01-29 08:50:44.534 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3a8[0m Processing field: Capabilities
[36m2019-01-29 08:50:44.534 UTC [common/channelconfig] NewStandardValues -> DEBU 3a9[0m Initializing protos for *channelconfig.ApplicationProtos
[36m2019-01-29 08:50:44.534 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3aa[0m Processing field: ACLs
[36m2019-01-29 08:50:44.534 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3ab[0m Processing field: Capabilities
[36m2019-01-29 08:50:44.534 UTC [common/channelconfig] NewStandardValues -> DEBU 3ac[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 08:50:44.534 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3ad[0m Processing field: AnchorPeers
[36m2019-01-29 08:50:44.534 UTC [common/channelconfig] NewStandardValues -> DEBU 3ae[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:50:44.534 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3af[0m Processing field: MSP
[36m2019-01-29 08:50:44.534 UTC [common/channelconfig] Validate -> DEBU 3b0[0m Anchor peers for org Hospital3MSP are 
[36m2019-01-29 08:50:44.534 UTC [common/channelconfig] validateMSP -> DEBU 3b1[0m Setting up MSP for org Hospital3MSP
[36m2019-01-29 08:50:44.534 UTC [msp] newBccspMsp -> DEBU 3b2[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:50:44.534 UTC [msp] New -> DEBU 3b3[0m Creating Cache-MSP instance
[36m2019-01-29 08:50:44.534 UTC [msp] Setup -> DEBU 3b4[0m Setting up MSP instance Hospital3MSP
[36m2019-01-29 08:50:44.535 UTC [msp/identity] newIdentity -> DEBU 3b5[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdTCCAhugAwIBAgIQCCXeHGgObIJo8aq4AdvgVjAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMy5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQm1Oo/AEu7ZZToE15Y5ntlaN128Hs/exX3jNH5nO6r4tvyFtBBNEn1ecXxxPtm
dmQ+nPHONorLwDC42SMAKwfro18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCA7vVWFXWpgZBojQ1ZL
xUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEAlOUoPtpfO8V7
vFRCanjU5zZWGO5ajFlPDJjjDTy3UrwCICrUYV32JrR/m3dslCl15dSThHrKtYCC
Ehvyw1+lNqiG
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.535 UTC [msp/identity] newIdentity -> DEBU 3b6[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAI8BA/5Jf+5m8ncI+D7aSiMwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwzLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABF1IehIKIjh8tXYZ7rw4QOuc
g/tpBAkJeOt44zr/LepbjhyFW8vIRnkw75i6Wcg+LOoAS+2NFNg2oNjqg//yCHWj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIDu9
VYVdamBkGiNDVkvFS+AEt6Jrn0yK8hFyPGZhqenJMAoGCCqGSM49BAMCA0cAMEQC
IFyc85pwbCAxGNdw4fflsX2I3HrP3h93YAB2zSDC8HtkAiAQxblAZfEnXUiQKrqE
8I10bYJqD6pJz1jb3Pv+4xplTA==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.536 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 3b7[0m Checking if identity satisfies role [CLIENT] for Hospital3MSP
[36m2019-01-29 08:50:44.536 UTC [msp] Validate -> DEBU 3b8[0m MSP Hospital3MSP validating identity
[36m2019-01-29 08:50:44.536 UTC [msp] getCertificationChain -> DEBU 3b9[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 08:50:44.536 UTC [msp] hasOURole -> DEBU 3ba[0m MSP Hospital3MSP checking if the identity is a client
[36m2019-01-29 08:50:44.536 UTC [msp] getCertificationChain -> DEBU 3bb[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 08:50:44.537 UTC [common/channelconfig] NewStandardValues -> DEBU 3bc[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 08:50:44.537 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3bd[0m Processing field: AnchorPeers
[36m2019-01-29 08:50:44.537 UTC [common/channelconfig] NewStandardValues -> DEBU 3be[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:50:44.537 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3bf[0m Processing field: MSP
[36m2019-01-29 08:50:44.537 UTC [common/channelconfig] Validate -> DEBU 3c0[0m Anchor peers for org Hospital1MSP are 
[36m2019-01-29 08:50:44.537 UTC [common/channelconfig] validateMSP -> DEBU 3c1[0m Setting up MSP for org Hospital1MSP
[36m2019-01-29 08:50:44.537 UTC [msp] newBccspMsp -> DEBU 3c2[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:50:44.537 UTC [msp] New -> DEBU 3c3[0m Creating Cache-MSP instance
[36m2019-01-29 08:50:44.537 UTC [msp] Setup -> DEBU 3c4[0m Setting up MSP instance Hospital1MSP
[36m2019-01-29 08:50:44.537 UTC [orderer/consensus/kafka] enqueue -> DEBU 3c5[0m [channel: testchainid] Envelope enqueued successfully
[36m2019-01-29 08:50:44.537 UTC [orderer/common/broadcast] Handle -> DEBU 3c6[0m [channel: comunitychannel] Broadcast has successfully enqueued message of type CONFIG_UPDATE from 10.0.0.183:55294
[36m2019-01-29 08:50:44.537 UTC [msp/identity] newIdentity -> DEBU 3c7[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdDCCAhugAwIBAgIQZxH8VsnceZxXgKxlURLgVTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMS5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQX9V4k/h/3pFxByKDscjgQrC0YS/bSbKvzSG2Z2PvltEWp40qB9OaKUN0GQYzo
yK2Etdvyjfd1OFwaTjpGSvv9o18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCBVU7DoP9Msf/nNTNl6
5FxwAXZ82eaGBT7zWYdyh4QbBjAKBggqhkjOPQQDAgNHADBEAiBmXGWQbxCGRp0c
5/y9qJyciB1CDHsknRZDjNzt5f563wIgMy7VPi1R8DpIIIRN9B3mfhDpRa+J+8PA
S4DOJy7rjp4=
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.537 UTC [msp/identity] newIdentity -> DEBU 3c8[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.538 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 3c9[0m Checking if identity satisfies role [CLIENT] for Hospital1MSP
[36m2019-01-29 08:50:44.538 UTC [msp] Validate -> DEBU 3ca[0m MSP Hospital1MSP validating identity
[36m2019-01-29 08:50:44.539 UTC [msp] getCertificationChain -> DEBU 3cb[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 08:50:44.539 UTC [msp] hasOURole -> DEBU 3cc[0m MSP Hospital1MSP checking if the identity is a client
[36m2019-01-29 08:50:44.539 UTC [msp] getCertificationChain -> DEBU 3cd[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 08:50:44.540 UTC [common/channelconfig] NewStandardValues -> DEBU 3ce[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 08:50:44.540 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3cf[0m Processing field: AnchorPeers
[36m2019-01-29 08:50:44.540 UTC [common/channelconfig] NewStandardValues -> DEBU 3d0[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:50:44.540 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3d1[0m Processing field: MSP
[36m2019-01-29 08:50:44.540 UTC [common/channelconfig] Validate -> DEBU 3d2[0m Anchor peers for org Hospital2MSP are 
[36m2019-01-29 08:50:44.540 UTC [common/channelconfig] validateMSP -> DEBU 3d3[0m Setting up MSP for org Hospital2MSP
[36m2019-01-29 08:50:44.540 UTC [msp] newBccspMsp -> DEBU 3d4[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:50:44.540 UTC [msp] New -> DEBU 3d5[0m Creating Cache-MSP instance
[36m2019-01-29 08:50:44.540 UTC [msp] Setup -> DEBU 3d6[0m Setting up MSP instance Hospital2MSP
[36m2019-01-29 08:50:44.540 UTC [msp/identity] newIdentity -> DEBU 3d7[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdjCCAhygAwIBAgIRALZ3Pffn/xWUtUWPSK6CYVAwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowgYsxCzAJBgNVBAYTAlVTMRMwEQYD
VQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4gRnJhbmNpc2NvMSUwIwYDVQQK
Exxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMSgwJgYDVQQDEx9jYS5ob3Nw
aXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcD
QgAESzVpiz6P4mIb+KwvrttcXJKZR5i137Is4298E7OrkYdAg0tB4rpfKTXzUMwW
vI12cTDujpyaZSiqQ3/zP4fOQqNfMF0wDgYDVR0PAQH/BAQDAgGmMA8GA1UdJQQI
MAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQg8QJqWKPaX24HINwR
S6sINFSwRD+440/ccSnvFVaGF24wCgYIKoZIzj0EAwIDSAAwRQIhAPO2Nng3/nqb
cKPliUqVPVjBMu+D55M90+jexWSOpkjcAiADPx8qv9exZ4BfP3+G49Qzpx0Meblj
53vzUgYFa/0EXw==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.541 UTC [msp/identity] newIdentity -> DEBU 3d8[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAJ6XtHMTTW1JzdpD54qCc3wwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA+z1rb9aA1HwHfNLi9V9VZe
bdwT4MXak445zGz4DXzRUsmNeBngzntKb6qpgHQpeFtzbzLx+zwiynRDvfdzORGj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPEC
alij2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQC
IA8A3CCJeCasyy7gYd/M1aHYmwWsecY7UY7cVUGy4XOxAiB0rrmfnGW6JA11PDCu
+5c5Rpn0jF5ddsFn+UIpzBFvmg==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.541 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 3d9[0m Checking if identity satisfies role [CLIENT] for Hospital2MSP
[36m2019-01-29 08:50:44.541 UTC [msp] Validate -> DEBU 3da[0m MSP Hospital2MSP validating identity
[36m2019-01-29 08:50:44.542 UTC [orderer/common/broadcast] Handle -> DEBU 3db[0m Received EOF from 10.0.0.183:55294, hangup
[36m2019-01-29 08:50:44.542 UTC [orderer/common/server] func1 -> DEBU 3dc[0m Closing Broadcast stream
[36m2019-01-29 08:50:44.542 UTC [msp] getCertificationChain -> DEBU 3dd[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 08:50:44.543 UTC [msp] hasOURole -> DEBU 3de[0m MSP Hospital2MSP checking if the identity is a client
[36m2019-01-29 08:50:44.543 UTC [msp] getCertificationChain -> DEBU 3df[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 08:50:44.544 UTC [common/channelconfig] NewStandardValues -> DEBU 3e0[0m Initializing protos for *channelconfig.OrdererProtos
[36m2019-01-29 08:50:44.544 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3e1[0m Processing field: ConsensusType
[36m2019-01-29 08:50:44.544 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3e2[0m Processing field: BatchSize
[36m2019-01-29 08:50:44.544 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3e3[0m Processing field: BatchTimeout
[36m2019-01-29 08:50:44.544 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3e4[0m Processing field: KafkaBrokers
[36m2019-01-29 08:50:44.544 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3e5[0m Processing field: ChannelRestrictions
[36m2019-01-29 08:50:44.544 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3e6[0m Processing field: Capabilities
[36m2019-01-29 08:50:44.544 UTC [common/channelconfig] NewStandardValues -> DEBU 3e7[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 08:50:44.544 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3e8[0m Processing field: MSP
[36m2019-01-29 08:50:44.544 UTC [common/channelconfig] validateMSP -> DEBU 3e9[0m Setting up MSP for org OrdererOrg
[36m2019-01-29 08:50:44.544 UTC [msp] newBccspMsp -> DEBU 3ea[0m Creating BCCSP-based MSP instance
[36m2019-01-29 08:50:44.544 UTC [msp] New -> DEBU 3eb[0m Creating Cache-MSP instance
[36m2019-01-29 08:50:44.544 UTC [msp] Setup -> DEBU 3ec[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 08:50:44.545 UTC [msp/identity] newIdentity -> DEBU 3ed[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.545 UTC [msp/identity] newIdentity -> DEBU 3ee[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:44.546 UTC [msp] Validate -> DEBU 3ef[0m MSP OrdererMSP validating identity
[36m2019-01-29 08:50:44.546 UTC [msp] Setup -> DEBU 3f0[0m Setting up the MSP manager (4 msps)
[36m2019-01-29 08:50:44.546 UTC [msp] Setup -> DEBU 3f1[0m MSP manager setup complete, setup 4 msps
[36m2019-01-29 08:50:44.546 UTC [policies] NewManagerImpl -> DEBU 3f2[0m Proposed new policy Admins for Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 3f3[0m Proposed new policy Readers for Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 3f4[0m Proposed new policy Writers for Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 3f5[0m Proposed new policy Readers for Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 3f6[0m Proposed new policy Writers for Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 3f7[0m Proposed new policy Admins for Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 3f8[0m Proposed new policy Writers for Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 3f9[0m Proposed new policy Admins for Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 3fa[0m Proposed new policy Readers for Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 3fb[0m Proposed new policy Writers for Channel/Application
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 3fc[0m Proposed new policy Admins for Channel/Application
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 3fd[0m Proposed new policy Readers for Channel/Application
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 3fe[0m Proposed new policy Admins for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 3ff[0m Proposed new policy Readers for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 400[0m Proposed new policy Writers for Channel/Orderer/OrdererOrg
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 401[0m Proposed new policy Writers for Channel/Orderer
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 402[0m Proposed new policy BlockValidation for Channel/Orderer
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 403[0m Proposed new policy Admins for Channel/Orderer
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 404[0m Proposed new policy Readers for Channel/Orderer
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 405[0m Proposed new policy Readers for Channel
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 406[0m Proposed new policy Writers for Channel
[36m2019-01-29 08:50:44.547 UTC [policies] NewManagerImpl -> DEBU 407[0m Proposed new policy Admins for Channel
[36m2019-01-29 08:50:44.547 UTC [common/configtx] addToMap -> DEBU 408[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 08:50:44.547 UTC [common/configtx] addToMap -> DEBU 409[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 08:50:44.547 UTC [common/configtx] addToMap -> DEBU 40a[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 40b[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/MSP
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 40c[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 40d[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 40e[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 40f[0m Adding to config map: [Group]  /Channel/Application/Hospital2MSP
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 410[0m Adding to config map: [Value]  /Channel/Application/Hospital2MSP/MSP
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 411[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 412[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Writers
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 413[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Admins
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 414[0m Adding to config map: [Group]  /Channel/Application/Hospital3MSP
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 415[0m Adding to config map: [Value]  /Channel/Application/Hospital3MSP/MSP
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 416[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Admins
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 417[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 418[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Writers
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 419[0m Adding to config map: [Value]  /Channel/Application/Capabilities
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 41a[0m Adding to config map: [Policy] /Channel/Application/Readers
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 41b[0m Adding to config map: [Policy] /Channel/Application/Writers
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 41c[0m Adding to config map: [Policy] /Channel/Application/Admins
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 41d[0m Adding to config map: [Group]  /Channel/Orderer
[36m2019-01-29 08:50:44.548 UTC [common/configtx] addToMap -> DEBU 41e[0m Adding to config map: [Group]  /Channel/Orderer/OrdererOrg
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 41f[0m Adding to config map: [Value]  /Channel/Orderer/OrdererOrg/MSP
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 420[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 421[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Admins
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 422[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 423[0m Adding to config map: [Value]  /Channel/Orderer/KafkaBrokers
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 424[0m Adding to config map: [Value]  /Channel/Orderer/ConsensusType
[36m2019-01-29 08:50:44.549 UTC [common/deliver] deliverBlocks -> DEBU 425[0m Rejecting deliver for 10.0.0.183:55292 because channel comunitychannel not found
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 426[0m Adding to config map: [Value]  /Channel/Orderer/BatchSize
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 427[0m Adding to config map: [Value]  /Channel/Orderer/BatchTimeout
[36m2019-01-29 08:50:44.549 UTC [common/deliver] Handle -> DEBU 428[0m Waiting for new SeekInfo from 10.0.0.183:55292
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 429[0m Adding to config map: [Value]  /Channel/Orderer/ChannelRestrictions
[36m2019-01-29 08:50:44.549 UTC [common/deliver] Handle -> DEBU 42a[0m Attempting to read seek info message from 10.0.0.183:55292
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 42b[0m Adding to config map: [Value]  /Channel/Orderer/Capabilities
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 42c[0m Adding to config map: [Policy] /Channel/Orderer/Writers
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 42d[0m Adding to config map: [Policy] /Channel/Orderer/BlockValidation
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 42e[0m Adding to config map: [Policy] /Channel/Orderer/Admins
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 42f[0m Adding to config map: [Policy] /Channel/Orderer/Readers
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 430[0m Adding to config map: [Value]  /Channel/Consortium
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 431[0m Adding to config map: [Value]  /Channel/HashingAlgorithm
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 432[0m Adding to config map: [Value]  /Channel/BlockDataHashingStructure
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 433[0m Adding to config map: [Value]  /Channel/OrdererAddresses
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 434[0m Adding to config map: [Value]  /Channel/Capabilities
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 435[0m Adding to config map: [Policy] /Channel/Readers
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 436[0m Adding to config map: [Policy] /Channel/Writers
[36m2019-01-29 08:50:44.549 UTC [common/configtx] addToMap -> DEBU 437[0m Adding to config map: [Policy] /Channel/Admins
[36m2019-01-29 08:50:44.549 UTC [common/channelconfig] LogSanityChecks -> DEBU 438[0m As expected, current configuration has policy '/Channel/Readers'
[36m2019-01-29 08:50:44.549 UTC [common/channelconfig] LogSanityChecks -> DEBU 439[0m As expected, current configuration has policy '/Channel/Writers'
[36m2019-01-29 08:50:44.550 UTC [policies] Manager -> DEBU 43a[0m Manager Channel looking up path [Application]
[36m2019-01-29 08:50:44.550 UTC [policies] Manager -> DEBU 43b[0m Manager Channel has managers Orderer
[36m2019-01-29 08:50:44.550 UTC [policies] Manager -> DEBU 43c[0m Manager Channel has managers Application
[36m2019-01-29 08:50:44.550 UTC [policies] Manager -> DEBU 43d[0m Manager Channel/Application looking up path []
[36m2019-01-29 08:50:44.550 UTC [policies] Manager -> DEBU 43e[0m Manager Channel/Application has managers Hospital1MSP
[36m2019-01-29 08:50:44.550 UTC [policies] Manager -> DEBU 43f[0m Manager Channel/Application has managers Hospital2MSP
[36m2019-01-29 08:50:44.550 UTC [policies] Manager -> DEBU 440[0m Manager Channel/Application has managers Hospital3MSP
[36m2019-01-29 08:50:44.550 UTC [common/channelconfig] LogSanityChecks -> DEBU 441[0m As expected, current configuration has policy '/Channel/Application/Readers'
[36m2019-01-29 08:50:44.550 UTC [common/channelconfig] LogSanityChecks -> DEBU 442[0m As expected, current configuration has policy '/Channel/Application/Writers'
[36m2019-01-29 08:50:44.550 UTC [common/channelconfig] LogSanityChecks -> DEBU 443[0m As expected, current configuration has policy '/Channel/Application/Admins'
[36m2019-01-29 08:50:44.550 UTC [policies] Manager -> DEBU 444[0m Manager Channel looking up path [Orderer]
[36m2019-01-29 08:50:44.550 UTC [policies] Manager -> DEBU 445[0m Manager Channel has managers Application
[36m2019-01-29 08:50:44.550 UTC [policies] Manager -> DEBU 446[0m Manager Channel has managers Orderer
[36m2019-01-29 08:50:44.550 UTC [policies] Manager -> DEBU 447[0m Manager Channel/Orderer looking up path []
[36m2019-01-29 08:50:44.550 UTC [policies] Manager -> DEBU 448[0m Manager Channel/Orderer has managers OrdererOrg
[36m2019-01-29 08:50:44.550 UTC [common/channelconfig] LogSanityChecks -> DEBU 449[0m As expected, current configuration has policy '/Channel/Orderer/BlockValidation'
[36m2019-01-29 08:50:44.550 UTC [common/capabilities] Supported -> DEBU 44a[0m Orderer capability V1_1 is supported and is enabled
[36m2019-01-29 08:50:44.550 UTC [common/capabilities] Supported -> DEBU 44b[0m Channel capability V1_1 is supported and is enabled
[36m2019-01-29 08:50:44.550 UTC [fsblkstorage] newBlockfileMgr -> DEBU 44c[0m newBlockfileMgr() initializing file-based block storage for ledger: comunitychannel 
[36m2019-01-29 08:50:44.550 UTC [kvledger.util] CreateDirIfMissing -> DEBU 44d[0m CreateDirIfMissing [/var/hyperledger/production/orderer/chains/comunitychannel/]
[36m2019-01-29 08:50:44.550 UTC [kvledger.util] logDirStatus -> DEBU 44e[0m Before creating dir - [/var/hyperledger/production/orderer/chains/comunitychannel/] does not exist
[36m2019-01-29 08:50:44.550 UTC [kvledger.util] logDirStatus -> DEBU 44f[0m After creating dir - [/var/hyperledger/production/orderer/chains/comunitychannel/] exists
2019-01-29 08:50:44.550 UTC [fsblkstorage] newBlockfileMgr -> INFO 450[0m Getting block information from block storage
[36m2019-01-29 08:50:44.550 UTC [fsblkstorage] constructCheckpointInfoFromBlockFiles -> DEBU 451[0m Retrieving checkpoint info from block files
[36m2019-01-29 08:50:44.550 UTC [fsblkstorage] retrieveLastFileSuffix -> DEBU 452[0m retrieveLastFileSuffix()
[36m2019-01-29 08:50:44.551 UTC [fsblkstorage] retrieveLastFileSuffix -> DEBU 453[0m retrieveLastFileSuffix() - biggestFileNum = -1
[36m2019-01-29 08:50:44.551 UTC [fsblkstorage] constructCheckpointInfoFromBlockFiles -> DEBU 454[0m Last file number found = -1
[36m2019-01-29 08:50:44.551 UTC [fsblkstorage] constructCheckpointInfoFromBlockFiles -> DEBU 455[0m No block file found
[36m2019-01-29 08:50:44.551 UTC [fsblkstorage] newBlockfileMgr -> DEBU 456[0m Info constructed by scanning the blocks dir = (*fsblkstorage.checkpointInfo)(0xc4208a5280)(latestFileChunkSuffixNum=[0], latestFileChunksize=[0], isChainEmpty=[true], lastBlockNumber=[0])
[36m2019-01-29 08:50:44.552 UTC [common/deliver] Handle -> DEBU 457[0m Received EOF from 10.0.0.183:55292, hangup
[36m2019-01-29 08:50:44.552 UTC [orderer/common/server] func1 -> DEBU 458[0m Closing Deliver stream
[36m2019-01-29 08:50:44.558 UTC [fsblkstorage] newBlockIndex -> DEBU 459[0m newBlockIndex() - indexItems:[[BlockNum]]
[36m2019-01-29 08:50:44.562 UTC [fsblkstorage] indexBlock -> DEBU 45a[0m Indexing block [blockNum=0, blockHash=[]byte{0x8, 0x71, 0xb0, 0xf, 0xc2, 0x82, 0x96, 0xbf, 0x59, 0x7d, 0xbb, 0x42, 0x74, 0x92, 0x69, 0x18, 0x64, 0xe3, 0xfa, 0x97, 0xec, 0x91, 0x6d, 0x3f, 0x65, 0x5b, 0x8a, 0x3e, 0x7, 0xc0, 0x9b, 0xb2} txOffsets=
txId= locPointer=offset=39, bytesLength=21701
]
[36m2019-01-29 08:50:44.564 UTC [fsblkstorage] updateCheckpoint -> DEBU 45b[0m Broadcasting about update checkpointInfo: latestFileChunkSuffixNum=[0], latestFileChunksize=[21745], isChainEmpty=[false], lastBlockNumber=[0]
[36m2019-01-29 08:50:44.564 UTC [fsblkstorage] Next -> DEBU 45c[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 08:50:44.566 UTC [fsblkstorage] newBlockfileStream -> DEBU 45d[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/comunitychannel/blockfile_000000], startOffset=[0]
[36m2019-01-29 08:50:44.567 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 45e[0m Remaining bytes=[21745], Going to peek [8] bytes
[36m2019-01-29 08:50:44.567 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 45f[0m Returning blockbytes - length=[21742], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 08:50:44.567 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 460[0m blockbytes [21742] read from file [0]
[36m2019-01-29 08:50:44.567 UTC [orderer/commmon/multichannel] newBlockWriter -> DEBU 461[0m [channel: comunitychannel] Creating block writer for tip of chain (blockNumber=0, lastConfigBlockNum=0, lastConfigSeq=1)
2019-01-29 08:50:44.567 UTC [orderer/consensus/kafka] newChain -> INFO 462[0m [channel: comunitychannel] Starting chain with last persisted offset -3 and last recorded block 0
[36m2019-01-29 08:50:44.567 UTC [orderer/commmon/multichannel] newChainSupport -> DEBU 463[0m [channel: comunitychannel] Done creating channel support resources
2019-01-29 08:50:44.567 UTC [orderer/commmon/multichannel] newChain -> INFO 464[0m Created and starting new chain comunitychannel
[36m2019-01-29 08:50:44.567 UTC [msp] GetDefaultSigningIdentity -> DEBU 465[0m Obtaining default signing identity
[36m2019-01-29 08:50:44.567 UTC [msp] GetDefaultSigningIdentity -> DEBU 466[0m Obtaining default signing identity
[36m2019-01-29 08:50:44.567 UTC [msp/identity] Sign -> DEBU 467[0m Sign: plaintext: 0ABD060A0A4F7264657265724D535012...03D85088F4235A86CFD335225E1971E5 
[36m2019-01-29 08:50:44.567 UTC [msp/identity] Sign -> DEBU 468[0m Sign: digest: 374B70CD631AE891EC0CA84B7142616E441C325ED918D4EB3EE1710F1E36E6D0 
[36m2019-01-29 08:50:44.567 UTC [msp] GetDefaultSigningIdentity -> DEBU 469[0m Obtaining default signing identity
[36m2019-01-29 08:50:44.567 UTC [orderer/commmon/multichannel] addLastConfigSignature -> DEBU 46a[0m [channel: testchainid] About to write block, setting its LAST_CONFIG to 0
[36m2019-01-29 08:50:44.568 UTC [msp] GetDefaultSigningIdentity -> DEBU 46b[0m Obtaining default signing identity
[36m2019-01-29 08:50:44.568 UTC [msp/identity] Sign -> DEBU 46c[0m Sign: plaintext: 0ABD060A0A4F7264657265724D535012...03D85088F4235A86CFD335225E1971E5 
[36m2019-01-29 08:50:44.568 UTC [msp/identity] Sign -> DEBU 46d[0m Sign: digest: A1FADC9F35DBE4A2B1E3A9409377586EE7A07F8B0B152D70487A57BA39A7187E 
2019-01-29 08:50:44.568 UTC [orderer/consensus/kafka] setupProducerForChannel -> INFO 46e[0m [channel: comunitychannel] Setting up the producer for this channel...
[36m2019-01-29 08:50:44.568 UTC [orderer/consensus/kafka] try -> DEBU 46f[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 08:50:44.568 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 470[0m Initializing new client
[36m2019-01-29 08:50:44.568 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 471[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:50:44.568 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 472[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:50:44.568 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 473[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 08:50:44.570 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 474[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.150:10092: connect: connection refused
[36m2019-01-29 08:50:44.570 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 475[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.150:10092: connect: connection refused
[36m2019-01-29 08:50:44.570 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 476[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:50:44.570 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 477[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 08:50:44.572 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 478[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.149:11092: connect: connection refused
[36m2019-01-29 08:50:44.572 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 479[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.149:11092: connect: connection refused
[36m2019-01-29 08:50:44.572 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 47a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:50:44.572 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 47b[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:50:44.586 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 47c[0m Connected to broker at kafka0.switch2logic.co.za:9092 (unregistered)
[36m2019-01-29 08:50:44.587 UTC [orderer/common/server] Deliver -> DEBU 47d[0m Starting new Deliver handler
[36m2019-01-29 08:50:44.587 UTC [common/deliver] Handle -> DEBU 47e[0m Starting new deliver loop for 10.0.0.183:55298
[36m2019-01-29 08:50:44.587 UTC [common/deliver] Handle -> DEBU 47f[0m Attempting to read seek info message from 10.0.0.183:55298
[36m2019-01-29 08:50:44.587 UTC [fsblkstorage] indexBlock -> DEBU 480[0m Indexing block [blockNum=1, blockHash=[]byte{0x98, 0x95, 0xc2, 0xd0, 0x46, 0x74, 0x58, 0xd2, 0x7a, 0xcf, 0xfc, 0x9b, 0x37, 0x5, 0x97, 0xa, 0x8a, 0xc0, 0x5, 0x46, 0x5b, 0x95, 0x7b, 0x3c, 0xc2, 0x17, 0x8e, 0x2c, 0xd1, 0x10, 0x37, 0xc4} txOffsets=
txId= locPointer=offset=71, bytesLength=22670
]
[36m2019-01-29 08:50:44.590 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 481[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 08:50:44.590 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 482[0m client/brokers registered new broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 08:50:44.590 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 483[0m client/brokers registered new broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:50:44.590 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 484[0m Successfully initialized new client
[36m2019-01-29 08:50:44.590 UTC [orderer/consensus/kafka] try -> DEBU 485[0m [channel: comunitychannel] Error is nil, breaking the retry loop
2019-01-29 08:50:44.590 UTC [orderer/consensus/kafka] startThread -> INFO 486[0m [channel: comunitychannel] Producer set up successfully
2019-01-29 08:50:44.590 UTC [orderer/consensus/kafka] sendConnectMessage -> INFO 487[0m [channel: comunitychannel] About to post the CONNECT message...
[36m2019-01-29 08:50:44.590 UTC [orderer/consensus/kafka] try -> DEBU 488[0m [channel: comunitychannel] Attempting to post the CONNECT message...
[36m2019-01-29 08:50:44.590 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 489[0m client/metadata fetching metadata for [comunitychannel] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:50:44.591 UTC [fsblkstorage] updateCheckpoint -> DEBU 48a[0m Broadcasting about update checkpointInfo: latestFileChunkSuffixNum=[0], latestFileChunksize=[43120], isChainEmpty=[false], lastBlockNumber=[1]
[36m2019-01-29 08:50:44.591 UTC [orderer/commmon/multichannel] commitBlock -> DEBU 48b[0m [channel: testchainid] Wrote block 1
[36m2019-01-29 08:50:44.636 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 48c[0m client/metadata found some partitions to be leaderless
[36m2019-01-29 08:50:44.636 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 48d[0m client/metadata retrying after 250ms... (3 attempts remaining)
[33m2019-01-29 08:50:44.781 UTC [common/deliver] deliverBlocks -> WARN 48e[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.183:55298 because of consenter error
[36m2019-01-29 08:50:44.781 UTC [common/deliver] Handle -> DEBU 48f[0m Waiting for new SeekInfo from 10.0.0.183:55298
[36m2019-01-29 08:50:44.781 UTC [common/deliver] Handle -> DEBU 490[0m Attempting to read seek info message from 10.0.0.183:55298
[36m2019-01-29 08:50:44.786 UTC [common/deliver] Handle -> DEBU 491[0m Received EOF from 10.0.0.183:55298, hangup
[36m2019-01-29 08:50:44.786 UTC [orderer/common/server] func1 -> DEBU 492[0m Closing Deliver stream
[36m2019-01-29 08:50:44.811 UTC [orderer/common/server] Deliver -> DEBU 493[0m Starting new Deliver handler
[36m2019-01-29 08:50:44.811 UTC [common/deliver] Handle -> DEBU 494[0m Starting new deliver loop for 10.0.0.183:55320
[36m2019-01-29 08:50:44.811 UTC [common/deliver] Handle -> DEBU 495[0m Attempting to read seek info message from 10.0.0.183:55320
[36m2019-01-29 08:50:44.886 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 496[0m client/metadata fetching metadata for [comunitychannel] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:50:44.889 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 497[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:50:44.890 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU 498[0m producer/broker/0 starting up
[36m2019-01-29 08:50:44.890 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU 499[0m producer/broker/0 state change to [open] on comunitychannel/0
[36m2019-01-29 08:50:44.890 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 49a[0m Connected to broker at kafka0.switch2logic.co.za:9092 (registered as #0)
[33m2019-01-29 08:50:45.013 UTC [common/deliver] deliverBlocks -> WARN 49b[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.183:55320 because of consenter error
[36m2019-01-29 08:50:45.013 UTC [common/deliver] Handle -> DEBU 49c[0m Waiting for new SeekInfo from 10.0.0.183:55320
[36m2019-01-29 08:50:45.013 UTC [common/deliver] Handle -> DEBU 49d[0m Attempting to read seek info message from 10.0.0.183:55320
[36m2019-01-29 08:50:45.015 UTC [common/deliver] Handle -> DEBU 49e[0m Received EOF from 10.0.0.183:55320, hangup
[36m2019-01-29 08:50:45.015 UTC [orderer/common/server] func1 -> DEBU 49f[0m Closing Deliver stream
[36m2019-01-29 08:50:45.022 UTC [orderer/common/server] Deliver -> DEBU 4a0[0m Starting new Deliver handler
[36m2019-01-29 08:50:45.022 UTC [common/deliver] Handle -> DEBU 4a1[0m Starting new deliver loop for 10.0.0.183:55330
[36m2019-01-29 08:50:45.022 UTC [common/deliver] Handle -> DEBU 4a2[0m Attempting to read seek info message from 10.0.0.183:55330
[33m2019-01-29 08:50:45.223 UTC [common/deliver] deliverBlocks -> WARN 4a3[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.183:55330 because of consenter error
[36m2019-01-29 08:50:45.223 UTC [common/deliver] Handle -> DEBU 4a4[0m Waiting for new SeekInfo from 10.0.0.183:55330
[36m2019-01-29 08:50:45.223 UTC [common/deliver] Handle -> DEBU 4a5[0m Attempting to read seek info message from 10.0.0.183:55330
[36m2019-01-29 08:50:45.224 UTC [common/deliver] Handle -> DEBU 4a6[0m Received EOF from 10.0.0.183:55330, hangup
[36m2019-01-29 08:50:45.224 UTC [orderer/common/server] func1 -> DEBU 4a7[0m Closing Deliver stream
[36m2019-01-29 08:50:45.228 UTC [orderer/common/server] Deliver -> DEBU 4a8[0m Starting new Deliver handler
[36m2019-01-29 08:50:45.228 UTC [common/deliver] Handle -> DEBU 4a9[0m Starting new deliver loop for 10.0.0.183:55332
[36m2019-01-29 08:50:45.228 UTC [common/deliver] Handle -> DEBU 4aa[0m Attempting to read seek info message from 10.0.0.183:55332
[33m2019-01-29 08:50:45.429 UTC [common/deliver] deliverBlocks -> WARN 4ab[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.183:55332 because of consenter error
[36m2019-01-29 08:50:45.429 UTC [common/deliver] Handle -> DEBU 4ac[0m Waiting for new SeekInfo from 10.0.0.183:55332
[36m2019-01-29 08:50:45.429 UTC [common/deliver] Handle -> DEBU 4ad[0m Attempting to read seek info message from 10.0.0.183:55332
[36m2019-01-29 08:50:45.430 UTC [common/deliver] Handle -> DEBU 4ae[0m Received EOF from 10.0.0.183:55332, hangup
[36m2019-01-29 08:50:45.430 UTC [orderer/common/server] func1 -> DEBU 4af[0m Closing Deliver stream
[36m2019-01-29 08:50:45.436 UTC [orderer/common/server] Deliver -> DEBU 4b0[0m Starting new Deliver handler
[36m2019-01-29 08:50:45.436 UTC [common/deliver] Handle -> DEBU 4b1[0m Starting new deliver loop for 10.0.0.183:55334
[36m2019-01-29 08:50:45.436 UTC [common/deliver] Handle -> DEBU 4b2[0m Attempting to read seek info message from 10.0.0.183:55334
[33m2019-01-29 08:50:45.637 UTC [common/deliver] deliverBlocks -> WARN 4b3[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.183:55334 because of consenter error
[36m2019-01-29 08:50:45.637 UTC [common/deliver] Handle -> DEBU 4b4[0m Waiting for new SeekInfo from 10.0.0.183:55334
[36m2019-01-29 08:50:45.637 UTC [common/deliver] Handle -> DEBU 4b5[0m Attempting to read seek info message from 10.0.0.183:55334
[36m2019-01-29 08:50:45.638 UTC [common/deliver] Handle -> DEBU 4b6[0m Received EOF from 10.0.0.183:55334, hangup
[36m2019-01-29 08:50:45.638 UTC [orderer/common/server] func1 -> DEBU 4b7[0m Closing Deliver stream
[36m2019-01-29 08:50:45.645 UTC [orderer/common/server] Deliver -> DEBU 4b8[0m Starting new Deliver handler
[36m2019-01-29 08:50:45.646 UTC [common/deliver] Handle -> DEBU 4b9[0m Starting new deliver loop for 10.0.0.183:55336
[36m2019-01-29 08:50:45.646 UTC [common/deliver] Handle -> DEBU 4ba[0m Attempting to read seek info message from 10.0.0.183:55336
[36m2019-01-29 08:50:45.825 UTC [orderer/consensus/kafka] try -> DEBU 4bb[0m [channel: comunitychannel] Error is nil, breaking the retry loop
2019-01-29 08:50:45.825 UTC [orderer/consensus/kafka] startThread -> INFO 4bc[0m [channel: comunitychannel] CONNECT message posted successfully
2019-01-29 08:50:45.825 UTC [orderer/consensus/kafka] setupParentConsumerForChannel -> INFO 4bd[0m [channel: comunitychannel] Setting up the parent consumer for this channel...
[36m2019-01-29 08:50:45.825 UTC [orderer/consensus/kafka] try -> DEBU 4be[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 08:50:45.825 UTC [orderer/consensus/kafka/sarama] NewConsumer -> DEBU 4bf[0m Initializing new client
[36m2019-01-29 08:50:45.825 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 4c0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:50:45.825 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4c1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:50:45.825 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 4c2[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 08:50:45.826 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4c3[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.149:11092: connect: connection refused
[36m2019-01-29 08:50:45.826 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 4c4[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.149:11092: connect: connection refused
[36m2019-01-29 08:50:45.826 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4c5[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:50:45.826 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 4c6[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:50:45.827 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4c7[0m Connected to broker at kafka0.switch2logic.co.za:9092 (unregistered)
[36m2019-01-29 08:50:45.830 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 4c8[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 08:50:45.830 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 4c9[0m client/brokers registered new broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 08:50:45.830 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 4ca[0m client/brokers registered new broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:50:45.830 UTC [orderer/consensus/kafka/sarama] NewConsumer -> DEBU 4cb[0m Successfully initialized new client
[36m2019-01-29 08:50:45.830 UTC [orderer/consensus/kafka] try -> DEBU 4cc[0m [channel: comunitychannel] Error is nil, breaking the retry loop
2019-01-29 08:50:45.830 UTC [orderer/consensus/kafka] startThread -> INFO 4cd[0m [channel: comunitychannel] Parent consumer set up successfully
2019-01-29 08:50:45.830 UTC [orderer/consensus/kafka] setupChannelConsumerForChannel -> INFO 4ce[0m [channel: comunitychannel] Setting up the channel consumer for this channel (start offset: -2)...
[36m2019-01-29 08:50:45.830 UTC [orderer/consensus/kafka] try -> DEBU 4cf[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 08:50:45.830 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4d0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 08:50:45.832 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4d1[0m Connected to broker at kafka0.switch2logic.co.za:9092 (registered as #0)
[33m2019-01-29 08:50:45.847 UTC [common/deliver] deliverBlocks -> WARN 4d2[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.183:55336 because of consenter error
[36m2019-01-29 08:50:45.847 UTC [common/deliver] Handle -> DEBU 4d3[0m Waiting for new SeekInfo from 10.0.0.183:55336
[36m2019-01-29 08:50:45.847 UTC [common/deliver] Handle -> DEBU 4d4[0m Attempting to read seek info message from 10.0.0.183:55336
[36m2019-01-29 08:50:45.848 UTC [common/deliver] Handle -> DEBU 4d5[0m Received EOF from 10.0.0.183:55336, hangup
[36m2019-01-29 08:50:45.849 UTC [orderer/common/server] func1 -> DEBU 4d6[0m Closing Deliver stream
[36m2019-01-29 08:50:45.853 UTC [orderer/common/server] Deliver -> DEBU 4d7[0m Starting new Deliver handler
[36m2019-01-29 08:50:45.854 UTC [common/deliver] Handle -> DEBU 4d8[0m Starting new deliver loop for 10.0.0.183:55360
[36m2019-01-29 08:50:45.854 UTC [common/deliver] Handle -> DEBU 4d9[0m Attempting to read seek info message from 10.0.0.183:55360
[36m2019-01-29 08:50:45.855 UTC [orderer/consensus/kafka/sarama] subscriptionConsumer -> DEBU 4da[0m consumer/broker/0 added subscription to comunitychannel/0
[36m2019-01-29 08:50:45.855 UTC [orderer/consensus/kafka] try -> DEBU 4db[0m [channel: comunitychannel] Error is nil, breaking the retry loop
2019-01-29 08:50:45.855 UTC [orderer/consensus/kafka] startThread -> INFO 4dc[0m [channel: comunitychannel] Channel consumer set up successfully
2019-01-29 08:50:45.855 UTC [orderer/consensus/kafka] startThread -> INFO 4dd[0m [channel: comunitychannel] Start phase completed successfully
[36m2019-01-29 08:50:45.864 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 4de[0m [channel: comunitychannel] Successfully unmarshalled consumed message, offset is 0. Inspecting type...
[36m2019-01-29 08:50:45.864 UTC [orderer/consensus/kafka] processConnect -> DEBU 4df[0m [channel: comunitychannel] It's a connect message - ignoring
[36m2019-01-29 08:50:45.864 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 4e0[0m [channel: comunitychannel] Successfully unmarshalled consumed message, offset is 1. Inspecting type...
[36m2019-01-29 08:50:45.864 UTC [orderer/consensus/kafka] processConnect -> DEBU 4e1[0m [channel: comunitychannel] It's a connect message - ignoring
[36m2019-01-29 08:50:45.865 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 4e2[0m [channel: comunitychannel] Successfully unmarshalled consumed message, offset is 2. Inspecting type...
[36m2019-01-29 08:50:45.865 UTC [orderer/consensus/kafka] processConnect -> DEBU 4e3[0m [channel: comunitychannel] It's a connect message - ignoring
[36m2019-01-29 08:50:46.055 UTC [policies] Evaluate -> DEBU 4e4[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers ==
[36m2019-01-29 08:50:46.055 UTC [policies] Evaluate -> DEBU 4e5[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 08:50:46.055 UTC [policies] Evaluate -> DEBU 4e6[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers ==
[36m2019-01-29 08:50:46.055 UTC [policies] Evaluate -> DEBU 4e7[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 08:50:46.055 UTC [policies] Evaluate -> DEBU 4e8[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers ==
2019-01-29 08:50:46.055 UTC [msp] DeserializeIdentity -> INFO 4e9[0m Obtaining identity
[36m2019-01-29 08:50:46.055 UTC [msp/identity] newIdentity -> DEBU 4ea[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 08:50:46.055 UTC [cauthdsl] func1 -> DEBU 4eb[0m 0xc420290288 gate 1548751846055782054 evaluation starts
[36m2019-01-29 08:50:46.055 UTC [cauthdsl] func2 -> DEBU 4ec[0m 0xc420290288 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 08:50:46.055 UTC [cauthdsl] func2 -> DEBU 4ed[0m 0xc420290288 processing identity 0 with bytes of 0a0c486f73706974616c314d535012df062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943547a4343416657674177494241674951627a7870534b69754a437a4469446c5a702b58625744414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42344d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a45504d4130474131554543784d470a593278705a5735304d5373774b5159445651514444434a425a473170626b426f62334e7761585268624445756333647064474e6f4d6d78765a326c6a4c6d4e760a4c6e70684d466b77457759484b6f5a497a6a3043415159494b6f5a497a6a304441516344516741456e7041446c382b6155426356577636713349674d513479670a49536177513077616d486d7053762b2f62314235686634563248796f50726177497234352b6f342f517a2b2b57617a5a6c795937506b6f54323772374b614e4e0a4d45737744675944565230504151482f42415144416765414d41774741315564457745422f7751434d4141774b7759445652306a42435177496f416756564f770a36442f544c482f357a557a5a6575526363414632664e6e6d6867552b38316d48636f654547775977436759494b6f5a497a6a30454177494453414177525149680a414c6176644d6d3969367853736e57646c6c694d6831307336646d474261584f30437861436f774c4a494d744169424564757564474a796d444445317a6269580a59775838336359575a4855635956384958655a5046796d432b773d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 08:50:46.055 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 4ee[0m Checking if identity satisfies MEMBER role for Hospital1MSP
[36m2019-01-29 08:50:46.055 UTC [msp] Validate -> DEBU 4ef[0m MSP Hospital1MSP validating identity
[36m2019-01-29 08:50:46.056 UTC [msp] getCertificationChain -> DEBU 4f0[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 08:50:46.056 UTC [cauthdsl] func2 -> DEBU 4f1[0m 0xc420290288 principal matched by identity 0
[36m2019-01-29 08:50:46.056 UTC [msp/identity] Verify -> DEBU 4f2[0m Verify: digest = 00000000  0a 3b 72 1d 88 32 98 15  de 1e 82 06 8a c8 e4 c8  |.;r..2..........|
00000010  5b f5 93 62 2b 81 ea 0e  ff e2 3c ea 7c 8d 96 82  |[..b+.....<.|...|
[36m2019-01-29 08:50:46.056 UTC [msp/identity] Verify -> DEBU 4f3[0m Verify: sig = 00000000  30 44 02 20 40 ac 32 85  f2 bc 30 d4 f7 8a 0b 04  |0D. @.2...0.....|
00000010  6d 35 79 c7 4f 64 52 bf  ff 93 3f 16 cc 8c 05 bd  |m5y.OdR...?.....|
00000020  34 ed 46 20 02 20 35 2c  73 d6 b2 82 6f bb e9 e6  |4.F . 5,s...o...|
00000030  b4 de 3f 88 00 22 d5 c4  b8 0d 91 1c 54 fa 87 d6  |..?.."......T...|
00000040  d5 ce ce 75 e3 2d                                 |...u.-|
[36m2019-01-29 08:50:46.056 UTC [cauthdsl] func2 -> DEBU 4f4[0m 0xc420290288 principal evaluation succeeds for identity 0
[36m2019-01-29 08:50:46.056 UTC [cauthdsl] func1 -> DEBU 4f5[0m 0xc420290288 gate 1548751846055782054 evaluation succeeds
[36m2019-01-29 08:50:46.056 UTC [policies] Evaluate -> DEBU 4f6[0m Signature set satisfies policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 08:50:46.056 UTC [policies] Evaluate -> DEBU 4f7[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 08:50:46.056 UTC [policies] Evaluate -> DEBU 4f8[0m Signature set satisfies policy /Channel/Application/Readers
[36m2019-01-29 08:50:46.056 UTC [policies] Evaluate -> DEBU 4f9[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers
[36m2019-01-29 08:50:46.056 UTC [policies] Evaluate -> DEBU 4fa[0m Signature set satisfies policy /Channel/Readers
[36m2019-01-29 08:50:46.056 UTC [policies] Evaluate -> DEBU 4fb[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers
[36m2019-01-29 08:50:46.057 UTC [common/deliver] deliverBlocks -> DEBU 4fc[0m [channel: comunitychannel] Received seekInfo (0xc4206131e0) start:<specified:<> > stop:<specified:<> >  from 10.0.0.183:55360
[36m2019-01-29 08:50:46.057 UTC [fsblkstorage] Next -> DEBU 4fd[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 08:50:46.057 UTC [fsblkstorage] newBlockfileStream -> DEBU 4fe[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/comunitychannel/blockfile_000000], startOffset=[0]
[36m2019-01-29 08:50:46.057 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 4ff[0m Remaining bytes=[21745], Going to peek [8] bytes
[36m2019-01-29 08:50:46.057 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 500[0m Returning blockbytes - length=[21742], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 08:50:46.057 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 501[0m blockbytes [21742] read from file [0]
[36m2019-01-29 08:50:46.057 UTC [common/deliver] deliverBlocks -> DEBU 502[0m [channel: comunitychannel] Delivering block for (0xc4206131e0) for 10.0.0.183:55360
[36m2019-01-29 08:50:46.058 UTC [common/deliver] deliverBlocks -> DEBU 503[0m [channel: comunitychannel] Done delivering to 10.0.0.183:55360 for (0xc4206131e0)
[36m2019-01-29 08:50:46.059 UTC [common/deliver] Handle -> DEBU 504[0m Waiting for new SeekInfo from 10.0.0.183:55360
[36m2019-01-29 08:50:46.060 UTC [common/deliver] Handle -> DEBU 505[0m Attempting to read seek info message from 10.0.0.183:55360
[36m2019-01-29 08:50:46.070 UTC [grpc] Printf -> DEBU 506[0m transport: http2Server.HandleStreams failed to read frame: read tcp 10.0.0.186:7050->10.0.0.183:55360: read: connection reset by peer
[33m2019-01-29 08:50:46.070 UTC [common/deliver] Handle -> WARN 507[0m Error reading from 10.0.0.183:55360: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 08:50:46.070 UTC [orderer/common/server] func1 -> DEBU 508[0m Closing Deliver stream
[36m2019-01-29 08:59:55.376 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 509[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 08:59:55.385 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 50a[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:00:45.003 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 50b[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:00:46.244 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 50c[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:09:55.810 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 50d[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:09:55.818 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 50e[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:10:45.437 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 50f[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:10:46.677 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 510[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:19:55.810 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 511[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:19:55.818 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 512[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:20:45.436 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 513[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:20:46.677 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 514[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:29:55.823 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 515[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:29:55.831 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 516[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:30:45.449 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 517[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:30:46.690 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 518[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:39:55.822 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 519[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:39:55.831 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 51a[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:40:45.449 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 51b[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:40:46.690 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 51c[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:49:55.822 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 51d[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:49:55.831 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 51e[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:50:45.449 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 51f[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:50:46.690 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 520[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:59:55.823 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 521[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 09:59:55.831 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 522[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:00:45.449 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 523[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:00:46.690 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 524[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:09:55.823 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 525[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:09:55.831 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 526[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:10:45.449 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 527[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:10:46.690 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 528[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:11:53.211 UTC [orderer/common/server] Deliver -> DEBU 529[0m Starting new Deliver handler
[36m2019-01-29 10:11:53.211 UTC [common/deliver] Handle -> DEBU 52a[0m Starting new deliver loop for 10.0.0.184:60176
[36m2019-01-29 10:11:53.211 UTC [common/deliver] Handle -> DEBU 52b[0m Attempting to read seek info message from 10.0.0.184:60176
[36m2019-01-29 10:11:53.212 UTC [policies] Evaluate -> DEBU 52c[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers ==
[36m2019-01-29 10:11:53.212 UTC [policies] Evaluate -> DEBU 52d[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 10:11:53.212 UTC [policies] Evaluate -> DEBU 52e[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers ==
[36m2019-01-29 10:11:53.212 UTC [policies] Evaluate -> DEBU 52f[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 10:11:53.212 UTC [policies] Evaluate -> DEBU 530[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers ==
2019-01-29 10:11:53.212 UTC [msp] DeserializeIdentity -> INFO 531[0m Obtaining identity
[36m2019-01-29 10:11:53.212 UTC [msp/identity] newIdentity -> DEBU 532[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTTCCAfOgAwIBAgIQDPIhkZbBMzAPRV2mdTJ2BTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB2MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzENMAsGA1UECxME
cGVlcjErMCkGA1UEAxMicGVlcjAuaG9zcGl0YWwxLnN3aXRjaDJsb2dpYy5jby56
YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABMHSfENe+q4SvhxLb6FxLr2SLYg+
qJnL3whOrSDaEyQgmDz7fwbLhWr0Mapiq8N3Xg1vudI6XbNZxyMTBmZbd3ujTTBL
MA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIFVTsOg/
0yx/+c1M2XrkXHABdnzZ5oYFPvNZh3KHhBsGMAoGCCqGSM49BAMCA0gAMEUCIQDn
gu8tRu0NvsaTNG17i4QtjRftqg56ptadF5YHXBQCGgIgeowGfoCWCIW5T09110Gv
cu4tVgSe4rBzE4yjal6cQVQ=
-----END CERTIFICATE-----
[36m2019-01-29 10:11:53.212 UTC [cauthdsl] func1 -> DEBU 533[0m 0xc4200bc0b8 gate 1548756713212764564 evaluation starts
[36m2019-01-29 10:11:53.212 UTC [cauthdsl] func2 -> DEBU 534[0m 0xc4200bc0b8 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 10:11:53.212 UTC [cauthdsl] func2 -> DEBU 535[0m 0xc4200bc0b8 processing identity 0 with bytes of 0a0c486f73706974616c314d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435454434341664f674177494241674951445049686b5a62424d7a41505256326d64544a324254414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777784c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424d485366454e652b7134537668784c623646784c7232534c59672b0a714a6e4c3377684f72534461457951676d447a376677624c685772304d61706971384e33586731767564493658624e5a78794d54426d5a626433756a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149465654734f672f0a3079782f2b63314d3258726b58484142646e7a5a356f594650764e5a68334b48684273474d416f4743437147534d343942414d43413067414d4555434951446e0a677538745275304e767361544e473137693451746a5266747167353670746164463559485842514347674967656f7747666f43574349573554303931313047760a63753474566753653472427a4534796a616c36635156513d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 10:11:53.212 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 536[0m Checking if identity satisfies MEMBER role for Hospital1MSP
[36m2019-01-29 10:11:53.212 UTC [msp] Validate -> DEBU 537[0m MSP Hospital1MSP validating identity
[36m2019-01-29 10:11:53.213 UTC [msp] getCertificationChain -> DEBU 538[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 10:11:53.213 UTC [cauthdsl] func2 -> DEBU 539[0m 0xc4200bc0b8 principal matched by identity 0
[36m2019-01-29 10:11:53.213 UTC [msp/identity] Verify -> DEBU 53a[0m Verify: digest = 00000000  00 e3 6e 5e bb 2a 44 1b  96 cb bd d9 dc 7b f7 05  |..n^.*D......{..|
00000010  0f 09 61 50 ba 38 b6 f8  fc dc 49 57 c6 01 f7 06  |..aP.8....IW....|
[36m2019-01-29 10:11:53.213 UTC [msp/identity] Verify -> DEBU 53b[0m Verify: sig = 00000000  30 45 02 21 00 bf bb 9b  9a 20 9d 5f 30 af 8e 94  |0E.!..... ._0...|
00000010  a9 fb b3 37 fc 9e 8b ad  b0 4c 33 ce 53 6f ae 4e  |...7.....L3.So.N|
00000020  94 8a b9 fd 8e 02 20 2f  d7 78 ba 0e 4e 12 83 d0  |...... /.x..N...|
00000030  d3 9b 02 4f 0f 1b 0d 56  ba c2 83 1c 4c ef 6e 99  |...O...V....L.n.|
00000040  c7 aa d4 0e 8c cd 9c                              |.......|
[36m2019-01-29 10:11:53.213 UTC [cauthdsl] func2 -> DEBU 53c[0m 0xc4200bc0b8 principal evaluation succeeds for identity 0
[36m2019-01-29 10:11:53.213 UTC [cauthdsl] func1 -> DEBU 53d[0m 0xc4200bc0b8 gate 1548756713212764564 evaluation succeeds
[36m2019-01-29 10:11:53.213 UTC [policies] Evaluate -> DEBU 53e[0m Signature set satisfies policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 10:11:53.213 UTC [policies] Evaluate -> DEBU 53f[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 10:11:53.213 UTC [policies] Evaluate -> DEBU 540[0m Signature set satisfies policy /Channel/Application/Readers
[36m2019-01-29 10:11:53.213 UTC [policies] Evaluate -> DEBU 541[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers
[36m2019-01-29 10:11:53.213 UTC [policies] Evaluate -> DEBU 542[0m Signature set satisfies policy /Channel/Readers
[36m2019-01-29 10:11:53.213 UTC [policies] Evaluate -> DEBU 543[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers
[36m2019-01-29 10:11:53.213 UTC [common/deliver] deliverBlocks -> DEBU 544[0m [channel: comunitychannel] Received seekInfo (0xc42035f040) start:<specified:<number:1 > > stop:<specified:<number:18446744073709551615 > >  from 10.0.0.184:60176
[36m2019-01-29 10:11:53.213 UTC [fsblkstorage] waitForBlock -> DEBU 545[0m Going to wait for newer blocks. maxAvailaBlockNumber=[0], waitForBlockNum=[1]
[36m2019-01-29 10:17:40.904 UTC [grpc] Printf -> DEBU 546[0m grpc: Server.Serve failed to complete security handshake from "10.0.0.229:51920": tls: first record does not look like a TLS handshake
[36m2019-01-29 10:18:34.181 UTC [grpc] Printf -> DEBU 547[0m grpc: Server.Serve failed to complete security handshake from "10.0.0.229:51922": tls: first record does not look like a TLS handshake
[36m2019-01-29 10:19:55.823 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 548[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:19:55.831 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 549[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:20:44.796 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 54a[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:20:46.036 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 54b[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:23:04.990 UTC [grpc] Printf -> DEBU 54c[0m grpc: Server.Serve failed to complete security handshake from "10.0.0.229:51952": tls: first record does not look like a TLS handshake
[36m2019-01-29 10:25:21.172 UTC [grpc] Printf -> DEBU 54d[0m grpc: Server.Serve failed to complete security handshake from "10.0.0.229:51972": tls: first record does not look like a TLS handshake
[36m2019-01-29 10:29:56.381 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 54e[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:29:56.390 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 54f[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:30:46.008 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 550[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:30:47.248 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 551[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:32:58.720 UTC [common/deliver] deliverBlocks -> DEBU 552[0m Context canceled, aborting wait for next block
[36m2019-01-29 10:32:58.720 UTC [orderer/common/server] func1 -> DEBU 553[0m Closing Deliver stream
[36m2019-01-29 10:32:58.720 UTC [fsblkstorage] waitForBlock -> DEBU 554[0m Came out of wait. maxAvailaBlockNumber=[0]
2019-01-29 10:33:57.951 UTC [localconfig] completeInitialization -> INFO 001[0m Kafka.Version unset, setting to 0.10.2.0
[36m2019-01-29 10:33:57.952 UTC [bccsp_sw] openKeyStore -> DEBU 002[0m KeyStore opened at [/etc/hyperledger/fabric/orderer/msp/keystore]...done
[36m2019-01-29 10:33:57.952 UTC [bccsp] initBCCSP -> DEBU 003[0m Initialize BCCSP [SW]
[36m2019-01-29 10:33:57.952 UTC [msp] getPemMaterialFromDir -> DEBU 004[0m Reading directory /etc/hyperledger/fabric/orderer/msp/signcerts
[36m2019-01-29 10:33:57.953 UTC [msp] getPemMaterialFromDir -> DEBU 005[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/signcerts/orderer0.hospital1.switch2logic.co.za-cert.pem
[36m2019-01-29 10:33:57.955 UTC [msp] getPemMaterialFromDir -> DEBU 006[0m Reading directory /etc/hyperledger/fabric/orderer/msp/cacerts
[36m2019-01-29 10:33:57.955 UTC [msp] getPemMaterialFromDir -> DEBU 007[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/cacerts/ca.switch2logic.co.za-cert.pem
[36m2019-01-29 10:33:57.955 UTC [msp] getPemMaterialFromDir -> DEBU 008[0m Reading directory /etc/hyperledger/fabric/orderer/msp/admincerts
[36m2019-01-29 10:33:57.956 UTC [msp] getPemMaterialFromDir -> DEBU 009[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/admincerts/Admin@switch2logic.co.za-cert.pem
[36m2019-01-29 10:33:57.956 UTC [msp] getPemMaterialFromDir -> DEBU 00a[0m Reading directory /etc/hyperledger/fabric/orderer/msp/intermediatecerts
[36m2019-01-29 10:33:57.956 UTC [msp] getMspConfig -> DEBU 00b[0m Intermediate certs folder not found at [/etc/hyperledger/fabric/orderer/msp/intermediatecerts]. Skipping. [stat /etc/hyperledger/fabric/orderer/msp/intermediatecerts: no such file or directory]
[36m2019-01-29 10:33:57.956 UTC [msp] getPemMaterialFromDir -> DEBU 00c[0m Reading directory /etc/hyperledger/fabric/orderer/msp/tlscacerts
[36m2019-01-29 10:33:57.956 UTC [msp] getPemMaterialFromDir -> DEBU 00d[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/tlscacerts/tlsca.switch2logic.co.za-cert.pem
[36m2019-01-29 10:33:57.956 UTC [msp] getPemMaterialFromDir -> DEBU 00e[0m Reading directory /etc/hyperledger/fabric/orderer/msp/tlsintermediatecerts
[36m2019-01-29 10:33:57.956 UTC [msp] getMspConfig -> DEBU 00f[0m TLS intermediate certs folder not found at [/etc/hyperledger/fabric/orderer/msp/tlsintermediatecerts]. Skipping. [stat /etc/hyperledger/fabric/orderer/msp/tlsintermediatecerts: no such file or directory]
[36m2019-01-29 10:33:57.956 UTC [msp] getPemMaterialFromDir -> DEBU 010[0m Reading directory /etc/hyperledger/fabric/orderer/msp/crls
[36m2019-01-29 10:33:57.956 UTC [msp] getMspConfig -> DEBU 011[0m crls folder not found at [/etc/hyperledger/fabric/orderer/msp/crls]. Skipping. [stat /etc/hyperledger/fabric/orderer/msp/crls: no such file or directory]
[36m2019-01-29 10:33:57.957 UTC [msp] getMspConfig -> DEBU 012[0m MSP configuration file not found at [/etc/hyperledger/fabric/orderer/msp/config.yaml]: [stat /etc/hyperledger/fabric/orderer/msp/config.yaml: no such file or directory]
[36m2019-01-29 10:33:57.957 UTC [msp] newBccspMsp -> DEBU 013[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:33:57.957 UTC [msp] New -> DEBU 014[0m Creating Cache-MSP instance
[36m2019-01-29 10:33:57.957 UTC [msp] loadLocaMSP -> DEBU 015[0m Created new local MSP
[36m2019-01-29 10:33:57.957 UTC [msp] Setup -> DEBU 016[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 10:33:57.957 UTC [msp/identity] newIdentity -> DEBU 017[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 10:33:57.958 UTC [msp/identity] newIdentity -> DEBU 018[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.008 UTC [msp/identity] newIdentity -> DEBU 019[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICLDCCAdKgAwIBAgIQIoN1EApgEjYu/O7unXVqYzAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBqMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEuMCwGA1UEAxMlb3JkZXJlcjAuaG9zcGl0YWwxLnN3
aXRjaDJsb2dpYy5jby56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABH2/gH3f
8uhcO2CKglQg53cAJL79dENZNdRWfaRikU5j3X60lh9Bx5YpznQ5l0qMaCK1Ogec
Nw5LGgie9ESxrHyjTTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsG
A1UdIwQkMCKAIBiGNntD5tBlG9C8QNsdXde3ItTRRa+p/+3yJwRCsbJxMAoGCCqG
SM49BAMCA0gAMEUCIQCBtzCy6LE3S8b/XG08Qcs01A8QjmCwqmZJQrDvRA8ZhgIg
aAwP9tYtEZs1DBv2bH7HwYIwmi8nlgD3+Xp6CWwvAkg=
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.009 UTC [bccsp_sw] loadPrivateKey -> DEBU 01a[0m Loading private key [f9564620d77ba2b2650b4916b06387b38f212a58c63268d4c08f9b019eaa0f87] at [/etc/hyperledger/fabric/orderer/msp/keystore/f9564620d77ba2b2650b4916b06387b38f212a58c63268d4c08f9b019eaa0f87_sk]...
[36m2019-01-29 10:33:58.011 UTC [msp/identity] newIdentity -> DEBU 01b[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICLDCCAdKgAwIBAgIQIoN1EApgEjYu/O7unXVqYzAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBqMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEuMCwGA1UEAxMlb3JkZXJlcjAuaG9zcGl0YWwxLnN3
aXRjaDJsb2dpYy5jby56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABH2/gH3f
8uhcO2CKglQg53cAJL79dENZNdRWfaRikU5j3X60lh9Bx5YpznQ5l0qMaCK1Ogec
Nw5LGgie9ESxrHyjTTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsG
A1UdIwQkMCKAIBiGNntD5tBlG9C8QNsdXde3ItTRRa+p/+3yJwRCsbJxMAoGCCqG
SM49BAMCA0gAMEUCIQCBtzCy6LE3S8b/XG08Qcs01A8QjmCwqmZJQrDvRA8ZhgIg
aAwP9tYtEZs1DBv2bH7HwYIwmi8nlgD3+Xp6CWwvAkg=
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.011 UTC [msp] setupSigningIdentity -> DEBU 01c[0m Signing identity expires at 2029-01-26 07:41:54 +0000 UTC
[36m2019-01-29 10:33:58.012 UTC [msp] Validate -> DEBU 01d[0m MSP OrdererMSP validating identity
2019-01-29 10:33:58.012 UTC [orderer/common/server] prettyPrintStruct -> INFO 01e[0m Orderer config values:
	General.LedgerType = "file"
	General.ListenAddress = "0.0.0.0"
	General.ListenPort = 7050
	General.TLS.Enabled = true
	General.TLS.PrivateKey = "/etc/hyperledger/fabric/orderer/tls/server.key"
	General.TLS.Certificate = "/etc/hyperledger/fabric/orderer/tls/server.crt"
	General.TLS.RootCAs = [/etc/hyperledger/fabric/orderer/tls/ca.crt]
	General.TLS.ClientAuthRequired = false
	General.TLS.ClientRootCAs = []
	General.Keepalive.ServerMinInterval = 1m0s
	General.Keepalive.ServerInterval = 2h0m0s
	General.Keepalive.ServerTimeout = 20s
	General.GenesisMethod = "file"
	General.GenesisProfile = "SampleInsecureSolo"
	General.SystemChannel = "test-system-channel-name"
	General.GenesisFile = "/etc/hyperledger/fabric/orderer/channel-artifacts/genesis.block"
	General.Profile.Enabled = false
	General.Profile.Address = "0.0.0.0:6060"
	General.LogLevel = "debug"
	General.LogFormat = "%{color}%{time:2006-01-02 15:04:05.000 MST} [%{module}] %{shortfunc} -> %{level:.4s} %{id:03x}%{color:reset} %{message}"
	General.LocalMSPDir = "/etc/hyperledger/fabric/orderer/msp"
	General.LocalMSPID = "OrdererMSP"
	General.BCCSP.ProviderName = "SW"
	General.BCCSP.SwOpts.SecLevel = 256
	General.BCCSP.SwOpts.HashFamily = "SHA2"
	General.BCCSP.SwOpts.Ephemeral = false
	General.BCCSP.SwOpts.FileKeystore.KeyStorePath = "/etc/hyperledger/fabric/orderer/msp/keystore"
	General.BCCSP.SwOpts.DummyKeystore =
	General.BCCSP.PluginOpts =
	General.Authentication.TimeWindow = 15m0s
	FileLedger.Location = "/var/hyperledger/production/orderer"
	FileLedger.Prefix = "hyperledger-fabric-ordererledger"
	RAMLedger.HistorySize = 1000
	Kafka.Retry.ShortInterval = 1s
	Kafka.Retry.ShortTotal = 30s
	Kafka.Retry.LongInterval = 5m0s
	Kafka.Retry.LongTotal = 12h0m0s
	Kafka.Retry.NetworkTimeouts.DialTimeout = 10s
	Kafka.Retry.NetworkTimeouts.ReadTimeout = 10s
	Kafka.Retry.NetworkTimeouts.WriteTimeout = 10s
	Kafka.Retry.Metadata.RetryMax = 3
	Kafka.Retry.Metadata.RetryBackoff = 250ms
	Kafka.Retry.Producer.RetryMax = 3
	Kafka.Retry.Producer.RetryBackoff = 100ms
	Kafka.Retry.Consumer.RetryBackoff = 2s
	Kafka.Verbose = true
	Kafka.Version = 0.10.2.0
	Kafka.TLS.Enabled = false
	Kafka.TLS.PrivateKey = ""
	Kafka.TLS.Certificate = ""
	Kafka.TLS.RootCAs = []
	Kafka.TLS.ClientAuthRequired = false
	Kafka.TLS.ClientRootCAs = []
	Debug.BroadcastTraceDir = ""
	Debug.DeliverTraceDir = ""
2019-01-29 10:33:58.014 UTC [orderer/common/server] initializeServerConfig -> INFO 01f[0m Starting orderer with TLS enabled
[36m2019-01-29 10:33:58.015 UTC [orderer/common/server] createLedgerFactory -> DEBU 020[0m Ledger dir: /var/hyperledger/production/orderer
[36m2019-01-29 10:33:58.015 UTC [kvledger.util] CreateDirIfMissing -> DEBU 021[0m CreateDirIfMissing [/var/hyperledger/production/orderer/index/]
[36m2019-01-29 10:33:58.016 UTC [kvledger.util] logDirStatus -> DEBU 022[0m Before creating dir - [/var/hyperledger/production/orderer/index/] exists
[36m2019-01-29 10:33:58.016 UTC [kvledger.util] logDirStatus -> DEBU 023[0m After creating dir - [/var/hyperledger/production/orderer/index/] exists
[36m2019-01-29 10:33:58.047 UTC [orderer/common/server] createSubDir -> DEBU 024[0m Found chains sub-dir and using it
2019-01-29 10:33:58.047 UTC [orderer/common/server] initializeMultichannelRegistrar -> INFO 025[0m Not bootstrapping because of existing chains
[36m2019-01-29 10:33:58.048 UTC [fsblkstorage] newBlockfileMgr -> DEBU 026[0m newBlockfileMgr() initializing file-based block storage for ledger: comunitychannel 
[36m2019-01-29 10:33:58.048 UTC [kvledger.util] CreateDirIfMissing -> DEBU 027[0m CreateDirIfMissing [/var/hyperledger/production/orderer/chains/comunitychannel/]
[36m2019-01-29 10:33:58.048 UTC [kvledger.util] logDirStatus -> DEBU 028[0m Before creating dir - [/var/hyperledger/production/orderer/chains/comunitychannel/] exists
[36m2019-01-29 10:33:58.048 UTC [kvledger.util] logDirStatus -> DEBU 029[0m After creating dir - [/var/hyperledger/production/orderer/chains/comunitychannel/] exists
[36m2019-01-29 10:33:58.049 UTC [fsblkstorage] loadCurrentInfo -> DEBU 02a[0m loaded checkpointInfo:latestFileChunkSuffixNum=[0], latestFileChunksize=[21745], isChainEmpty=[false], lastBlockNumber=[0]
[36m2019-01-29 10:33:58.050 UTC [fsblkstorage] newBlockfileMgr -> DEBU 02b[0m Synching block information from block storage (if needed)
[36m2019-01-29 10:33:58.050 UTC [fsblkstorage] syncCPInfoFromFS -> DEBU 02c[0m Starting checkpoint=latestFileChunkSuffixNum=[0], latestFileChunksize=[21745], isChainEmpty=[false], lastBlockNumber=[0]
[36m2019-01-29 10:33:58.050 UTC [fsblkstorage] syncCPInfoFromFS -> DEBU 02d[0m status of file [/var/hyperledger/production/orderer/chains/comunitychannel/blockfile_000000]: exists=[true], size=[21745]
[36m2019-01-29 10:33:58.052 UTC [fsblkstorage] newBlockIndex -> DEBU 02e[0m newBlockIndex() - indexItems:[[BlockNum]]
[36m2019-01-29 10:33:58.052 UTC [fsblkstorage] syncIndex -> DEBU 02f[0m Both the block files and indices are in sync.
[36m2019-01-29 10:33:58.052 UTC [fsblkstorage] retrieveBlockHeaderByNumber -> DEBU 030[0m retrieveBlockHeaderByNumber() - blockNum = [0]
[36m2019-01-29 10:33:58.052 UTC [fsblkstorage] newBlockfileStream -> DEBU 031[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/comunitychannel/blockfile_000000], startOffset=[0]
[36m2019-01-29 10:33:58.052 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 032[0m Remaining bytes=[21745], Going to peek [8] bytes
[36m2019-01-29 10:33:58.053 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 033[0m Returning blockbytes - length=[21742], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 10:33:58.053 UTC [fsblkstorage] Next -> DEBU 034[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 10:33:58.053 UTC [fsblkstorage] newBlockfileStream -> DEBU 035[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/comunitychannel/blockfile_000000], startOffset=[0]
[36m2019-01-29 10:33:58.053 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 036[0m Remaining bytes=[21745], Going to peek [8] bytes
[36m2019-01-29 10:33:58.053 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 037[0m Returning blockbytes - length=[21742], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 10:33:58.053 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 038[0m blockbytes [21742] read from file [0]
[36m2019-01-29 10:33:58.053 UTC [fsblkstorage] Next -> DEBU 039[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 10:33:58.053 UTC [fsblkstorage] newBlockfileStream -> DEBU 03a[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/comunitychannel/blockfile_000000], startOffset=[0]
[36m2019-01-29 10:33:58.053 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 03b[0m Remaining bytes=[21745], Going to peek [8] bytes
[36m2019-01-29 10:33:58.053 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 03c[0m Returning blockbytes - length=[21742], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 10:33:58.054 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 03d[0m blockbytes [21742] read from file [0]
[36m2019-01-29 10:33:58.054 UTC [common/channelconfig] NewStandardValues -> DEBU 03e[0m Initializing protos for *channelconfig.ChannelProtos
[36m2019-01-29 10:33:58.054 UTC [common/channelconfig] initializeProtosStruct -> DEBU 03f[0m Processing field: HashingAlgorithm
[36m2019-01-29 10:33:58.054 UTC [common/channelconfig] initializeProtosStruct -> DEBU 040[0m Processing field: BlockDataHashingStructure
[36m2019-01-29 10:33:58.054 UTC [common/channelconfig] initializeProtosStruct -> DEBU 041[0m Processing field: OrdererAddresses
[36m2019-01-29 10:33:58.054 UTC [common/channelconfig] initializeProtosStruct -> DEBU 042[0m Processing field: Consortium
[36m2019-01-29 10:33:58.054 UTC [common/channelconfig] initializeProtosStruct -> DEBU 043[0m Processing field: Capabilities
[36m2019-01-29 10:33:58.054 UTC [common/channelconfig] NewStandardValues -> DEBU 044[0m Initializing protos for *channelconfig.ApplicationProtos
[36m2019-01-29 10:33:58.054 UTC [common/channelconfig] initializeProtosStruct -> DEBU 045[0m Processing field: ACLs
[36m2019-01-29 10:33:58.054 UTC [common/channelconfig] initializeProtosStruct -> DEBU 046[0m Processing field: Capabilities
[36m2019-01-29 10:33:58.054 UTC [common/channelconfig] NewStandardValues -> DEBU 047[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 10:33:58.054 UTC [common/channelconfig] initializeProtosStruct -> DEBU 048[0m Processing field: AnchorPeers
[36m2019-01-29 10:33:58.055 UTC [common/channelconfig] NewStandardValues -> DEBU 049[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 10:33:58.055 UTC [common/channelconfig] initializeProtosStruct -> DEBU 04a[0m Processing field: MSP
[36m2019-01-29 10:33:58.055 UTC [common/channelconfig] Validate -> DEBU 04b[0m Anchor peers for org Hospital1MSP are 
[36m2019-01-29 10:33:58.055 UTC [common/channelconfig] validateMSP -> DEBU 04c[0m Setting up MSP for org Hospital1MSP
[36m2019-01-29 10:33:58.055 UTC [msp] newBccspMsp -> DEBU 04d[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:33:58.055 UTC [msp] New -> DEBU 04e[0m Creating Cache-MSP instance
[36m2019-01-29 10:33:58.055 UTC [msp] Setup -> DEBU 04f[0m Setting up MSP instance Hospital1MSP
[36m2019-01-29 10:33:58.055 UTC [msp/identity] newIdentity -> DEBU 050[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdDCCAhugAwIBAgIQZxH8VsnceZxXgKxlURLgVTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMS5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQX9V4k/h/3pFxByKDscjgQrC0YS/bSbKvzSG2Z2PvltEWp40qB9OaKUN0GQYzo
yK2Etdvyjfd1OFwaTjpGSvv9o18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCBVU7DoP9Msf/nNTNl6
5FxwAXZ82eaGBT7zWYdyh4QbBjAKBggqhkjOPQQDAgNHADBEAiBmXGWQbxCGRp0c
5/y9qJyciB1CDHsknRZDjNzt5f563wIgMy7VPi1R8DpIIIRN9B3mfhDpRa+J+8PA
S4DOJy7rjp4=
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.056 UTC [msp/identity] newIdentity -> DEBU 051[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.057 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 052[0m Checking if identity satisfies role [CLIENT] for Hospital1MSP
[36m2019-01-29 10:33:58.057 UTC [msp] Validate -> DEBU 053[0m MSP Hospital1MSP validating identity
[36m2019-01-29 10:33:58.057 UTC [msp] getCertificationChain -> DEBU 054[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 10:33:58.057 UTC [msp] hasOURole -> DEBU 055[0m MSP Hospital1MSP checking if the identity is a client
[36m2019-01-29 10:33:58.057 UTC [msp] getCertificationChain -> DEBU 056[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 10:33:58.058 UTC [common/channelconfig] NewStandardValues -> DEBU 057[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 10:33:58.058 UTC [common/channelconfig] initializeProtosStruct -> DEBU 058[0m Processing field: AnchorPeers
[36m2019-01-29 10:33:58.058 UTC [common/channelconfig] NewStandardValues -> DEBU 059[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 10:33:58.058 UTC [common/channelconfig] initializeProtosStruct -> DEBU 05a[0m Processing field: MSP
[36m2019-01-29 10:33:58.058 UTC [common/channelconfig] Validate -> DEBU 05b[0m Anchor peers for org Hospital2MSP are 
[36m2019-01-29 10:33:58.058 UTC [common/channelconfig] validateMSP -> DEBU 05c[0m Setting up MSP for org Hospital2MSP
[36m2019-01-29 10:33:58.058 UTC [msp] newBccspMsp -> DEBU 05d[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:33:58.058 UTC [msp] New -> DEBU 05e[0m Creating Cache-MSP instance
[36m2019-01-29 10:33:58.058 UTC [msp] Setup -> DEBU 05f[0m Setting up MSP instance Hospital2MSP
[36m2019-01-29 10:33:58.058 UTC [msp/identity] newIdentity -> DEBU 060[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdjCCAhygAwIBAgIRALZ3Pffn/xWUtUWPSK6CYVAwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowgYsxCzAJBgNVBAYTAlVTMRMwEQYD
VQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4gRnJhbmNpc2NvMSUwIwYDVQQK
Exxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMSgwJgYDVQQDEx9jYS5ob3Nw
aXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcD
QgAESzVpiz6P4mIb+KwvrttcXJKZR5i137Is4298E7OrkYdAg0tB4rpfKTXzUMwW
vI12cTDujpyaZSiqQ3/zP4fOQqNfMF0wDgYDVR0PAQH/BAQDAgGmMA8GA1UdJQQI
MAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQg8QJqWKPaX24HINwR
S6sINFSwRD+440/ccSnvFVaGF24wCgYIKoZIzj0EAwIDSAAwRQIhAPO2Nng3/nqb
cKPliUqVPVjBMu+D55M90+jexWSOpkjcAiADPx8qv9exZ4BfP3+G49Qzpx0Meblj
53vzUgYFa/0EXw==
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.058 UTC [msp/identity] newIdentity -> DEBU 061[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAJ6XtHMTTW1JzdpD54qCc3wwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA+z1rb9aA1HwHfNLi9V9VZe
bdwT4MXak445zGz4DXzRUsmNeBngzntKb6qpgHQpeFtzbzLx+zwiynRDvfdzORGj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPEC
alij2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQC
IA8A3CCJeCasyy7gYd/M1aHYmwWsecY7UY7cVUGy4XOxAiB0rrmfnGW6JA11PDCu
+5c5Rpn0jF5ddsFn+UIpzBFvmg==
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.059 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 062[0m Checking if identity satisfies role [CLIENT] for Hospital2MSP
[36m2019-01-29 10:33:58.059 UTC [msp] Validate -> DEBU 063[0m MSP Hospital2MSP validating identity
[36m2019-01-29 10:33:58.060 UTC [msp] getCertificationChain -> DEBU 064[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 10:33:58.060 UTC [msp] hasOURole -> DEBU 065[0m MSP Hospital2MSP checking if the identity is a client
[36m2019-01-29 10:33:58.060 UTC [msp] getCertificationChain -> DEBU 066[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 10:33:58.060 UTC [common/channelconfig] NewStandardValues -> DEBU 067[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 10:33:58.060 UTC [common/channelconfig] initializeProtosStruct -> DEBU 068[0m Processing field: AnchorPeers
[36m2019-01-29 10:33:58.060 UTC [common/channelconfig] NewStandardValues -> DEBU 069[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 10:33:58.061 UTC [common/channelconfig] initializeProtosStruct -> DEBU 06a[0m Processing field: MSP
[36m2019-01-29 10:33:58.061 UTC [common/channelconfig] Validate -> DEBU 06b[0m Anchor peers for org Hospital3MSP are 
[36m2019-01-29 10:33:58.061 UTC [common/channelconfig] validateMSP -> DEBU 06c[0m Setting up MSP for org Hospital3MSP
[36m2019-01-29 10:33:58.061 UTC [msp] newBccspMsp -> DEBU 06d[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:33:58.061 UTC [msp] New -> DEBU 06e[0m Creating Cache-MSP instance
[36m2019-01-29 10:33:58.061 UTC [msp] Setup -> DEBU 06f[0m Setting up MSP instance Hospital3MSP
[36m2019-01-29 10:33:58.061 UTC [msp/identity] newIdentity -> DEBU 070[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdTCCAhugAwIBAgIQCCXeHGgObIJo8aq4AdvgVjAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMy5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQm1Oo/AEu7ZZToE15Y5ntlaN128Hs/exX3jNH5nO6r4tvyFtBBNEn1ecXxxPtm
dmQ+nPHONorLwDC42SMAKwfro18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCA7vVWFXWpgZBojQ1ZL
xUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEAlOUoPtpfO8V7
vFRCanjU5zZWGO5ajFlPDJjjDTy3UrwCICrUYV32JrR/m3dslCl15dSThHrKtYCC
Ehvyw1+lNqiG
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.062 UTC [msp/identity] newIdentity -> DEBU 071[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAI8BA/5Jf+5m8ncI+D7aSiMwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwzLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABF1IehIKIjh8tXYZ7rw4QOuc
g/tpBAkJeOt44zr/LepbjhyFW8vIRnkw75i6Wcg+LOoAS+2NFNg2oNjqg//yCHWj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIDu9
VYVdamBkGiNDVkvFS+AEt6Jrn0yK8hFyPGZhqenJMAoGCCqGSM49BAMCA0cAMEQC
IFyc85pwbCAxGNdw4fflsX2I3HrP3h93YAB2zSDC8HtkAiAQxblAZfEnXUiQKrqE
8I10bYJqD6pJz1jb3Pv+4xplTA==
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.063 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 072[0m Checking if identity satisfies role [CLIENT] for Hospital3MSP
[36m2019-01-29 10:33:58.063 UTC [msp] Validate -> DEBU 073[0m MSP Hospital3MSP validating identity
[36m2019-01-29 10:33:58.063 UTC [msp] getCertificationChain -> DEBU 074[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 10:33:58.064 UTC [msp] hasOURole -> DEBU 075[0m MSP Hospital3MSP checking if the identity is a client
[36m2019-01-29 10:33:58.064 UTC [msp] getCertificationChain -> DEBU 076[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 10:33:58.064 UTC [common/channelconfig] NewStandardValues -> DEBU 077[0m Initializing protos for *channelconfig.OrdererProtos
[36m2019-01-29 10:33:58.064 UTC [common/channelconfig] initializeProtosStruct -> DEBU 078[0m Processing field: ConsensusType
[36m2019-01-29 10:33:58.064 UTC [common/channelconfig] initializeProtosStruct -> DEBU 079[0m Processing field: BatchSize
[36m2019-01-29 10:33:58.064 UTC [common/channelconfig] initializeProtosStruct -> DEBU 07a[0m Processing field: BatchTimeout
[36m2019-01-29 10:33:58.064 UTC [common/channelconfig] initializeProtosStruct -> DEBU 07b[0m Processing field: KafkaBrokers
[36m2019-01-29 10:33:58.064 UTC [common/channelconfig] initializeProtosStruct -> DEBU 07c[0m Processing field: ChannelRestrictions
[36m2019-01-29 10:33:58.064 UTC [common/channelconfig] initializeProtosStruct -> DEBU 07d[0m Processing field: Capabilities
[36m2019-01-29 10:33:58.064 UTC [common/channelconfig] NewStandardValues -> DEBU 07e[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 10:33:58.064 UTC [common/channelconfig] initializeProtosStruct -> DEBU 07f[0m Processing field: MSP
[36m2019-01-29 10:33:58.064 UTC [common/channelconfig] validateMSP -> DEBU 080[0m Setting up MSP for org OrdererOrg
[36m2019-01-29 10:33:58.064 UTC [msp] newBccspMsp -> DEBU 081[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:33:58.064 UTC [msp] New -> DEBU 082[0m Creating Cache-MSP instance
[36m2019-01-29 10:33:58.064 UTC [msp] Setup -> DEBU 083[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 10:33:58.065 UTC [msp/identity] newIdentity -> DEBU 084[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.065 UTC [msp/identity] newIdentity -> DEBU 085[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.066 UTC [msp] Validate -> DEBU 086[0m MSP OrdererMSP validating identity
[36m2019-01-29 10:33:58.066 UTC [msp] Setup -> DEBU 087[0m Setting up the MSP manager (4 msps)
[36m2019-01-29 10:33:58.066 UTC [msp] Setup -> DEBU 088[0m MSP manager setup complete, setup 4 msps
[36m2019-01-29 10:33:58.066 UTC [policies] NewManagerImpl -> DEBU 089[0m Proposed new policy Admins for Channel/Application/Hospital3MSP
[36m2019-01-29 10:33:58.067 UTC [policies] NewManagerImpl -> DEBU 08a[0m Proposed new policy Readers for Channel/Application/Hospital3MSP
[36m2019-01-29 10:33:58.067 UTC [policies] NewManagerImpl -> DEBU 08b[0m Proposed new policy Writers for Channel/Application/Hospital3MSP
[36m2019-01-29 10:33:58.067 UTC [policies] NewManagerImpl -> DEBU 08c[0m Proposed new policy Admins for Channel/Application/Hospital1MSP
[36m2019-01-29 10:33:58.067 UTC [policies] NewManagerImpl -> DEBU 08d[0m Proposed new policy Readers for Channel/Application/Hospital1MSP
[36m2019-01-29 10:33:58.067 UTC [policies] NewManagerImpl -> DEBU 08e[0m Proposed new policy Writers for Channel/Application/Hospital1MSP
[36m2019-01-29 10:33:58.067 UTC [policies] NewManagerImpl -> DEBU 08f[0m Proposed new policy Readers for Channel/Application/Hospital2MSP
[36m2019-01-29 10:33:58.067 UTC [policies] NewManagerImpl -> DEBU 090[0m Proposed new policy Writers for Channel/Application/Hospital2MSP
[36m2019-01-29 10:33:58.067 UTC [policies] NewManagerImpl -> DEBU 091[0m Proposed new policy Admins for Channel/Application/Hospital2MSP
[36m2019-01-29 10:33:58.067 UTC [policies] NewManagerImpl -> DEBU 092[0m Proposed new policy Writers for Channel/Application
[36m2019-01-29 10:33:58.067 UTC [policies] NewManagerImpl -> DEBU 093[0m Proposed new policy Admins for Channel/Application
[36m2019-01-29 10:33:58.068 UTC [policies] NewManagerImpl -> DEBU 094[0m Proposed new policy Readers for Channel/Application
[36m2019-01-29 10:33:58.068 UTC [policies] NewManagerImpl -> DEBU 095[0m Proposed new policy Writers for Channel/Orderer/OrdererOrg
[36m2019-01-29 10:33:58.068 UTC [policies] NewManagerImpl -> DEBU 096[0m Proposed new policy Admins for Channel/Orderer/OrdererOrg
[36m2019-01-29 10:33:58.068 UTC [policies] NewManagerImpl -> DEBU 097[0m Proposed new policy Readers for Channel/Orderer/OrdererOrg
[36m2019-01-29 10:33:58.068 UTC [policies] NewManagerImpl -> DEBU 098[0m Proposed new policy Writers for Channel/Orderer
[36m2019-01-29 10:33:58.068 UTC [policies] NewManagerImpl -> DEBU 099[0m Proposed new policy BlockValidation for Channel/Orderer
[36m2019-01-29 10:33:58.068 UTC [policies] NewManagerImpl -> DEBU 09a[0m Proposed new policy Admins for Channel/Orderer
[36m2019-01-29 10:33:58.068 UTC [policies] NewManagerImpl -> DEBU 09b[0m Proposed new policy Readers for Channel/Orderer
[36m2019-01-29 10:33:58.068 UTC [policies] NewManagerImpl -> DEBU 09c[0m Proposed new policy Readers for Channel
[36m2019-01-29 10:33:58.068 UTC [policies] NewManagerImpl -> DEBU 09d[0m Proposed new policy Writers for Channel
[36m2019-01-29 10:33:58.068 UTC [policies] NewManagerImpl -> DEBU 09e[0m Proposed new policy Admins for Channel
[36m2019-01-29 10:33:58.068 UTC [common/configtx] addToMap -> DEBU 09f[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 10:33:58.068 UTC [common/configtx] addToMap -> DEBU 0a0[0m Adding to config map: [Group]  /Channel/Orderer
[36m2019-01-29 10:33:58.068 UTC [common/configtx] addToMap -> DEBU 0a1[0m Adding to config map: [Group]  /Channel/Orderer/OrdererOrg
[36m2019-01-29 10:33:58.068 UTC [common/configtx] addToMap -> DEBU 0a2[0m Adding to config map: [Value]  /Channel/Orderer/OrdererOrg/MSP
[36m2019-01-29 10:33:58.068 UTC [common/configtx] addToMap -> DEBU 0a3[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 10:33:58.068 UTC [common/configtx] addToMap -> DEBU 0a4[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Admins
[36m2019-01-29 10:33:58.068 UTC [common/configtx] addToMap -> DEBU 0a5[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 10:33:58.068 UTC [common/configtx] addToMap -> DEBU 0a6[0m Adding to config map: [Value]  /Channel/Orderer/ChannelRestrictions
[36m2019-01-29 10:33:58.068 UTC [common/configtx] addToMap -> DEBU 0a7[0m Adding to config map: [Value]  /Channel/Orderer/Capabilities
[36m2019-01-29 10:33:58.068 UTC [common/configtx] addToMap -> DEBU 0a8[0m Adding to config map: [Value]  /Channel/Orderer/KafkaBrokers
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0a9[0m Adding to config map: [Value]  /Channel/Orderer/ConsensusType
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0aa[0m Adding to config map: [Value]  /Channel/Orderer/BatchSize
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0ab[0m Adding to config map: [Value]  /Channel/Orderer/BatchTimeout
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0ac[0m Adding to config map: [Policy] /Channel/Orderer/Readers
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0ad[0m Adding to config map: [Policy] /Channel/Orderer/Writers
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0ae[0m Adding to config map: [Policy] /Channel/Orderer/BlockValidation
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0af[0m Adding to config map: [Policy] /Channel/Orderer/Admins
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0b0[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0b1[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0b2[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/MSP
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0b3[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0b4[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0b5[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0b6[0m Adding to config map: [Group]  /Channel/Application/Hospital2MSP
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0b7[0m Adding to config map: [Value]  /Channel/Application/Hospital2MSP/MSP
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0b8[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0b9[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Writers
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0ba[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Admins
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0bb[0m Adding to config map: [Group]  /Channel/Application/Hospital3MSP
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0bc[0m Adding to config map: [Value]  /Channel/Application/Hospital3MSP/MSP
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0bd[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Admins
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0be[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0bf[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Writers
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0c0[0m Adding to config map: [Value]  /Channel/Application/Capabilities
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0c1[0m Adding to config map: [Policy] /Channel/Application/Writers
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0c2[0m Adding to config map: [Policy] /Channel/Application/Admins
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0c3[0m Adding to config map: [Policy] /Channel/Application/Readers
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0c4[0m Adding to config map: [Value]  /Channel/Capabilities
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0c5[0m Adding to config map: [Value]  /Channel/Consortium
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0c6[0m Adding to config map: [Value]  /Channel/HashingAlgorithm
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0c7[0m Adding to config map: [Value]  /Channel/BlockDataHashingStructure
[36m2019-01-29 10:33:58.069 UTC [common/configtx] addToMap -> DEBU 0c8[0m Adding to config map: [Value]  /Channel/OrdererAddresses
[36m2019-01-29 10:33:58.070 UTC [common/configtx] addToMap -> DEBU 0c9[0m Adding to config map: [Policy] /Channel/Readers
[36m2019-01-29 10:33:58.070 UTC [common/configtx] addToMap -> DEBU 0ca[0m Adding to config map: [Policy] /Channel/Writers
[36m2019-01-29 10:33:58.070 UTC [common/configtx] addToMap -> DEBU 0cb[0m Adding to config map: [Policy] /Channel/Admins
[36m2019-01-29 10:33:58.070 UTC [common/channelconfig] LogSanityChecks -> DEBU 0cc[0m As expected, current configuration has policy '/Channel/Readers'
[36m2019-01-29 10:33:58.070 UTC [common/channelconfig] LogSanityChecks -> DEBU 0cd[0m As expected, current configuration has policy '/Channel/Writers'
[36m2019-01-29 10:33:58.070 UTC [policies] Manager -> DEBU 0ce[0m Manager Channel looking up path [Application]
[36m2019-01-29 10:33:58.070 UTC [policies] Manager -> DEBU 0cf[0m Manager Channel has managers Application
[36m2019-01-29 10:33:58.070 UTC [policies] Manager -> DEBU 0d0[0m Manager Channel has managers Orderer
[36m2019-01-29 10:33:58.070 UTC [policies] Manager -> DEBU 0d1[0m Manager Channel/Application looking up path []
[36m2019-01-29 10:33:58.070 UTC [policies] Manager -> DEBU 0d2[0m Manager Channel/Application has managers Hospital1MSP
[36m2019-01-29 10:33:58.070 UTC [policies] Manager -> DEBU 0d3[0m Manager Channel/Application has managers Hospital2MSP
[36m2019-01-29 10:33:58.070 UTC [policies] Manager -> DEBU 0d4[0m Manager Channel/Application has managers Hospital3MSP
[36m2019-01-29 10:33:58.070 UTC [common/channelconfig] LogSanityChecks -> DEBU 0d5[0m As expected, current configuration has policy '/Channel/Application/Readers'
[36m2019-01-29 10:33:58.070 UTC [common/channelconfig] LogSanityChecks -> DEBU 0d6[0m As expected, current configuration has policy '/Channel/Application/Writers'
[36m2019-01-29 10:33:58.070 UTC [common/channelconfig] LogSanityChecks -> DEBU 0d7[0m As expected, current configuration has policy '/Channel/Application/Admins'
[36m2019-01-29 10:33:58.070 UTC [policies] Manager -> DEBU 0d8[0m Manager Channel looking up path [Orderer]
[36m2019-01-29 10:33:58.070 UTC [policies] Manager -> DEBU 0d9[0m Manager Channel has managers Application
[36m2019-01-29 10:33:58.070 UTC [policies] Manager -> DEBU 0da[0m Manager Channel has managers Orderer
[36m2019-01-29 10:33:58.070 UTC [policies] Manager -> DEBU 0db[0m Manager Channel/Orderer looking up path []
[36m2019-01-29 10:33:58.070 UTC [policies] Manager -> DEBU 0dc[0m Manager Channel/Orderer has managers OrdererOrg
[36m2019-01-29 10:33:58.070 UTC [common/channelconfig] LogSanityChecks -> DEBU 0dd[0m As expected, current configuration has policy '/Channel/Orderer/BlockValidation'
[36m2019-01-29 10:33:58.070 UTC [common/capabilities] Supported -> DEBU 0de[0m Orderer capability V1_1 is supported and is enabled
[36m2019-01-29 10:33:58.070 UTC [common/capabilities] Supported -> DEBU 0df[0m Channel capability V1_1 is supported and is enabled
[36m2019-01-29 10:33:58.070 UTC [orderer/commmon/multichannel] NewRegistrar -> DEBU 0e0[0m Starting chain: comunitychannel
[36m2019-01-29 10:33:58.070 UTC [fsblkstorage] Next -> DEBU 0e1[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 10:33:58.070 UTC [fsblkstorage] newBlockfileStream -> DEBU 0e2[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/comunitychannel/blockfile_000000], startOffset=[0]
[36m2019-01-29 10:33:58.070 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0e3[0m Remaining bytes=[21745], Going to peek [8] bytes
[36m2019-01-29 10:33:58.070 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0e4[0m Returning blockbytes - length=[21742], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 10:33:58.070 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0e5[0m blockbytes [21742] read from file [0]
[36m2019-01-29 10:33:58.070 UTC [orderer/commmon/multichannel] newBlockWriter -> DEBU 0e6[0m [channel: comunitychannel] Creating block writer for tip of chain (blockNumber=0, lastConfigBlockNum=0, lastConfigSeq=1)
2019-01-29 10:33:58.070 UTC [orderer/consensus/kafka] newChain -> INFO 0e7[0m [channel: comunitychannel] Starting chain with last persisted offset -3 and last recorded block 0
[36m2019-01-29 10:33:58.070 UTC [orderer/commmon/multichannel] newChainSupport -> DEBU 0e8[0m [channel: comunitychannel] Done creating channel support resources
[36m2019-01-29 10:33:58.070 UTC [fsblkstorage] newBlockfileMgr -> DEBU 0e9[0m newBlockfileMgr() initializing file-based block storage for ledger: testchainid 
[36m2019-01-29 10:33:58.070 UTC [kvledger.util] CreateDirIfMissing -> DEBU 0ea[0m CreateDirIfMissing [/var/hyperledger/production/orderer/chains/testchainid/]
[36m2019-01-29 10:33:58.070 UTC [kvledger.util] logDirStatus -> DEBU 0eb[0m Before creating dir - [/var/hyperledger/production/orderer/chains/testchainid/] exists
[36m2019-01-29 10:33:58.070 UTC [kvledger.util] logDirStatus -> DEBU 0ec[0m After creating dir - [/var/hyperledger/production/orderer/chains/testchainid/] exists
2019-01-29 10:33:58.070 UTC [orderer/consensus/kafka] setupProducerForChannel -> INFO 0ed[0m [channel: comunitychannel] Setting up the producer for this channel...
[36m2019-01-29 10:33:58.070 UTC [orderer/consensus/kafka] try -> DEBU 0ee[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:33:58.070 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 0ef[0m Initializing new client
[36m2019-01-29 10:33:58.070 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 0f0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:58.071 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 0f1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:58.071 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0f2[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:33:58.072 UTC [fsblkstorage] loadCurrentInfo -> DEBU 0f3[0m loaded checkpointInfo:latestFileChunkSuffixNum=[0], latestFileChunksize=[43120], isChainEmpty=[false], lastBlockNumber=[1]
[36m2019-01-29 10:33:58.072 UTC [fsblkstorage] newBlockfileMgr -> DEBU 0f4[0m Synching block information from block storage (if needed)
[36m2019-01-29 10:33:58.072 UTC [fsblkstorage] syncCPInfoFromFS -> DEBU 0f5[0m Starting checkpoint=latestFileChunkSuffixNum=[0], latestFileChunksize=[43120], isChainEmpty=[false], lastBlockNumber=[1]
[36m2019-01-29 10:33:58.073 UTC [fsblkstorage] syncCPInfoFromFS -> DEBU 0f6[0m status of file [/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000]: exists=[true], size=[43120]
[36m2019-01-29 10:33:58.077 UTC [fsblkstorage] newBlockIndex -> DEBU 0f7[0m newBlockIndex() - indexItems:[[BlockNum]]
[36m2019-01-29 10:33:58.078 UTC [fsblkstorage] syncIndex -> DEBU 0f8[0m Both the block files and indices are in sync.
[36m2019-01-29 10:33:58.078 UTC [fsblkstorage] retrieveBlockHeaderByNumber -> DEBU 0f9[0m retrieveBlockHeaderByNumber() - blockNum = [1]
[36m2019-01-29 10:33:58.078 UTC [fsblkstorage] newBlockfileStream -> DEBU 0fa[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[18495]
[36m2019-01-29 10:33:58.078 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0fb[0m Remaining bytes=[24625], Going to peek [8] bytes
[36m2019-01-29 10:33:58.079 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0fc[0m Returning blockbytes - length=[24622], placementInfo={fileNum=[0], startOffset=[18495], bytesOffset=[18498]}
[36m2019-01-29 10:33:58.079 UTC [fsblkstorage] Next -> DEBU 0fd[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=1
[36m2019-01-29 10:33:58.080 UTC [fsblkstorage] newBlockfileStream -> DEBU 0fe[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[18495]
[36m2019-01-29 10:33:58.080 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0ff[0m Remaining bytes=[24625], Going to peek [8] bytes
[36m2019-01-29 10:33:58.080 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 100[0m Returning blockbytes - length=[24622], placementInfo={fileNum=[0], startOffset=[18495], bytesOffset=[18498]}
[36m2019-01-29 10:33:58.081 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 101[0m blockbytes [24622] read from file [0]
[36m2019-01-29 10:33:58.082 UTC [fsblkstorage] Next -> DEBU 102[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=1
[36m2019-01-29 10:33:58.082 UTC [fsblkstorage] newBlockfileStream -> DEBU 103[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 10:33:58.082 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 104[0m Remaining bytes=[43120], Going to peek [8] bytes
[36m2019-01-29 10:33:58.082 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 105[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 10:33:58.083 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 106[0m blockbytes [18492] read from file [0]
[36m2019-01-29 10:33:58.083 UTC [common/channelconfig] NewStandardValues -> DEBU 107[0m Initializing protos for *channelconfig.ChannelProtos
[36m2019-01-29 10:33:58.084 UTC [common/channelconfig] initializeProtosStruct -> DEBU 108[0m Processing field: HashingAlgorithm
[36m2019-01-29 10:33:58.084 UTC [common/channelconfig] initializeProtosStruct -> DEBU 109[0m Processing field: BlockDataHashingStructure
[36m2019-01-29 10:33:58.084 UTC [common/channelconfig] initializeProtosStruct -> DEBU 10a[0m Processing field: OrdererAddresses
[36m2019-01-29 10:33:58.084 UTC [common/channelconfig] initializeProtosStruct -> DEBU 10b[0m Processing field: Consortium
[36m2019-01-29 10:33:58.084 UTC [common/channelconfig] initializeProtosStruct -> DEBU 10c[0m Processing field: Capabilities
[36m2019-01-29 10:33:58.084 UTC [common/channelconfig] NewStandardValues -> DEBU 10d[0m Initializing protos for *channelconfig.OrdererProtos
[36m2019-01-29 10:33:58.085 UTC [common/channelconfig] initializeProtosStruct -> DEBU 10e[0m Processing field: ConsensusType
[36m2019-01-29 10:33:58.085 UTC [common/channelconfig] initializeProtosStruct -> DEBU 10f[0m Processing field: BatchSize
[36m2019-01-29 10:33:58.085 UTC [common/channelconfig] initializeProtosStruct -> DEBU 110[0m Processing field: BatchTimeout
[36m2019-01-29 10:33:58.085 UTC [common/channelconfig] initializeProtosStruct -> DEBU 111[0m Processing field: KafkaBrokers
[36m2019-01-29 10:33:58.085 UTC [common/channelconfig] initializeProtosStruct -> DEBU 112[0m Processing field: ChannelRestrictions
[36m2019-01-29 10:33:58.085 UTC [common/channelconfig] initializeProtosStruct -> DEBU 113[0m Processing field: Capabilities
[36m2019-01-29 10:33:58.085 UTC [common/channelconfig] NewStandardValues -> DEBU 114[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 10:33:58.085 UTC [common/channelconfig] initializeProtosStruct -> DEBU 115[0m Processing field: MSP
[36m2019-01-29 10:33:58.085 UTC [common/channelconfig] validateMSP -> DEBU 116[0m Setting up MSP for org OrdererOrg
[36m2019-01-29 10:33:58.086 UTC [msp] newBccspMsp -> DEBU 117[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:33:58.086 UTC [msp] New -> DEBU 118[0m Creating Cache-MSP instance
[36m2019-01-29 10:33:58.086 UTC [msp] Setup -> DEBU 119[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 10:33:58.087 UTC [msp/identity] newIdentity -> DEBU 11a[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.088 UTC [msp/identity] newIdentity -> DEBU 11b[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.089 UTC [msp] Validate -> DEBU 11c[0m MSP OrdererMSP validating identity
[36m2019-01-29 10:33:58.092 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 11d[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:58.092 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 11e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:58.092 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 11f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:58.092 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 120[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:33:58.094 UTC [common/channelconfig] NewStandardValues -> DEBU 121[0m Initializing protos for *channelconfig.ConsortiumProtos
[36m2019-01-29 10:33:58.095 UTC [common/channelconfig] initializeProtosStruct -> DEBU 122[0m Processing field: ChannelCreationPolicy
[36m2019-01-29 10:33:58.095 UTC [common/channelconfig] NewStandardValues -> DEBU 123[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 10:33:58.095 UTC [common/channelconfig] initializeProtosStruct -> DEBU 124[0m Processing field: MSP
[36m2019-01-29 10:33:58.096 UTC [common/channelconfig] validateMSP -> DEBU 125[0m Setting up MSP for org Hospital1MSP
[36m2019-01-29 10:33:58.096 UTC [msp] newBccspMsp -> DEBU 126[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:33:58.096 UTC [msp] New -> DEBU 127[0m Creating Cache-MSP instance
[36m2019-01-29 10:33:58.096 UTC [msp] Setup -> DEBU 128[0m Setting up MSP instance Hospital1MSP
[36m2019-01-29 10:33:58.097 UTC [msp/identity] newIdentity -> DEBU 129[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdDCCAhugAwIBAgIQZxH8VsnceZxXgKxlURLgVTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMS5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQX9V4k/h/3pFxByKDscjgQrC0YS/bSbKvzSG2Z2PvltEWp40qB9OaKUN0GQYzo
yK2Etdvyjfd1OFwaTjpGSvv9o18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCBVU7DoP9Msf/nNTNl6
5FxwAXZ82eaGBT7zWYdyh4QbBjAKBggqhkjOPQQDAgNHADBEAiBmXGWQbxCGRp0c
5/y9qJyciB1CDHsknRZDjNzt5f563wIgMy7VPi1R8DpIIIRN9B3mfhDpRa+J+8PA
S4DOJy7rjp4=
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.097 UTC [msp/identity] newIdentity -> DEBU 12a[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.098 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 12b[0m Checking if identity satisfies role [CLIENT] for Hospital1MSP
[36m2019-01-29 10:33:58.099 UTC [msp] Validate -> DEBU 12c[0m MSP Hospital1MSP validating identity
[36m2019-01-29 10:33:58.100 UTC [msp] getCertificationChain -> DEBU 12d[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 10:33:58.101 UTC [msp] hasOURole -> DEBU 12e[0m MSP Hospital1MSP checking if the identity is a client
[36m2019-01-29 10:33:58.101 UTC [msp] getCertificationChain -> DEBU 12f[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 10:33:58.102 UTC [common/channelconfig] NewStandardValues -> DEBU 130[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 10:33:58.102 UTC [common/channelconfig] initializeProtosStruct -> DEBU 131[0m Processing field: MSP
[36m2019-01-29 10:33:58.103 UTC [common/channelconfig] validateMSP -> DEBU 132[0m Setting up MSP for org Hospital2MSP
[36m2019-01-29 10:33:58.103 UTC [msp] newBccspMsp -> DEBU 133[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:33:58.103 UTC [msp] New -> DEBU 134[0m Creating Cache-MSP instance
[36m2019-01-29 10:33:58.103 UTC [msp] Setup -> DEBU 135[0m Setting up MSP instance Hospital2MSP
[36m2019-01-29 10:33:58.104 UTC [msp/identity] newIdentity -> DEBU 136[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdjCCAhygAwIBAgIRALZ3Pffn/xWUtUWPSK6CYVAwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowgYsxCzAJBgNVBAYTAlVTMRMwEQYD
VQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4gRnJhbmNpc2NvMSUwIwYDVQQK
Exxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMSgwJgYDVQQDEx9jYS5ob3Nw
aXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcD
QgAESzVpiz6P4mIb+KwvrttcXJKZR5i137Is4298E7OrkYdAg0tB4rpfKTXzUMwW
vI12cTDujpyaZSiqQ3/zP4fOQqNfMF0wDgYDVR0PAQH/BAQDAgGmMA8GA1UdJQQI
MAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQg8QJqWKPaX24HINwR
S6sINFSwRD+440/ccSnvFVaGF24wCgYIKoZIzj0EAwIDSAAwRQIhAPO2Nng3/nqb
cKPliUqVPVjBMu+D55M90+jexWSOpkjcAiADPx8qv9exZ4BfP3+G49Qzpx0Meblj
53vzUgYFa/0EXw==
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.104 UTC [msp/identity] newIdentity -> DEBU 137[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAJ6XtHMTTW1JzdpD54qCc3wwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA+z1rb9aA1HwHfNLi9V9VZe
bdwT4MXak445zGz4DXzRUsmNeBngzntKb6qpgHQpeFtzbzLx+zwiynRDvfdzORGj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPEC
alij2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQC
IA8A3CCJeCasyy7gYd/M1aHYmwWsecY7UY7cVUGy4XOxAiB0rrmfnGW6JA11PDCu
+5c5Rpn0jF5ddsFn+UIpzBFvmg==
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.105 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 138[0m Checking if identity satisfies role [CLIENT] for Hospital2MSP
[36m2019-01-29 10:33:58.106 UTC [msp] Validate -> DEBU 139[0m MSP Hospital2MSP validating identity
[36m2019-01-29 10:33:58.106 UTC [msp] getCertificationChain -> DEBU 13a[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 10:33:58.107 UTC [msp] hasOURole -> DEBU 13b[0m MSP Hospital2MSP checking if the identity is a client
[36m2019-01-29 10:33:58.107 UTC [msp] getCertificationChain -> DEBU 13c[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 10:33:58.108 UTC [common/channelconfig] NewStandardValues -> DEBU 13d[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 10:33:58.108 UTC [common/channelconfig] initializeProtosStruct -> DEBU 13e[0m Processing field: MSP
[36m2019-01-29 10:33:58.108 UTC [common/channelconfig] validateMSP -> DEBU 13f[0m Setting up MSP for org Hospital3MSP
[36m2019-01-29 10:33:58.108 UTC [msp] newBccspMsp -> DEBU 140[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:33:58.108 UTC [msp] New -> DEBU 141[0m Creating Cache-MSP instance
[36m2019-01-29 10:33:58.108 UTC [msp] Setup -> DEBU 142[0m Setting up MSP instance Hospital3MSP
[36m2019-01-29 10:33:58.109 UTC [msp/identity] newIdentity -> DEBU 143[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdTCCAhugAwIBAgIQCCXeHGgObIJo8aq4AdvgVjAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMy5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQm1Oo/AEu7ZZToE15Y5ntlaN128Hs/exX3jNH5nO6r4tvyFtBBNEn1ecXxxPtm
dmQ+nPHONorLwDC42SMAKwfro18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCA7vVWFXWpgZBojQ1ZL
xUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEAlOUoPtpfO8V7
vFRCanjU5zZWGO5ajFlPDJjjDTy3UrwCICrUYV32JrR/m3dslCl15dSThHrKtYCC
Ehvyw1+lNqiG
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.109 UTC [msp/identity] newIdentity -> DEBU 144[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAI8BA/5Jf+5m8ncI+D7aSiMwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwzLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABF1IehIKIjh8tXYZ7rw4QOuc
g/tpBAkJeOt44zr/LepbjhyFW8vIRnkw75i6Wcg+LOoAS+2NFNg2oNjqg//yCHWj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIDu9
VYVdamBkGiNDVkvFS+AEt6Jrn0yK8hFyPGZhqenJMAoGCCqGSM49BAMCA0cAMEQC
IFyc85pwbCAxGNdw4fflsX2I3HrP3h93YAB2zSDC8HtkAiAQxblAZfEnXUiQKrqE
8I10bYJqD6pJz1jb3Pv+4xplTA==
-----END CERTIFICATE-----
[36m2019-01-29 10:33:58.111 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 145[0m Checking if identity satisfies role [CLIENT] for Hospital3MSP
[36m2019-01-29 10:33:58.111 UTC [msp] Validate -> DEBU 146[0m MSP Hospital3MSP validating identity
[36m2019-01-29 10:33:58.113 UTC [msp] getCertificationChain -> DEBU 147[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 10:33:58.114 UTC [msp] hasOURole -> DEBU 148[0m MSP Hospital3MSP checking if the identity is a client
[36m2019-01-29 10:33:58.114 UTC [msp] getCertificationChain -> DEBU 149[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 10:33:58.115 UTC [msp] Setup -> DEBU 14a[0m Setting up the MSP manager (4 msps)
[36m2019-01-29 10:33:58.115 UTC [msp] Setup -> DEBU 14b[0m MSP manager setup complete, setup 4 msps
[36m2019-01-29 10:33:58.115 UTC [policies] NewManagerImpl -> DEBU 14c[0m Proposed new policy Admins for Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 10:33:58.115 UTC [policies] NewManagerImpl -> DEBU 14d[0m Proposed new policy Readers for Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 10:33:58.115 UTC [policies] NewManagerImpl -> DEBU 14e[0m Proposed new policy Writers for Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 10:33:58.115 UTC [policies] NewManagerImpl -> DEBU 14f[0m Proposed new policy Admins for Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 10:33:58.115 UTC [policies] NewManagerImpl -> DEBU 150[0m Proposed new policy Readers for Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 10:33:58.116 UTC [policies] NewManagerImpl -> DEBU 151[0m Proposed new policy Writers for Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 10:33:58.116 UTC [policies] NewManagerImpl -> DEBU 152[0m Proposed new policy Admins for Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 10:33:58.116 UTC [policies] NewManagerImpl -> DEBU 153[0m Proposed new policy Readers for Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 10:33:58.116 UTC [policies] NewManagerImpl -> DEBU 154[0m Proposed new policy Writers for Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 10:33:58.116 UTC [policies] NewManagerImpl -> DEBU 155[0m Proposed new policy Admins for Channel/Consortiums
[36m2019-01-29 10:33:58.116 UTC [policies] NewManagerImpl -> DEBU 156[0m Proposed new policy Admins for Channel/Orderer/OrdererOrg
[36m2019-01-29 10:33:58.116 UTC [policies] NewManagerImpl -> DEBU 157[0m Proposed new policy Readers for Channel/Orderer/OrdererOrg
[36m2019-01-29 10:33:58.116 UTC [policies] NewManagerImpl -> DEBU 158[0m Proposed new policy Writers for Channel/Orderer/OrdererOrg
[36m2019-01-29 10:33:58.117 UTC [policies] NewManagerImpl -> DEBU 159[0m Proposed new policy Writers for Channel/Orderer
[36m2019-01-29 10:33:58.117 UTC [policies] NewManagerImpl -> DEBU 15a[0m Proposed new policy BlockValidation for Channel/Orderer
[36m2019-01-29 10:33:58.117 UTC [policies] NewManagerImpl -> DEBU 15b[0m Proposed new policy Admins for Channel/Orderer
[36m2019-01-29 10:33:58.117 UTC [policies] NewManagerImpl -> DEBU 15c[0m Proposed new policy Readers for Channel/Orderer
[36m2019-01-29 10:33:58.117 UTC [policies] NewManagerImpl -> DEBU 15d[0m Proposed new policy Admins for Channel
[36m2019-01-29 10:33:58.118 UTC [policies] GetPolicy -> DEBU 15e[0m Returning dummy reject all policy because Readers could not be found in Channel/Consortiums/Readers
[36m2019-01-29 10:33:58.118 UTC [policies] NewManagerImpl -> DEBU 15f[0m Proposed new policy Readers for Channel
[36m2019-01-29 10:33:58.118 UTC [policies] GetPolicy -> DEBU 160[0m Returning dummy reject all policy because Writers could not be found in Channel/Consortiums/Writers
[36m2019-01-29 10:33:58.118 UTC [policies] NewManagerImpl -> DEBU 161[0m Proposed new policy Writers for Channel
[36m2019-01-29 10:33:58.118 UTC [common/configtx] addToMap -> DEBU 162[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 10:33:58.119 UTC [common/configtx] addToMap -> DEBU 163[0m Adding to config map: [Group]  /Channel/Consortiums
[36m2019-01-29 10:33:58.119 UTC [common/configtx] addToMap -> DEBU 164[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium
[36m2019-01-29 10:33:58.119 UTC [common/configtx] addToMap -> DEBU 165[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 10:33:58.119 UTC [common/configtx] addToMap -> DEBU 166[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/Hospital1MSP/MSP
[36m2019-01-29 10:33:58.119 UTC [common/configtx] addToMap -> DEBU 167[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital1MSP/Admins
[36m2019-01-29 10:33:58.119 UTC [common/configtx] addToMap -> DEBU 168[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital1MSP/Readers
[36m2019-01-29 10:33:58.119 UTC [common/configtx] addToMap -> DEBU 169[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital1MSP/Writers
[36m2019-01-29 10:33:58.119 UTC [common/configtx] addToMap -> DEBU 16a[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 10:33:58.119 UTC [common/configtx] addToMap -> DEBU 16b[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/Hospital2MSP/MSP
[36m2019-01-29 10:33:58.119 UTC [common/configtx] addToMap -> DEBU 16c[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital2MSP/Admins
[36m2019-01-29 10:33:58.119 UTC [common/configtx] addToMap -> DEBU 16d[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital2MSP/Readers
[36m2019-01-29 10:33:58.119 UTC [common/configtx] addToMap -> DEBU 16e[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital2MSP/Writers
[36m2019-01-29 10:33:58.119 UTC [common/configtx] addToMap -> DEBU 16f[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 10:33:58.119 UTC [common/configtx] addToMap -> DEBU 170[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/Hospital3MSP/MSP
[36m2019-01-29 10:33:58.119 UTC [common/configtx] addToMap -> DEBU 171[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital3MSP/Readers
[36m2019-01-29 10:33:58.120 UTC [common/configtx] addToMap -> DEBU 172[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital3MSP/Writers
[36m2019-01-29 10:33:58.120 UTC [common/configtx] addToMap -> DEBU 173[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital3MSP/Admins
[36m2019-01-29 10:33:58.120 UTC [common/configtx] addToMap -> DEBU 174[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/ChannelCreationPolicy
[36m2019-01-29 10:33:58.120 UTC [common/configtx] addToMap -> DEBU 175[0m Adding to config map: [Policy] /Channel/Consortiums/Admins
[36m2019-01-29 10:33:58.120 UTC [common/configtx] addToMap -> DEBU 176[0m Adding to config map: [Group]  /Channel/Orderer
[36m2019-01-29 10:33:58.120 UTC [common/configtx] addToMap -> DEBU 177[0m Adding to config map: [Group]  /Channel/Orderer/OrdererOrg
[36m2019-01-29 10:33:58.121 UTC [common/configtx] addToMap -> DEBU 178[0m Adding to config map: [Value]  /Channel/Orderer/OrdererOrg/MSP
[36m2019-01-29 10:33:58.121 UTC [common/configtx] addToMap -> DEBU 179[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 10:33:58.121 UTC [common/configtx] addToMap -> DEBU 17a[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 10:33:58.121 UTC [common/configtx] addToMap -> DEBU 17b[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Admins
[36m2019-01-29 10:33:58.121 UTC [common/configtx] addToMap -> DEBU 17c[0m Adding to config map: [Value]  /Channel/Orderer/BatchTimeout
[36m2019-01-29 10:33:58.121 UTC [common/configtx] addToMap -> DEBU 17d[0m Adding to config map: [Value]  /Channel/Orderer/ChannelRestrictions
[36m2019-01-29 10:33:58.121 UTC [common/configtx] addToMap -> DEBU 17e[0m Adding to config map: [Value]  /Channel/Orderer/Capabilities
[36m2019-01-29 10:33:58.122 UTC [common/configtx] addToMap -> DEBU 17f[0m Adding to config map: [Value]  /Channel/Orderer/KafkaBrokers
[36m2019-01-29 10:33:58.122 UTC [common/configtx] addToMap -> DEBU 180[0m Adding to config map: [Value]  /Channel/Orderer/ConsensusType
[36m2019-01-29 10:33:58.122 UTC [common/configtx] addToMap -> DEBU 181[0m Adding to config map: [Value]  /Channel/Orderer/BatchSize
[36m2019-01-29 10:33:58.122 UTC [common/configtx] addToMap -> DEBU 182[0m Adding to config map: [Policy] /Channel/Orderer/Writers
[36m2019-01-29 10:33:58.122 UTC [common/configtx] addToMap -> DEBU 183[0m Adding to config map: [Policy] /Channel/Orderer/BlockValidation
[36m2019-01-29 10:33:58.122 UTC [common/configtx] addToMap -> DEBU 184[0m Adding to config map: [Policy] /Channel/Orderer/Admins
[36m2019-01-29 10:33:58.122 UTC [common/configtx] addToMap -> DEBU 185[0m Adding to config map: [Policy] /Channel/Orderer/Readers
[36m2019-01-29 10:33:58.122 UTC [common/configtx] addToMap -> DEBU 186[0m Adding to config map: [Value]  /Channel/HashingAlgorithm
[36m2019-01-29 10:33:58.122 UTC [common/configtx] addToMap -> DEBU 187[0m Adding to config map: [Value]  /Channel/BlockDataHashingStructure
[36m2019-01-29 10:33:58.122 UTC [common/configtx] addToMap -> DEBU 188[0m Adding to config map: [Value]  /Channel/OrdererAddresses
[36m2019-01-29 10:33:58.122 UTC [common/configtx] addToMap -> DEBU 189[0m Adding to config map: [Value]  /Channel/Capabilities
[36m2019-01-29 10:33:58.122 UTC [common/configtx] addToMap -> DEBU 18a[0m Adding to config map: [Policy] /Channel/Writers
[36m2019-01-29 10:33:58.123 UTC [common/configtx] addToMap -> DEBU 18b[0m Adding to config map: [Policy] /Channel/Admins
[36m2019-01-29 10:33:58.123 UTC [common/configtx] addToMap -> DEBU 18c[0m Adding to config map: [Policy] /Channel/Readers
[36m2019-01-29 10:33:58.123 UTC [common/channelconfig] LogSanityChecks -> DEBU 18d[0m As expected, current configuration has policy '/Channel/Readers'
[36m2019-01-29 10:33:58.123 UTC [common/channelconfig] LogSanityChecks -> DEBU 18e[0m As expected, current configuration has policy '/Channel/Writers'
[36m2019-01-29 10:33:58.123 UTC [policies] Manager -> DEBU 18f[0m Manager Channel looking up path [Application]
[36m2019-01-29 10:33:58.123 UTC [policies] Manager -> DEBU 190[0m Manager Channel has managers Consortiums
[36m2019-01-29 10:33:58.123 UTC [policies] Manager -> DEBU 191[0m Manager Channel has managers Orderer
[36m2019-01-29 10:33:58.123 UTC [policies] Manager -> DEBU 192[0m Manager Channel looking up path [Orderer]
[36m2019-01-29 10:33:58.123 UTC [policies] Manager -> DEBU 193[0m Manager Channel has managers Consortiums
[36m2019-01-29 10:33:58.123 UTC [policies] Manager -> DEBU 194[0m Manager Channel has managers Orderer
[36m2019-01-29 10:33:58.123 UTC [policies] Manager -> DEBU 195[0m Manager Channel/Orderer looking up path []
[36m2019-01-29 10:33:58.123 UTC [policies] Manager -> DEBU 196[0m Manager Channel/Orderer has managers OrdererOrg
[36m2019-01-29 10:33:58.123 UTC [common/channelconfig] LogSanityChecks -> DEBU 197[0m As expected, current configuration has policy '/Channel/Orderer/BlockValidation'
[36m2019-01-29 10:33:58.123 UTC [common/capabilities] Supported -> DEBU 198[0m Orderer capability V1_1 is supported and is enabled
[36m2019-01-29 10:33:58.123 UTC [common/capabilities] Supported -> DEBU 199[0m Channel capability V1_1 is supported and is enabled
[36m2019-01-29 10:33:58.123 UTC [fsblkstorage] Next -> DEBU 19a[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=1
[36m2019-01-29 10:33:58.123 UTC [fsblkstorage] newBlockfileStream -> DEBU 19b[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[18495]
[36m2019-01-29 10:33:58.123 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 19c[0m Remaining bytes=[24625], Going to peek [8] bytes
[36m2019-01-29 10:33:58.123 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 19d[0m Returning blockbytes - length=[24622], placementInfo={fileNum=[0], startOffset=[18495], bytesOffset=[18498]}
[36m2019-01-29 10:33:58.123 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 19e[0m blockbytes [24622] read from file [0]
[36m2019-01-29 10:33:58.123 UTC [orderer/commmon/multichannel] newBlockWriter -> DEBU 19f[0m [channel: testchainid] Creating block writer for tip of chain (blockNumber=1, lastConfigBlockNum=0, lastConfigSeq=0)
2019-01-29 10:33:58.124 UTC [orderer/consensus/kafka] newChain -> INFO 1a0[0m [channel: testchainid] Starting chain with last persisted offset 5 and last recorded block 1
[36m2019-01-29 10:33:58.124 UTC [orderer/commmon/multichannel] newChainSupport -> DEBU 1a1[0m [channel: testchainid] Done creating channel support resources
[36m2019-01-29 10:33:58.124 UTC [orderer/common/msgprocessor] NewSystemChannel -> DEBU 1a2[0m Creating system channel msg processor for channel testchainid
[36m2019-01-29 10:33:58.124 UTC [fsblkstorage] Next -> DEBU 1a3[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=1
[36m2019-01-29 10:33:58.124 UTC [fsblkstorage] newBlockfileStream -> DEBU 1a4[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 10:33:58.124 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 1a5[0m Remaining bytes=[43120], Going to peek [8] bytes
[36m2019-01-29 10:33:58.124 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 1a6[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 10:33:58.124 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 1a7[0m blockbytes [18492] read from file [0]
2019-01-29 10:33:58.124 UTC [orderer/commmon/multichannel] NewRegistrar -> INFO 1a8[0m Starting system channel 'testchainid' with genesis block hash c84c51e50921c0294405e41d16cb6234273f63b37a240438cc9f32578277d7e3 and orderer type kafka
2019-01-29 10:33:58.124 UTC [orderer/common/server] Start -> INFO 1a9[0m Starting orderer:
 Version: 1.2.0
 Commit SHA: cae2ad4
 Go version: go1.10
 OS/Arch: linux/amd64
 Experimental features: false
2019-01-29 10:33:58.124 UTC [orderer/consensus/kafka] setupProducerForChannel -> INFO 1ab[0m [channel: testchainid] Setting up the producer for this channel...
[36m2019-01-29 10:33:58.124 UTC [orderer/consensus/kafka] try -> DEBU 1ac[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:33:58.124 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 1ad[0m Initializing new client
[36m2019-01-29 10:33:58.124 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 1ae[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:58.124 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1af[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:58.124 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1b0[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
2019-01-29 10:33:58.124 UTC [orderer/common/server] Start -> INFO 1aa[0m Beginning to serve requests
[36m2019-01-29 10:33:58.775 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1b1[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:58.776 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1b2[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:58.776 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1b3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:58.776 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1b4[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:33:58.777 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1b5[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:58.777 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1b6[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:58.777 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1b7[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:58.777 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1b8[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:33:58.813 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1b9[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:58.813 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1ba[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:58.813 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1bb[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:33:58.813 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 1bc[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:33:58.813 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 1bd[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:33:58.813 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1be[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:58.813 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1bf[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:58.813 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1c0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:58.813 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1c1[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:33:58.837 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1c2[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:58.837 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1c3[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:58.837 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1c4[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:33:58.837 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 1c5[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:33:58.838 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 1c6[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:33:59.063 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1c7[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.063 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1c8[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:33:59.088 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1c9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.088 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1ca[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:33:59.103 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1cb[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.103 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1cc[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.103 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1cd[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.103 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1ce[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:33:59.106 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1cf[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.107 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1d0[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.107 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1d1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.107 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1d2[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:33:59.107 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1d3[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.107 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1d4[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.108 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1d5[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.108 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1d6[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:33:59.135 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1d7[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.135 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1d8[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.136 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1d9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.136 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1da[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:33:59.136 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1db[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.136 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1dc[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.136 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1dd[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:33:59.137 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 1de[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:33:59.137 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 1df[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:33:59.170 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1e0[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.171 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1e1[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.171 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1e2[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:33:59.171 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 1e3[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:33:59.171 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 1e4[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:33:59.387 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1e5[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.387 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1e6[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:33:59.421 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1e7[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.422 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1e8[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:33:59.467 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1e9[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.467 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1ea[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.467 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1eb[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.468 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1ec[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:33:59.471 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1ed[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.471 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1ee[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.471 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1ef[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.471 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1f0[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:33:59.473 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1f1[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.473 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1f2[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.474 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1f3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.474 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1f4[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:33:59.502 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1f5[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.503 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1f6[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.503 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1f7[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.505 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1f8[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:33:59.505 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1f9[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.505 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1fa[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.505 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1fb[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:33:59.505 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 1fc[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:33:59.505 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 1fd[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:33:59.544 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1fe[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.544 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 1ff[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.545 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 200[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:33:59.546 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 201[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:33:59.547 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 202[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:33:59.755 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 203[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.756 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 204[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:33:59.797 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 205[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.797 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 206[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:33:59.861 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 207[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.862 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 209[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.862 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 20a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.862 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 20b[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:33:59.862 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 208[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.864 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 20c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.865 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 20d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.865 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 20e[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:33:59.896 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 20f[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.896 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 210[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.897 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 211[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.897 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 212[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:33:59.898 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 213[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.898 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 214[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.898 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 215[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:33:59.898 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 216[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:33:59.928 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 217[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.929 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 218[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.929 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 219[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:33:59.930 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 21a[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:33:59.930 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 21b[0m Closing Client
[36m2019-01-29 10:33:59.930 UTC [orderer/consensus/kafka] try -> DEBU 21c[0m [channel: testchainid] Initial attempt failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:33:59.930 UTC [orderer/consensus/kafka] try -> DEBU 21d[0m [channel: testchainid] Retrying every 1s for a total of 30s
[36m2019-01-29 10:33:59.932 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 21e[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.932 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 21f[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:33:59.932 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 220[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:33:59.932 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 221[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:33:59.932 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 222[0m Closing Client
[36m2019-01-29 10:33:59.932 UTC [orderer/consensus/kafka] try -> DEBU 223[0m [channel: comunitychannel] Initial attempt failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:33:59.932 UTC [orderer/consensus/kafka] try -> DEBU 224[0m [channel: comunitychannel] Retrying every 1s for a total of 30s
[36m2019-01-29 10:34:00.931 UTC [orderer/consensus/kafka] try -> DEBU 225[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:34:00.931 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 226[0m Initializing new client
[36m2019-01-29 10:34:00.931 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 227[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:00.931 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 228[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:00.931 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 229[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:00.932 UTC [orderer/consensus/kafka] try -> DEBU 22a[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:34:00.932 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 22b[0m Initializing new client
[36m2019-01-29 10:34:00.933 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 22c[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:00.933 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 22d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:00.933 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 22e[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:01.010 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 22f[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.010 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 230[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.010 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 231[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.010 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 232[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:01.012 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 233[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.012 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 234[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.012 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 235[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.014 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 236[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:01.046 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 237[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.046 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 238[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.046 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 239[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.046 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 23a[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:01.047 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 23b[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.047 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 23c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.047 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 23d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.047 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 23e[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:01.082 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 23f[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.082 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 240[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.082 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 241[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:01.082 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 242[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:01.082 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 243[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:01.082 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 244[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.082 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 245[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.083 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 246[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:01.083 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 247[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:01.083 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 248[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:01.332 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 249[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.333 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 24a[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:01.335 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 24b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.336 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 24c[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:01.376 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 24d[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.377 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 24e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.377 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 24f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.377 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 250[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:01.378 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 251[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.378 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 252[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.378 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 253[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.379 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 254[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:01.411 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 255[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.411 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 256[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.411 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 257[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.411 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 258[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.411 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 259[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.411 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 25a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.411 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 25c[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:01.411 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 25b[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:01.423 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 25d[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.424 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 25e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.424 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 25f[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:01.424 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 260[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:01.424 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 261[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:01.428 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 262[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.428 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 263[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.428 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 264[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:01.428 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 265[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:01.428 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 266[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:01.675 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 267[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.675 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 268[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:01.678 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 269[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.678 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 26a[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:01.712 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 26b[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.712 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 26c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.712 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 26d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.712 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 26e[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:01.716 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 26f[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.716 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 270[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.716 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 271[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.716 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 272[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:01.717 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 273[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.717 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 274[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.717 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 275[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.717 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 277[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.717 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 276[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.717 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 279[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:01.717 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 278[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.718 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 27a[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:01.747 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 27b[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.747 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 27c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.747 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 27d[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:01.747 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 27e[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:01.747 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 27f[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:01.748 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 280[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.748 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 281[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:01.748 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 282[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:01.748 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 283[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:01.748 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 284[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:01.998 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 285[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.998 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 286[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:01.999 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 287[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:01.999 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 288[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:02.045 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 289[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.045 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 28a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.046 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 28b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.046 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 28c[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:02.047 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 28d[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.047 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 28e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.047 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 28f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.047 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 290[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:02.080 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 291[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.081 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 292[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.081 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 293[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.081 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 294[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.081 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 295[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.081 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 296[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.081 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 297[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:02.081 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 298[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:02.117 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 299[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.117 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 29a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.117 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 29b[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:02.117 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 29c[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:02.117 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 29d[0m Closing Client
[36m2019-01-29 10:34:02.117 UTC [orderer/consensus/kafka] try -> DEBU 29e[0m [channel: testchainid] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:02.117 UTC [orderer/consensus/kafka] try -> DEBU 29f[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:34:02.117 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 2a0[0m Initializing new client
[36m2019-01-29 10:34:02.117 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 2a1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.117 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2a2[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.117 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2a3[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:02.118 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2a4[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.119 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2a5[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.119 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2a6[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.119 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2a7[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.119 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2a8[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:02.119 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2a9[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.119 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2aa[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:02.119 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2ab[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:02.119 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 2ac[0m Closing Client
[36m2019-01-29 10:34:02.119 UTC [orderer/consensus/kafka] try -> DEBU 2ad[0m [channel: comunitychannel] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:02.119 UTC [orderer/consensus/kafka] try -> DEBU 2ae[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:34:02.119 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 2af[0m Initializing new client
[36m2019-01-29 10:34:02.119 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 2b0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.119 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2b1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.119 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2b2[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:02.131 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2b3[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.131 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2b4[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.131 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2b5[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.132 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2b6[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.132 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2b7[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.132 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2b8[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.135 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2ba[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:02.134 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2b9[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:02.145 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2bb[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.145 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2bc[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.145 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2bd[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.145 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2be[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:02.167 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2bf[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.167 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2c1[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.167 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2c2[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:02.167 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2c3[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:02.167 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2c4[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:02.167 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2c0[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.167 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2c5[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.167 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2c6[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:02.167 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2c7[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:02.167 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2c8[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:02.417 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2c9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.418 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2ca[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:02.418 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2cb[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.418 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2cc[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:02.475 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2cd[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.475 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2ce[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.475 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2cf[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.475 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2d0[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:02.476 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2d1[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.476 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2d2[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.476 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2d3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.477 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2d4[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:02.477 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2d5[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.477 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2d6[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.477 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2d7[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.477 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2d8[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:02.512 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2d9[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.512 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2da[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:02.513 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2db[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:02.513 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2dc[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:03.273 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2dd[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.273 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2df[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.273 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2e0[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:03.273 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2e1[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:03.273 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2e2[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:03.273 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2de[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.274 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2e3[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.274 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2e4[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:03.274 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2e5[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:03.274 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2e6[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:03.524 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2e7[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:03.524 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2e8[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:03.525 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2e9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:03.525 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2ea[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:03.566 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2eb[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.567 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2ec[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.567 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2ed[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:03.567 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2ee[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:03.567 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2ef[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.567 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2f0[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.567 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2f1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:03.567 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2f2[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:03.568 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2f3[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.568 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2f4[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.568 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2f5[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:03.568 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2f6[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:03.601 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2f7[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.601 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2f8[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.602 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2f9[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:03.602 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2fa[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:03.602 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2fb[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:03.604 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2fc[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.604 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2fd[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.604 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2fe[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:03.604 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2ff[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:03.635 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 300[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.635 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 301[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.635 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 302[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:03.635 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 303[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:03.635 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 304[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:03.852 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 305[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:03.852 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 306[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:03.886 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 307[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:03.886 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 308[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:03.945 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 309[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.945 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 30a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.945 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 30b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:03.945 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 30c[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:03.946 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 30d[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.946 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 30e[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.946 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 30f[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.946 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 310[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.946 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 311[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:03.946 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 312[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:03.946 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 313[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:03.946 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 314[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:03.981 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 315[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.981 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 316[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.981 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 317[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:03.981 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 318[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:03.981 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 319[0m Closing Client
[36m2019-01-29 10:34:03.981 UTC [orderer/consensus/kafka] try -> DEBU 31a[0m [channel: testchainid] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:03.981 UTC [orderer/consensus/kafka] try -> DEBU 31b[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:34:03.981 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 31c[0m Initializing new client
[36m2019-01-29 10:34:03.981 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 31d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:03.982 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 31e[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.982 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 31f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:03.982 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 320[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:03.982 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 321[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:03.982 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 322[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:03.982 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 323[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:04.012 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 324[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.012 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 325[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.013 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 326[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.013 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 327[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:04.013 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 328[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.013 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 329[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:04.013 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 32a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.013 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 32c[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:04.013 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 32b[0m Closing Client
[36m2019-01-29 10:34:04.013 UTC [orderer/consensus/kafka] try -> DEBU 32d[0m [channel: comunitychannel] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:04.013 UTC [orderer/consensus/kafka] try -> DEBU 32e[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:34:04.013 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 32f[0m Initializing new client
[36m2019-01-29 10:34:04.013 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 330[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.013 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 331[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.013 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 332[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:04.047 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 334[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.047 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 333[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.047 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 335[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.047 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 336[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.047 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 337[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.048 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 339[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:04.047 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 338[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.048 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 33a[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:04.076 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 33b[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.076 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 33c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.076 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 33d[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:04.077 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 33e[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:04.077 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 33f[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:04.079 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 340[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.079 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 341[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.079 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 342[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.079 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 343[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:04.116 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 344[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.116 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 345[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.116 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 346[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:04.116 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 347[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:04.116 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 348[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:04.327 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 349[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.327 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 34a[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:04.365 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 34b[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.366 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 34c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.366 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 34d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.366 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 34e[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:04.367 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 34f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.367 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 350[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:04.415 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 351[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.415 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 352[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.415 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 353[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.416 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 355[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.416 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 356[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:04.415 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 354[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.416 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 357[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.416 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 358[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:04.462 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 359[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.462 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 35a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.462 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 35b[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:04.462 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 35c[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:04.463 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 35d[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:04.464 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 35e[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.464 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 35f[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.464 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 360[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.464 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 361[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:04.494 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 362[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.494 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 363[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.494 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 364[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:04.494 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 365[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:04.495 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 366[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:04.636 UTC [orderer/common/server] Deliver -> DEBU 367[0m Starting new Deliver handler
[36m2019-01-29 10:34:04.636 UTC [common/deliver] Handle -> DEBU 368[0m Starting new deliver loop for 10.0.0.22:52250
[36m2019-01-29 10:34:04.636 UTC [common/deliver] Handle -> DEBU 369[0m Attempting to read seek info message from 10.0.0.22:52250
[33m2019-01-29 10:34:04.636 UTC [common/deliver] deliverBlocks -> WARN 36a[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:52250 because of consenter error
[36m2019-01-29 10:34:04.636 UTC [common/deliver] Handle -> DEBU 36b[0m Waiting for new SeekInfo from 10.0.0.22:52250
[36m2019-01-29 10:34:04.636 UTC [common/deliver] Handle -> DEBU 36c[0m Attempting to read seek info message from 10.0.0.22:52250
[36m2019-01-29 10:34:04.713 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 36d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.713 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 36e[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[33m2019-01-29 10:34:04.738 UTC [common/deliver] Handle -> WARN 36f[0m Error reading from 10.0.0.22:52250: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:34:04.738 UTC [orderer/common/server] func1 -> DEBU 370[0m Closing Deliver stream
[36m2019-01-29 10:34:04.745 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 371[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.745 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 372[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:04.750 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 373[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.750 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 374[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.750 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 375[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.750 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 376[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:04.787 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 377[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.787 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 378[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.787 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 379[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.787 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 37b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.787 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 37c[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:04.787 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 37a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.788 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 37d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.788 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 37e[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:04.817 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 37f[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.817 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 380[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.817 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 381[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:04.818 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 382[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:04.819 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 383[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.819 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 385[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.819 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 386[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:04.819 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 387[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:04.819 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 388[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:04.819 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 384[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.819 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 389[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:04.819 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 38a[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:04.819 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 38b[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:04.819 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 38c[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:05.069 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 38d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.069 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 38e[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:05.070 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 38f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.070 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 390[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:05.122 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 391[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.122 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 392[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.122 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 393[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.122 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 394[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:05.159 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 395[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.159 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 396[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.159 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 397[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.159 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 398[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:05.160 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 399[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.160 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 39a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.160 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 39b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.161 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 39c[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:05.166 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 39d[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.166 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 39e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.166 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 39f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.166 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3a0[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:05.206 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3a1[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.206 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3a3[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.206 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3a4[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:05.206 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3a2[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.206 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 3a5[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:05.206 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3a6[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.206 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 3a7[0m Closing Client
[36m2019-01-29 10:34:05.210 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3a8[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:05.210 UTC [orderer/consensus/kafka] try -> DEBU 3a9[0m [channel: testchainid] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:05.210 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 3aa[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:05.210 UTC [orderer/consensus/kafka] try -> DEBU 3ab[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:34:05.210 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 3ac[0m Closing Client
[36m2019-01-29 10:34:05.210 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 3ad[0m Initializing new client
[36m2019-01-29 10:34:05.210 UTC [orderer/consensus/kafka] try -> DEBU 3ae[0m [channel: comunitychannel] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:05.210 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 3af[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.210 UTC [orderer/consensus/kafka] try -> DEBU 3b0[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:34:05.210 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 3b1[0m Initializing new client
[36m2019-01-29 10:34:05.210 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 3b2[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.210 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 3b3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.210 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 3b4[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:05.210 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 3b5[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.210 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 3b6[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:05.514 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3b7[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.515 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 3b8[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.515 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 3b9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.515 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 3ba[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:05.517 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3bb[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.517 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 3bc[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.517 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 3bd[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.517 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 3be[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:05.560 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3bf[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.560 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3c0[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.560 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 3c1[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.561 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 3c3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.561 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 3c4[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:05.560 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 3c2[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.562 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 3c5[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.562 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 3c6[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:05.601 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3c7[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.601 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 3c8[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.601 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 3c9[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:05.601 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 3ca[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:05.602 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 3cb[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:05.602 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3cc[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.602 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 3cd[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.602 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 3ce[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:05.602 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 3cf[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:05.602 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 3d0[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:05.852 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 3d1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.852 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3d2[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:05.853 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 3d3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.853 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3d4[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:05.875 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3d5[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.876 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3d6[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.876 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 3d7[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.876 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3d8[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:05.878 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3d9[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.878 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3da[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.878 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 3db[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.879 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3dc[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:05.914 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3dd[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.914 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3de[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.914 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 3df[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.914 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3e0[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:05.915 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3e1[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.915 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3e2[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:05.915 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 3e3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:05.915 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3e4[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:06.234 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3e5[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:06.234 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3e6[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:06.234 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3e7[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:06.234 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 3e8[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:06.234 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 3e9[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:06.235 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3ea[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:06.236 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3eb[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:06.236 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3ec[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:06.236 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 3ed[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:06.236 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 3ee[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:06.485 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 3ef[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:06.485 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3f0[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:06.486 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 3f1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:06.486 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3f2[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:06.584 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3f3[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:06.584 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3f4[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:06.584 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 3f5[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:06.584 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3f6[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:06.585 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3f7[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:06.585 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3f8[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:06.585 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 3f9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:06.586 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3fa[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:06.953 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3fb[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:06.953 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 3fc[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:06.954 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3fd[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:06.954 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 3ff[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:06.954 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 400[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:06.954 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 3fe[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:06.954 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 401[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:06.954 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 402[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:06.985 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 403[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:06.985 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 404[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:06.985 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 405[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:06.985 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 406[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:06.986 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 407[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:06.986 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 408[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:06.986 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 409[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:06.987 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 40a[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:06.987 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 40b[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:06.987 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 40c[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:07.236 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 40d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:07.236 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 40e[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:07.237 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 40f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:07.237 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 410[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:07.288 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 411[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.288 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 412[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.288 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 413[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:07.288 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 414[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:07.325 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 415[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.325 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 416[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.325 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 417[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:07.325 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 418[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:07.326 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 419[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.327 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 41a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.327 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 41b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:07.327 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 41c[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:07.327 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 41d[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.327 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 41e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.327 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 41f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:07.327 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 420[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:07.360 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 421[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.360 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 422[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.360 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 423[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:07.360 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 424[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:07.360 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 425[0m Closing Client
[36m2019-01-29 10:34:07.360 UTC [orderer/consensus/kafka] try -> DEBU 426[0m [channel: comunitychannel] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:07.360 UTC [orderer/consensus/kafka] try -> DEBU 427[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:34:07.360 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 428[0m Initializing new client
[36m2019-01-29 10:34:07.360 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 429[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:07.360 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 42a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:07.360 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 42b[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:07.363 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 42c[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.363 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 42d[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.363 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 42e[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:07.363 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 42f[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:07.363 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 430[0m Closing Client
[36m2019-01-29 10:34:07.363 UTC [orderer/consensus/kafka] try -> DEBU 431[0m [channel: testchainid] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:07.363 UTC [orderer/consensus/kafka] try -> DEBU 432[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:34:07.364 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 433[0m Initializing new client
[36m2019-01-29 10:34:07.364 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 434[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:07.364 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 435[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:07.364 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 436[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:07.676 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 437[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.677 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 438[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.677 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 439[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:07.677 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 43a[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:07.678 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 43b[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.679 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 43c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.679 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 43d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:07.679 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 43e[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:07.679 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 43f[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.680 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 440[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.680 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 441[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:07.680 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 442[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:07.709 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 443[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.709 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 444[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.709 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 445[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:07.709 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 446[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:07.710 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 447[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.710 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 448[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.710 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 449[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:07.710 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 44a[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:07.710 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 44c[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:07.710 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 44b[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.710 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 44d[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:07.710 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 44e[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:07.710 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 44f[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:07.710 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 450[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:07.960 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 451[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:07.960 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 453[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:07.960 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 452[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:07.961 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 454[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:08.086 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 455[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.086 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 456[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.086 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 457[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:08.086 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 458[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:08.088 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 459[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.088 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 45a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.088 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 45b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:08.088 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 45c[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:08.088 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 45d[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.089 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 45e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.089 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 45f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:08.089 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 460[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:08.123 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 462[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.123 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 463[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.123 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 464[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:08.123 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 465[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:08.123 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 461[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.123 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 466[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.123 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 467[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:08.124 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 468[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:08.124 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 469[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:08.374 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 46a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:08.374 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 46b[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:08.433 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 46c[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.433 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 46d[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.434 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 46e[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:08.434 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 46f[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:08.434 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 470[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:08.466 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 471[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.466 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 472[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.466 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 473[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:08.466 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 474[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:08.478 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 475[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.479 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 476[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.479 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 477[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:08.479 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 478[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:08.494 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 479[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.494 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 47a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.494 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 47b[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:08.494 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 47c[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:08.494 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 47d[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:08.684 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 47e[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:08.684 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 47f[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:08.745 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 480[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:08.745 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 481[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:08.816 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 482[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.817 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 483[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.817 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 484[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:08.817 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 485[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:08.849 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 486[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.849 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 487[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.849 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 488[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.849 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 489[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.849 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 48a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:08.849 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 48b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:08.849 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 48c[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:08.849 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 48d[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:08.880 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 48e[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.880 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 48f[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.880 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 490[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:08.880 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 491[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:08.880 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 492[0m Closing Client
[36m2019-01-29 10:34:08.880 UTC [orderer/consensus/kafka] try -> DEBU 493[0m [channel: testchainid] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:08.880 UTC [orderer/consensus/kafka] try -> DEBU 494[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:34:08.881 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 495[0m Initializing new client
[36m2019-01-29 10:34:08.881 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 496[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:08.881 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 497[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:08.881 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 498[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:08.881 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 499[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.881 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 49a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.881 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 49b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:08.881 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 49c[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:08.914 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 49e[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.914 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 49f[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.914 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 49d[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.914 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4a0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:08.914 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 4a1[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:08.914 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4a2[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.914 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4a3[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:08.914 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 4a4[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:08.914 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 4a5[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:08.944 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4a6[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.945 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 4a7[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.945 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4a8[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:08.945 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 4a9[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:08.977 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4aa[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.978 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 4ab[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:08.978 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 4ac[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:08.978 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 4ad[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:08.978 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 4ae[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:09.165 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4af[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.165 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4b0[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:09.193 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4b1[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.193 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4b2[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.193 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4b3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.193 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4b4[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:09.227 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4b5[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.227 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4b6[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.227 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4b7[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.228 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4b8[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:09.228 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4b9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.228 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4ba[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:09.258 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4bb[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.258 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4bc[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.258 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4bd[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.258 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4be[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:09.258 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4bf[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.258 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4c0[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.259 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4c1[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:09.259 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 4c2[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:09.259 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 4c3[0m Closing Client
[36m2019-01-29 10:34:09.259 UTC [orderer/consensus/kafka] try -> DEBU 4c4[0m [channel: comunitychannel] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:09.259 UTC [orderer/consensus/kafka] try -> DEBU 4c5[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:34:09.259 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 4c6[0m Initializing new client
[36m2019-01-29 10:34:09.259 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 4c7[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.259 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4c8[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.259 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 4c9[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:09.291 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4ca[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.291 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 4cb[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.291 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4cc[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.291 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 4cd[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:09.292 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4ce[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.292 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4cf[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.292 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4d0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.292 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4d1[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:09.325 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4d2[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.325 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4d3[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.325 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4d4[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:09.325 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 4d5[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:09.325 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 4d6[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:09.325 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4d7[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.325 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 4d8[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.325 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4d9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.325 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 4da[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:09.360 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4db[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.360 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 4dc[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.360 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 4dd[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:09.360 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 4de[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:09.360 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 4df[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:09.575 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4e0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.576 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4e1[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:09.583 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4e2[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.583 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4e3[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.583 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4e4[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.583 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4e5[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:09.590 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4e6[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.590 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4e7[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.590 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4e8[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.590 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4e9[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:09.611 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4ea[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.611 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4eb[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:09.621 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4ec[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.621 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4ed[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.622 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4ee[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:09.622 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 4ef[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:09.622 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 4f0[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:09.622 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4f1[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.622 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4f2[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.622 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4f3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.622 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4f4[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:09.656 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4f5[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.656 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4f6[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.656 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4f7[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.656 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4f8[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:09.688 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 4f9[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.688 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4fa[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.689 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4fb[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:09.689 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 4fc[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:09.689 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 4fd[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:09.872 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 4fe[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.873 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 4ff[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:09.923 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 500[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.923 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 501[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.923 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 502[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.923 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 503[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:09.941 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 504[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.941 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 505[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:09.956 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 506[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.956 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 507[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.957 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 508[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.957 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 509[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:09.958 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 50a[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.958 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 50b[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.958 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 50c[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.958 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 50d[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:09.984 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 50e[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.984 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 50f[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.984 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 510[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.984 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 511[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:09.985 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 512[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.985 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 513[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:09.985 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 514[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:09.985 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 515[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:09.985 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 516[0m Closing Client
[36m2019-01-29 10:34:09.985 UTC [orderer/consensus/kafka] try -> DEBU 517[0m [channel: testchainid] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:09.985 UTC [orderer/consensus/kafka] try -> DEBU 518[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:34:09.985 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 519[0m Initializing new client
[36m2019-01-29 10:34:09.985 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 51a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.986 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 51b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:09.986 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 51c[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:10.019 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 51d[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:10.019 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 51e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:10.020 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 51f[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:10.020 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 520[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:10.020 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 521[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:10.021 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 522[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:10.021 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 523[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:10.021 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 524[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:10.022 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 525[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:10.054 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 526[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:10.054 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 527[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:10.054 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 528[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:10.055 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 529[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:10.095 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 52a[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:10.095 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 52b[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:10.095 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 52c[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:10.095 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 52d[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:10.095 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 52e[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:10.270 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 52f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:10.270 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 530[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:10.300 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 531[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:10.300 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 532[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:10.300 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 533[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:10.300 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 534[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:10.336 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 535[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:10.336 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 536[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:10.336 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 537[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:10.336 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 538[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:10.346 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 539[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:10.346 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 53a[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:10.364 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 53b[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:10.364 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 53c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:10.364 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 53d[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:10.364 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 53e[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:10.365 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 53f[0m Closing Client
[36m2019-01-29 10:34:10.365 UTC [orderer/consensus/kafka] try -> DEBU 540[0m [channel: comunitychannel] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:10.365 UTC [orderer/consensus/kafka] try -> DEBU 541[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:34:10.365 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 542[0m Initializing new client
[36m2019-01-29 10:34:10.365 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 543[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:10.365 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 544[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:10.365 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 545[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:11.262 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 546[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.262 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 547[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.262 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 548[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:11.263 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 549[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:11.263 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 54a[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.263 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 54b[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.263 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 54c[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:11.263 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 54d[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:11.297 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 54e[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.297 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 54f[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.297 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 550[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:11.297 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 551[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:11.297 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 552[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.297 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 553[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.297 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 554[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:11.297 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 555[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:11.332 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 557[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.332 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 556[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.332 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 558[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.332 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 55a[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:11.332 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 55b[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:11.332 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 55c[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:11.332 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 559[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.333 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 55d[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:11.333 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 55e[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:11.333 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 55f[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:11.583 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 560[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:11.583 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 561[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:11.584 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 562[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:11.584 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 563[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:11.613 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 564[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.613 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 565[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.613 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 567[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:11.613 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 568[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:11.613 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 566[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.613 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 569[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.613 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 56a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:11.613 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 56b[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:11.642 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 56c[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.642 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 56e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.642 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 56f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:11.642 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 570[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:11.642 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 56d[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.643 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 571[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.643 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 572[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:11.643 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 573[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:11.674 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 574[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.674 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 576[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.674 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 577[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.674 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 578[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:11.674 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 579[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:11.674 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 57a[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:11.674 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 575[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.674 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 57b[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:11.674 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 57c[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:11.674 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 57d[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:11.924 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 57e[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:11.925 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 57f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:11.925 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 580[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:11.925 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 581[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:11.962 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 582[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.962 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 583[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.962 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 584[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:11.962 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 585[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:11.963 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 586[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.963 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 587[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:11.963 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 588[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:11.963 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 589[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:12.775 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 58a[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:12.776 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 58b[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:12.776 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 58c[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:12.776 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 58d[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:12.777 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 58e[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:12.777 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 58f[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:12.777 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 590[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:12.778 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 591[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:12.814 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 592[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:12.814 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 593[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:12.814 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 595[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:12.814 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 596[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:12.814 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 597[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:12.814 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 598[0m Closing Client
[36m2019-01-29 10:34:12.814 UTC [orderer/consensus/kafka] try -> DEBU 599[0m [channel: testchainid] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:12.814 UTC [orderer/consensus/kafka] try -> DEBU 59a[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:34:12.814 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 59b[0m Initializing new client
[36m2019-01-29 10:34:12.814 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 59c[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:12.814 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 59d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:12.814 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 594[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:12.814 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 59e[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:12.815 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 59f[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:12.815 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 5a0[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:12.815 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 5a1[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:12.842 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5a2[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:12.842 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 5a3[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:12.842 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5a4[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:12.842 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 5a5[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:12.876 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5a6[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:12.876 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 5a7[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:12.876 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5a8[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:12.876 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 5a9[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:13.065 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5aa[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:13.065 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5ab[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:13.073 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5ac[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:13.073 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5ad[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:13.073 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5ae[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:13.073 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5af[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:13.765 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5b0[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:13.765 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5b1[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:13.765 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5b2[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:13.765 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5b3[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:13.766 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5b4[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:13.766 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 5b5[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:13.766 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 5b6[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:13.767 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 5b7[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:13.767 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 5b8[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:13.799 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5b9[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:13.799 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5ba[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:13.799 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5bb[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:13.799 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 5bc[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:13.800 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 5bd[0m Closing Client
[36m2019-01-29 10:34:13.800 UTC [orderer/consensus/kafka] try -> DEBU 5be[0m [channel: comunitychannel] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:13.800 UTC [orderer/consensus/kafka] try -> DEBU 5bf[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:34:13.800 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 5c0[0m Initializing new client
[36m2019-01-29 10:34:13.800 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 5c1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:13.801 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5c2[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:13.801 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 5c3[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:13.834 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5c4[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:13.834 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 5c5[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:13.834 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5c6[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:13.834 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 5c7[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:13.863 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5c8[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:13.863 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 5c9[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:13.863 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5ca[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:13.863 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 5cb[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:13.895 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5cc[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:13.895 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 5cd[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:13.895 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 5ce[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:13.895 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 5cf[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:13.895 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 5d0[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:14.017 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5d1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:14.018 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5d2[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:14.146 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5d3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:14.146 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5d4[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:16.791 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5d5[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:16.791 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5d7[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:16.791 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5d8[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:16.791 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5d9[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:16.791 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5d6[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:16.792 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5da[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:16.792 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5db[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:16.792 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5dc[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:16.824 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5dd[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:16.825 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5de[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:16.825 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5df[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:16.825 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5e0[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:16.825 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5e1[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:16.827 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5e3[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:16.827 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5e4[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:16.827 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 5e5[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:16.827 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 5e6[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:16.826 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5e2[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:16.827 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5e7[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:16.827 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5e8[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:16.827 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5e9[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:16.860 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5ea[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:16.860 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5eb[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:16.861 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5ec[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:16.861 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 5ed[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:16.861 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 5ee[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:17.077 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5ef[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.077 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5f0[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:17.111 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5f1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.111 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5f2[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:17.157 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5f3[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.157 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5f5[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.157 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5f6[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.157 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5f7[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:17.157 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5f4[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.158 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5f8[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.158 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5f9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.158 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5fa[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:17.196 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5fb[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.196 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5fc[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.196 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 5fd[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.196 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 5fe[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:17.196 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 5ff[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.197 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 600[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.197 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 602[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:17.197 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 603[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:17.197 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 604[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:17.197 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 601[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.198 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 605[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.198 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 606[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.198 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 607[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:17.229 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 608[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.229 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 609[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.229 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 60a[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:17.229 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 60b[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:17.229 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 60c[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:17.448 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 60d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.448 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 60e[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:17.479 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 60f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.480 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 610[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:17.488 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 611[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.488 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 612[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.488 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 613[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.488 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 614[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.488 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 616[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.488 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 615[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.489 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 618[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:17.488 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 617[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:17.503 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 619[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.504 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 61a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.504 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 61b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.504 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 61c[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:17.514 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 61d[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.514 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 61e[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.514 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 61f[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.514 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 621[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:17.514 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 620[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.514 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 622[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:17.514 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 623[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.514 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 624[0m Closing Client
[36m2019-01-29 10:34:17.514 UTC [orderer/consensus/kafka] try -> DEBU 626[0m [channel: comunitychannel] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:17.514 UTC [orderer/consensus/kafka] try -> DEBU 627[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:34:17.514 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 628[0m Initializing new client
[36m2019-01-29 10:34:17.514 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 629[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.514 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 625[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:17.514 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 62a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.514 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 62b[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:17.527 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 62c[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.527 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 62d[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.527 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 62e[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.527 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 62f[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:17.543 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 630[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.543 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 631[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.543 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 632[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:17.543 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 633[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:17.543 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 634[0m Closing Client
[36m2019-01-29 10:34:17.543 UTC [orderer/consensus/kafka] try -> DEBU 635[0m [channel: testchainid] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:17.544 UTC [orderer/consensus/kafka] try -> DEBU 636[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:34:17.544 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 637[0m Initializing new client
[36m2019-01-29 10:34:17.544 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 638[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.544 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 639[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.544 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 63a[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:17.575 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 63b[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.575 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 63c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.576 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 63d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.576 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 63e[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:17.580 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 63f[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.580 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 640[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.580 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 641[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.580 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 642[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:17.607 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 644[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.607 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 645[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.607 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 646[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:17.607 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 647[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:17.607 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 648[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:17.607 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 643[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.607 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 649[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.607 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 64a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.607 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 64b[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:17.637 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 64c[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.638 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 64d[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.638 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 64e[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:17.638 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 64f[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:17.638 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 650[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:17.857 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 651[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.858 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 652[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:17.880 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 653[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.881 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 654[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.881 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 655[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.881 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 656[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:17.888 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 657[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.889 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 658[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:17.907 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 659[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.908 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 65a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.908 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 65b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.908 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 65c[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:17.913 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 65d[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.914 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 65e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.914 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 65f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.914 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 660[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:17.936 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 661[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.937 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 663[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.937 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 664[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:17.937 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 665[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:17.937 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 662[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.938 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 666[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.938 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 667[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:17.938 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 668[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:17.938 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 669[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:17.966 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 66a[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.966 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 66b[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:17.966 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 66c[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:17.966 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 66d[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:17.966 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 66e[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:18.189 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 66f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.189 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 670[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:18.216 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 671[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.216 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 672[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:18.228 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 673[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.228 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 674[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.228 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 675[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.228 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 676[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.228 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 677[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.229 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 679[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:18.228 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 678[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.229 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 67a[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:18.241 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 67b[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.241 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 67c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.241 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 67d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.241 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 67e[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:18.255 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 67f[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.255 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 680[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.255 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 681[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:18.255 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 682[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:18.255 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 683[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:18.255 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 684[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.255 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 685[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.255 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 686[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.255 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 687[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:18.287 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 688[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.287 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 689[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.287 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 68a[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:18.287 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 68b[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:18.287 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 68c[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:18.506 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 68d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.506 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 68e[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:18.538 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 68f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.538 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 690[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:18.554 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 691[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.554 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 692[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.554 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 693[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.554 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 694[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.555 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 696[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.554 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 695[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.555 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 697[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:18.555 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 698[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:18.581 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 699[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.581 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 69a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.581 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 69b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.581 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 69c[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:18.595 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 69d[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.595 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 69e[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.595 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 69f[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.595 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6a1[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:18.595 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 6a2[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:18.596 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 6a3[0m Closing Client
[36m2019-01-29 10:34:18.596 UTC [orderer/consensus/kafka] try -> DEBU 6a4[0m [channel: testchainid] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:18.596 UTC [orderer/consensus/kafka] try -> DEBU 6a5[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:34:18.595 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6a0[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.596 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 6a6[0m Initializing new client
[36m2019-01-29 10:34:18.596 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 6a8[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.596 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6a9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.596 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 6aa[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:18.596 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6a7[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.597 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6ab[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:18.624 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 6ac[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.624 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6ad[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.624 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 6ae[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.624 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 6b0[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.624 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6af[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:18.624 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 6b2[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:18.624 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6b1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.624 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 6b3[0m Closing Client
[36m2019-01-29 10:34:18.624 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 6b4[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:18.624 UTC [orderer/consensus/kafka] try -> DEBU 6b5[0m [channel: comunitychannel] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:18.624 UTC [orderer/consensus/kafka] try -> DEBU 6b6[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:34:18.624 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 6b7[0m Initializing new client
[36m2019-01-29 10:34:18.624 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 6b8[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.624 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6b9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.624 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 6ba[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:18.629 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 6bb[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.629 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 6bc[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.629 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6bd[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.630 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 6be[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:18.651 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 6bf[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.651 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 6c0[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.651 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6c1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.651 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 6c2[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:18.651 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 6c3[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.651 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 6c4[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.651 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 6c5[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:18.651 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 6c6[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:18.651 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 6c7[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:18.665 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 6c8[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.665 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 6c9[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.665 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6ca[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.665 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 6cb[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:18.676 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 6cc[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.676 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 6cd[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.676 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 6ce[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:18.676 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 6cf[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:18.676 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 6d0[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:18.902 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6d1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.902 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6d2[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:18.926 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6d3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.926 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6d4[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:18.974 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 6d5[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.974 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 6d6[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.975 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6d8[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.975 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6d9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.975 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6da[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:18.974 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6d7[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.976 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6db[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.976 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6dc[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:18.983 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 6dd[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.983 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6de[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.983 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6df[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.983 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6e0[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:18.983 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 6e1[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.984 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6e2[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:18.984 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6e3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:18.984 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6e4[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:19.015 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 6e5[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.015 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 6e6[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.015 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6e7[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.015 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6e8[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.015 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6e9[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:19.015 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6ea[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:19.015 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 6eb[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:19.015 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 6ec[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:19.015 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 6ed[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:19.015 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 6ee[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:19.265 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6ef[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:19.265 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6f0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:19.266 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6f1[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:19.266 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6f2[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:19.302 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 6f3[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.302 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6f5[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.302 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6f6[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:19.302 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6f7[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:19.302 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 6f4[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.302 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6f8[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.302 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6f9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:19.302 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6fa[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:19.320 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 6fb[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.321 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6fd[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.321 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 6fe[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:19.321 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 6fc[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.321 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 6ff[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:19.321 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 700[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.321 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 701[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:19.321 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 702[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:19.355 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 703[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.355 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 704[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.355 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 705[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:19.356 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 706[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:19.356 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 707[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:19.356 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 708[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.356 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 709[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.356 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 70a[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:19.356 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 70b[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:19.356 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 70c[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:19.606 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 70d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:19.606 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 70e[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:19.606 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 70f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:19.606 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 710[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:19.687 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 711[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.688 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 712[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.688 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 713[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.688 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 715[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:19.688 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 714[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.688 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 716[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:19.688 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 717[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:19.689 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 718[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:19.694 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 719[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.694 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 71a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.696 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 71b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:19.696 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 71c[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:19.696 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 71d[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.696 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 71e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:19.696 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 71f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:19.696 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 720[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:20.515 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 721[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.516 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 723[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.516 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 724[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:20.517 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 725[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:20.517 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 726[0m Closing Client
[36m2019-01-29 10:34:20.517 UTC [orderer/consensus/kafka] try -> DEBU 727[0m [channel: testchainid] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:20.516 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 722[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.517 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 729[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.518 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 72a[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:20.518 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 72b[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:20.518 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 72c[0m Closing Client
[36m2019-01-29 10:34:20.518 UTC [orderer/consensus/kafka] try -> DEBU 72d[0m [channel: comunitychannel] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:20.518 UTC [orderer/consensus/kafka] try -> DEBU 72e[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:34:20.518 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 72f[0m Initializing new client
[36m2019-01-29 10:34:20.518 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 730[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:20.517 UTC [orderer/consensus/kafka] try -> DEBU 728[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:34:20.519 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 731[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:20.519 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 733[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:20.519 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 732[0m Initializing new client
[36m2019-01-29 10:34:20.519 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 734[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:20.519 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 735[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:20.519 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 736[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:20.550 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 737[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.550 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 738[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.550 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 739[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:20.550 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 73a[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:20.551 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 73b[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.551 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 73c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.551 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 73d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:20.551 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 73e[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:20.580 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 73f[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.580 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 740[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.580 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 741[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.580 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 742[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.580 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 744[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:20.580 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 743[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:20.580 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 745[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:20.580 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 746[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:20.610 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 747[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.610 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 748[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.610 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 749[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:20.611 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 74a[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:20.611 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 74b[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:20.611 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 74c[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.611 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 74d[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.611 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 74e[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:20.611 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 74f[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:20.611 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 750[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:20.861 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 751[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:20.861 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 752[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:20.861 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 753[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:20.862 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 754[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:20.866 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 755[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.866 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 756[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.866 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 757[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:20.866 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 758[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:20.897 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 759[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.897 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 75a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.897 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 75b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:20.897 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 75c[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:20.899 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 75d[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.899 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 75e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.899 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 75f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:20.899 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 760[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.899 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 762[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.899 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 763[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:20.899 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 764[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:20.899 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 761[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:20.928 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 765[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.928 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 766[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.928 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 767[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:20.928 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 768[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:20.928 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 769[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:20.944 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 76a[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.944 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 76b[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:20.944 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 76c[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:20.944 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 76d[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:20.944 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 76e[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:21.179 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 76f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:21.179 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 770[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:21.194 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 771[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:21.194 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 773[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:21.194 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 774[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:21.194 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 772[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:21.195 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 775[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:21.195 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 776[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:21.226 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 777[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:21.226 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 778[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:21.227 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 779[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:21.227 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 77a[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:22.061 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 77b[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.062 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 77c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.062 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 77d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.062 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 77e[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:22.062 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 77f[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.062 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 780[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.062 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 781[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.062 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 782[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:22.089 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 783[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.089 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 784[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.089 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 785[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:22.089 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 786[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:22.089 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 787[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:22.090 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 788[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.090 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 789[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.090 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 78a[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:22.090 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 78b[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:22.090 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 78c[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:22.340 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 78d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.340 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 78e[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:22.341 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 78f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.341 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 790[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:22.364 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 791[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.364 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 792[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.364 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 793[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.364 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 794[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:22.367 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 795[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.367 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 796[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.368 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 797[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.369 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 798[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:22.393 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 799[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.393 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 79a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.395 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 79c[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.393 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 79b[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.396 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 79d[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:22.397 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 79e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.397 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 79f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.397 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7a0[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:22.422 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 7a1[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.422 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7a2[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.422 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7a3[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:22.422 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 7a4[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:22.422 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 7a5[0m Closing Client
[36m2019-01-29 10:34:22.422 UTC [orderer/consensus/kafka] try -> DEBU 7a6[0m [channel: testchainid] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:22.422 UTC [orderer/consensus/kafka] try -> DEBU 7a7[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:34:22.422 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 7a8[0m Initializing new client
[36m2019-01-29 10:34:22.422 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 7a9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.422 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 7aa[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.422 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 7ab[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:22.423 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 7ac[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.423 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 7ad[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.423 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 7ae[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.423 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 7af[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:22.423 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 7b0[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.424 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7b1[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.424 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7b2[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:22.424 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 7b3[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:22.424 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 7b4[0m Closing Client
[36m2019-01-29 10:34:22.424 UTC [orderer/consensus/kafka] try -> DEBU 7b5[0m [channel: comunitychannel] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:22.424 UTC [orderer/consensus/kafka] try -> DEBU 7b6[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:34:22.424 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 7b7[0m Initializing new client
[36m2019-01-29 10:34:22.424 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 7b8[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.424 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 7b9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.424 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 7ba[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:22.452 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 7bb[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.452 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 7bd[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.453 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 7be[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.453 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 7bf[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:22.452 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 7bc[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.453 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 7c0[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.453 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 7c1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.453 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 7c2[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:22.479 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 7c4[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.479 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 7c3[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.479 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 7c5[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.479 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 7c6[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.479 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 7c8[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:22.479 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 7c7[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.479 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 7c9[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:22.479 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 7ca[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:22.480 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 7cb[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:22.509 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 7cc[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.509 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 7cd[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.509 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 7ce[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:22.509 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 7cf[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:22.509 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 7d0[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:22.730 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 7d1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.730 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7d2[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:22.758 UTC [orderer/common/server] Deliver -> DEBU 7d3[0m Starting new Deliver handler
[36m2019-01-29 10:34:22.758 UTC [common/deliver] Handle -> DEBU 7d4[0m Starting new deliver loop for 10.0.0.22:52286
[36m2019-01-29 10:34:22.758 UTC [common/deliver] Handle -> DEBU 7d5[0m Attempting to read seek info message from 10.0.0.22:52286
[33m2019-01-29 10:34:22.758 UTC [common/deliver] deliverBlocks -> WARN 7d6[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:52286 because of consenter error
[36m2019-01-29 10:34:22.758 UTC [common/deliver] Handle -> DEBU 7d7[0m Waiting for new SeekInfo from 10.0.0.22:52286
[36m2019-01-29 10:34:22.758 UTC [common/deliver] Handle -> DEBU 7d8[0m Attempting to read seek info message from 10.0.0.22:52286
[36m2019-01-29 10:34:22.759 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 7d9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.759 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7da[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:22.762 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 7db[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.763 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7dd[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.763 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 7de[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.763 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7df[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:22.763 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 7dc[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.763 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7e0[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.763 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 7e1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.763 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7e2[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:22.793 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 7e3[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.793 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7e4[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.793 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 7e5[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.793 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7e6[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:22.793 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 7e7[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.794 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 7e8[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.794 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7ea[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.794 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 7eb[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:22.794 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7ec[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:22.794 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7e9[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.795 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7ed[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:22.795 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 7ee[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:22.795 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 7ef[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:22.826 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 7f0[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.826 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7f1[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:22.826 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7f2[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:22.826 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 7f3[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:22.826 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 7f4[0m client/metadata retrying after 250ms... (2 attempts remaining)
[33m2019-01-29 10:34:22.959 UTC [common/deliver] Handle -> WARN 7f5[0m Error reading from 10.0.0.22:52286: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:34:22.960 UTC [orderer/common/server] func1 -> DEBU 7f6[0m Closing Deliver stream
[36m2019-01-29 10:34:23.045 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 7f7[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:23.045 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7f8[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:23.049 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 7f9[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.049 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7fa[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.049 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 7fb[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:23.049 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7fc[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:23.076 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 7fd[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:23.076 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 7fe[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:23.080 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 7ff[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.080 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 800[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.080 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 801[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.080 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 802[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.080 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 804[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:23.080 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 803[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:23.080 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 805[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:23.080 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 806[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:23.113 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 807[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.113 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 808[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.113 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 80a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.113 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 809[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.113 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 80c[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:23.113 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 80d[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:23.113 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 80b[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:23.114 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 80e[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:23.114 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 80f[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:23.152 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 810[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.153 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 811[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.153 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 812[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:23.153 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 813[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:23.154 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 814[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:23.365 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 815[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:23.365 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 816[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:23.404 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 817[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:23.404 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 818[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:23.415 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 819[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.415 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 81a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.416 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 81b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:23.416 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 81c[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:23.426 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 81e[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.426 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 81f[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.426 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 820[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:23.426 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 821[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:23.426 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 81d[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.426 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 822[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.426 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 823[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:23.426 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 824[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:23.441 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 825[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.441 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 826[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.441 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 828[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:23.441 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 827[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.441 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 82a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.441 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 82b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:23.442 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 82c[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:23.441 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 829[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:23.442 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 82d[0m Closing Client
[36m2019-01-29 10:34:23.442 UTC [orderer/consensus/kafka] try -> DEBU 82e[0m [channel: testchainid] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:23.442 UTC [orderer/consensus/kafka] try -> DEBU 82f[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:34:23.442 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 830[0m Initializing new client
[36m2019-01-29 10:34:23.442 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 831[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:23.442 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 832[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:23.442 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 833[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:23.478 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 834[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.478 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 835[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:23.478 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 836[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:23.478 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 837[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:23.478 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 838[0m Closing Client
[36m2019-01-29 10:34:23.478 UTC [orderer/consensus/kafka] try -> DEBU 839[0m [channel: comunitychannel] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:23.478 UTC [orderer/consensus/kafka] try -> DEBU 83a[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:34:23.478 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 83b[0m Initializing new client
[36m2019-01-29 10:34:23.478 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 83c[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:23.479 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 83d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:23.479 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 83e[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:24.275 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 83f[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.275 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 840[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.276 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 842[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:24.275 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 841[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.276 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 843[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:24.276 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 844[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.277 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 845[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:24.277 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 846[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:24.308 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 847[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.308 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 848[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.308 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 849[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:24.308 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 84a[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:24.309 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 84b[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.309 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 84c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.309 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 84d[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:24.309 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 84e[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:24.309 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 84f[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:24.309 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 850[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.309 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 851[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.309 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 852[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:24.309 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 853[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:24.337 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 854[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.337 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 855[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.338 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 856[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:24.338 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 857[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:24.338 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 858[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:24.559 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 859[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:24.560 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 85a[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:24.588 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 85b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:24.588 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 85c[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:24.599 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 85d[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.600 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 85e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.600 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 85f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:24.601 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 860[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.601 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 862[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.601 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 863[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:24.601 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 864[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:24.601 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 861[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:24.634 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 865[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.634 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 866[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.634 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 867[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:24.634 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 868[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:24.636 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 869[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.636 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 86a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.636 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 86b[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:24.637 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 86c[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:24.637 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 86d[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:24.637 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 86e[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.637 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 86f[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.638 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 870[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:24.638 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 871[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:24.669 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 872[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.669 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 873[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:24.669 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 874[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:24.670 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 875[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:24.670 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 876[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:24.887 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 877[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:24.887 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 878[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:24.920 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 879[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:24.920 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 87a[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:25.525 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 87c[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.525 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 87d[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.526 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 87e[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.526 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 87f[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:25.525 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 87b[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.526 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 880[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.527 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 881[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.527 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 882[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:25.557 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 883[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.557 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 884[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.557 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 885[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.557 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 886[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:25.558 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 887[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.558 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 888[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.558 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 889[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.558 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 88a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.558 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 88b[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:25.558 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 88d[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:25.558 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 88c[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.558 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 88e[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:25.558 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 88f[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:25.593 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 890[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.593 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 891[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.593 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 892[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:25.594 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 893[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:25.594 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 894[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:25.808 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 895[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.808 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 896[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:25.835 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 897[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.835 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 898[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.835 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 899[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.835 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 89a[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:25.844 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 89b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.844 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 89c[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:25.845 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 89d[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.845 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 89e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.845 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 89f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.845 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8a0[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:25.849 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8a1[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.849 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8a2[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.849 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8a3[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:25.849 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 8a4[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:25.849 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 8a5[0m Closing Client
[36m2019-01-29 10:34:25.849 UTC [orderer/consensus/kafka] try -> DEBU 8a6[0m [channel: testchainid] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:25.849 UTC [orderer/consensus/kafka] try -> DEBU 8a7[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:34:25.849 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 8a8[0m Initializing new client
[36m2019-01-29 10:34:25.849 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 8a9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.849 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 8aa[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.849 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 8ab[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:25.877 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8ac[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.877 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 8ad[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.877 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 8ae[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.877 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 8af[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:25.878 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8b0[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.878 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8b1[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.878 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 8b2[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.878 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8b3[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:25.909 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8b4[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.909 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8b5[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.909 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 8b6[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.909 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8b7[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:25.909 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8b8[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.909 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8b9[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.909 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8ba[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:25.909 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 8bb[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:25.910 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 8bc[0m Closing Client
[36m2019-01-29 10:34:25.910 UTC [orderer/consensus/kafka] try -> DEBU 8bd[0m [channel: comunitychannel] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:25.910 UTC [orderer/consensus/kafka] try -> DEBU 8be[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:34:25.910 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 8bf[0m Initializing new client
[36m2019-01-29 10:34:25.910 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 8c0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.910 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8c1[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.910 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 8c2[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.910 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 8c3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.910 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 8c4[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.910 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 8c5[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:25.910 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 8c6[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:25.946 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8c7[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.946 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 8c9[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.946 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 8ca[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:25.946 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 8cb[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:25.946 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 8cc[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:25.946 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8c8[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.946 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 8cd[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.946 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 8ce[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.947 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 8cf[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:25.976 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8d0[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.976 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 8d1[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:25.976 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 8d2[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:25.976 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 8d3[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:26.004 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8d4[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.004 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 8d5[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.004 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 8d6[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:26.004 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 8d7[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:26.004 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 8d8[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:26.197 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 8d9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.197 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8da[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:26.219 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8db[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.219 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8dc[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.219 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 8dd[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.219 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8de[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:26.251 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8df[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.251 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8e0[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.251 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 8e1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.251 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8e2[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:26.254 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 8e3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.254 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8e4[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:26.281 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8e6[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.281 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8e5[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.281 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8e7[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.281 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8e9[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:26.281 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 8ea[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:26.281 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8e8[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.281 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 8ec[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.281 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8ed[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:26.281 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 8eb[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:26.312 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8ee[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.312 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8ef[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.312 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 8f0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.312 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8f1[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:26.345 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8f2[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.346 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8f3[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.346 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8f4[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:26.346 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 8f5[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:26.346 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 8f6[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:26.531 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 8f7[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.532 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8f8[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:26.570 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8f9[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.570 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8fa[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.570 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 8fb[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.570 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8fc[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:26.595 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 8fd[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.595 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 8fe[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.595 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 8ff[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.595 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 900[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:26.596 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 901[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.596 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 902[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:26.625 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 904[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.625 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 903[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.625 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 905[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.625 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 906[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.625 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 907[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:26.625 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 908[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.625 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 909[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:26.625 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 90a[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:26.625 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 90b[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:26.658 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 90c[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.658 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 90d[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.658 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 90e[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.658 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 90f[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:26.691 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 910[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.691 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 911[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.691 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 912[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:26.691 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 913[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:26.691 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 914[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:26.875 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 915[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.875 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 916[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:26.911 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 917[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.911 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 918[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.911 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 919[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.911 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 91a[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:26.939 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 91b[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.939 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 91c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.939 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 91d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.939 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 91e[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:26.941 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 91f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.941 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 920[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:26.968 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 922[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.968 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 921[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.968 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 923[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.968 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 924[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:26.968 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 925[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.968 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 926[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:26.968 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 927[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:26.968 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 928[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:26.968 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 929[0m Closing Client
[36m2019-01-29 10:34:26.968 UTC [orderer/consensus/kafka] try -> DEBU 92a[0m [channel: testchainid] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:26.968 UTC [orderer/consensus/kafka] try -> DEBU 92b[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:34:26.968 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 92c[0m Initializing new client
[36m2019-01-29 10:34:26.968 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 92d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.968 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 92e[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:26.968 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 92f[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:27.765 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 931[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:27.765 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 932[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:27.765 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 933[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:27.765 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 934[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:27.765 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 930[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:27.767 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 935[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:27.767 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 936[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:27.767 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 937[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:27.795 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 938[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:27.796 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 93a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:27.796 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 93b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:27.795 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 939[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:27.796 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 93c[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:27.796 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 93d[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:27.797 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 93e[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:27.797 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 93f[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:27.797 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 940[0m Closing Client
[36m2019-01-29 10:34:27.797 UTC [orderer/consensus/kafka] try -> DEBU 941[0m [channel: comunitychannel] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:27.797 UTC [orderer/consensus/kafka] try -> DEBU 942[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:34:27.797 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 943[0m Initializing new client
[36m2019-01-29 10:34:27.797 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 944[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:27.797 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 945[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:27.798 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 946[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:27.825 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 947[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:27.825 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 948[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:27.825 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 949[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:27.825 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 94a[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:27.826 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 94b[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:27.826 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 94c[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:27.826 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 94d[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:27.826 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 94e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:27.826 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 950[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:27.826 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 94f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:27.826 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 952[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:27.826 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 951[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:27.826 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 953[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:27.830 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 954[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:27.830 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 955[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:27.830 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 956[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:27.830 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 957[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:27.830 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 958[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:28.077 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 959[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:28.077 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 95a[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:28.081 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 95b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:28.081 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 95c[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:28.086 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 95d[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.086 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 95e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.086 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 95f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:28.087 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 960[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:28.096 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 961[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.096 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 962[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.097 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 963[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:28.097 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 964[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:28.110 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 965[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.110 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 966[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.110 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 967[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:28.111 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 968[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:28.123 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 969[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.123 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 96a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.123 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 96b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:28.123 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 96c[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:28.136 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 96d[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.137 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 96e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.137 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 96f[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:28.137 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 970[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:28.137 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 971[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:28.144 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 972[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.144 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 973[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.145 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 974[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:28.145 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 975[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:28.145 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 976[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:28.387 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 977[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:28.388 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 978[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:28.395 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 979[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:28.395 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 97a[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:28.415 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 97b[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.415 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 97c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.415 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 97d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:28.416 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 97e[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:28.421 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 97f[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.421 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 980[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.421 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 981[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:28.421 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 982[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:28.438 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 983[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.438 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 984[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.438 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 985[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:28.438 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 986[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:28.442 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 987[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.442 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 988[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.442 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 989[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:28.442 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 98a[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:28.461 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 98b[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.462 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 98c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.462 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 98d[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:28.462 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 98e[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:28.462 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 98f[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:28.467 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 990[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.467 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 991[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.467 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 992[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:28.468 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 993[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:28.468 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 994[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:28.713 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 995[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:28.713 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 996[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:28.718 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 997[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:28.718 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 998[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:28.748 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 999[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.748 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 99a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.748 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 99b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:28.748 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 99c[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:28.753 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 99d[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.753 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 99e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.753 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 99f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:28.753 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9a0[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:28.786 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9a1[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.786 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9a2[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:28.786 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 9a3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:28.786 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9a4[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:29.386 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9a5[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.386 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9a6[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.386 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9a7[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:29.386 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 9a8[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:29.386 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 9a9[0m Closing Client
[36m2019-01-29 10:34:29.386 UTC [orderer/consensus/kafka] try -> DEBU 9aa[0m [channel: testchainid] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:29.386 UTC [orderer/consensus/kafka] try -> DEBU 9ab[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:34:29.386 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 9ad[0m Initializing new client
[36m2019-01-29 10:34:29.387 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 9ae[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:29.386 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9ac[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.387 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9af[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.387 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 9b0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:29.387 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 9b1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:29.387 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 9b2[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:29.387 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9b3[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:29.417 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9b4[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.417 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 9b5[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.417 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 9b6[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:29.417 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 9b7[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:29.418 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9b8[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.418 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9b9[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.418 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9ba[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:29.418 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 9bb[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:29.418 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 9bc[0m Closing Client
[36m2019-01-29 10:34:29.418 UTC [orderer/consensus/kafka] try -> DEBU 9bd[0m [channel: comunitychannel] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:29.418 UTC [orderer/consensus/kafka] try -> DEBU 9be[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:34:29.418 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 9bf[0m Initializing new client
[36m2019-01-29 10:34:29.418 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 9c0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:29.418 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 9c1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:29.418 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 9c2[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:29.451 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9c3[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.451 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9c4[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.451 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 9c5[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.451 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 9c6[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.451 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 9c8[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:29.451 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 9c7[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:29.451 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 9c9[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:29.451 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 9ca[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:29.478 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9cb[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.478 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 9cc[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.478 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 9cd[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:29.478 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 9cf[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:29.478 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9ce[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.479 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 9d0[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.479 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 9d1[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:29.479 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 9d2[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:29.479 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 9d3[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:29.506 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9d4[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.506 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 9d5[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.506 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 9d6[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:29.506 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 9d7[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:29.506 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 9d8[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:29.729 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 9d9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:29.729 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9da[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:29.756 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 9db[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:29.757 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9dc[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:29.759 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9dd[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.759 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9de[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.760 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 9df[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:29.760 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9e0[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:29.783 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9e1[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.783 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9e3[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.783 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9e2[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.783 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 9e4[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:29.783 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9e5[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.783 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9e6[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:29.783 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 9e7[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:29.783 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9e8[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:29.814 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9e9[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.814 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9ea[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.814 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9eb[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:29.814 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 9ec[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:29.814 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 9ed[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:29.814 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9ee[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.814 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9ef[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.814 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 9f0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:29.814 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9f1[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:29.840 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9f2[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.840 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9f3[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:29.840 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9f4[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:29.840 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 9f5[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:29.840 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 9f6[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:30.064 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 9f7[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:30.065 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9f8[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:30.090 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 9f9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:30.090 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9fa[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:30.097 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9fb[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:30.097 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9fc[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:30.097 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 9fd[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:30.097 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 9fe[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:30.123 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 9ff[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:30.123 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a00[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:30.124 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a01[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:30.124 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a03[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:30.124 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a02[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:30.124 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a04[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:30.124 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a05[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:30.124 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a06[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:31.017 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a07[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.017 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a09[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.018 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a0a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.018 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a0b[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:31.017 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a08[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.019 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a0c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.020 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a0d[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:31.020 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a0e[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:31.020 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a0f[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:31.106 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a10[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.107 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a11[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.107 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a12[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:31.107 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a13[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:31.107 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a14[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:31.270 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a15[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.271 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a16[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:31.317 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a17[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.318 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a18[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.318 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a19[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.318 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a1a[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:31.357 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a1b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.357 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a1c[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:31.364 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a1d[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.364 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a1e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.364 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a20[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.364 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a1f[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.365 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a22[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.366 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a23[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.366 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a24[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:31.365 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a21[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:31.395 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a25[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.395 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a27[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.395 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a26[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.395 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a28[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:31.395 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a29[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:31.395 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a2a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.395 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU a2b[0m Closing Client
[36m2019-01-29 10:34:31.395 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a2c[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.396 UTC [orderer/consensus/kafka] try -> DEBU a2d[0m [channel: testchainid] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:31.396 UTC [orderer/consensus/kafka] retry -> DEBU a2f[0m [channel: testchainid] Switching to the long retry interval
[36m2019-01-29 10:34:31.396 UTC [orderer/consensus/kafka] try -> DEBU a30[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:34:31.396 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU a31[0m Initializing new client
[36m2019-01-29 10:34:31.396 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU a32[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.396 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a33[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.396 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU a34[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:31.396 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a2e[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:31.428 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a35[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.428 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a36[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.428 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a37[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:31.428 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a38[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:31.428 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU a39[0m Closing Client
[36m2019-01-29 10:34:31.428 UTC [orderer/consensus/kafka] try -> DEBU a3a[0m [channel: comunitychannel] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:31.428 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a3b[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.428 UTC [orderer/consensus/kafka] retry -> DEBU a3c[0m [channel: comunitychannel] Switching to the long retry interval
[36m2019-01-29 10:34:31.428 UTC [orderer/consensus/kafka] try -> DEBU a3d[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:34:31.429 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU a3f[0m Initializing new client
[36m2019-01-29 10:34:31.429 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU a40[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.429 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a41[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.429 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU a42[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:31.428 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU a3e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.430 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a43[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.430 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU a44[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:31.459 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a45[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.459 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU a46[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.459 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a47[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.459 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a48[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.459 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU a49[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.459 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU a4a[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:31.459 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a4b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.459 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU a4c[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:31.492 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a4e[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.492 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a4d[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.492 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU a4f[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.492 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU a50[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:31.492 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU a51[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.492 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a53[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.492 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU a54[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:31.492 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a52[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:31.493 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a55[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:31.525 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a56[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.525 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU a57[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.525 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU a58[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:31.527 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a59[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:31.527 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a5a[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:34:31.743 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a5b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.743 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a5c[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:31.774 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a5d[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.774 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a5e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.774 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a5f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.774 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a60[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:31.777 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a61[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.777 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a62[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:31.803 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a63[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.804 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a64[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.804 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a65[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.804 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a67[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.804 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a66[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.804 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a68[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:31.804 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a69[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.804 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a6a[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:31.835 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a6c[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.835 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a6b[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.835 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a6d[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.835 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a6f[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:31.835 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a6e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.836 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a70[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:31.836 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a71[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:31.836 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a72[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:31.836 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a73[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:31.869 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a74[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.869 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a75[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:31.869 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a76[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:31.870 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a77[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:31.870 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a78[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:34:32.086 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a79[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:32.086 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a7a[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:32.120 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a7b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:32.120 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a7c[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:32.134 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a7d[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.134 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a7e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.134 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a7f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:32.134 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a80[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:32.147 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a81[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.147 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a82[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.147 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a83[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.147 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a84[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:32.147 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a85[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.147 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a86[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:32.147 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a87[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:32.147 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a88[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:32.179 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a89[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.179 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a8a[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.179 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a8c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.180 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a8d[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:32.179 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a8b[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.180 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a8e[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:32.180 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a8f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:32.180 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a91[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:32.180 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a90[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:32.210 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a92[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.210 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a93[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.210 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a94[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:32.211 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a95[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:32.211 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU a96[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:34:32.430 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a97[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:32.431 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a98[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:32.461 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a99[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:32.461 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a9a[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:32.471 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a9b[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.471 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a9c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.471 UTC [orderer/consensus/kafka/sarama] Open -> DEBU a9d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:32.471 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU a9e[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:34:32.486 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU a9f[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.486 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU aa0[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.486 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU aa1[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.486 UTC [orderer/consensus/kafka/sarama] Open -> DEBU aa3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:32.486 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU aa2[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.487 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU aa4[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:32.487 UTC [orderer/consensus/kafka/sarama] Open -> DEBU aa5[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:32.487 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU aa6[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:34:32.521 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU aa7[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.521 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU aa8[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.521 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU aa9[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.521 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU aaa[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.521 UTC [orderer/consensus/kafka/sarama] Open -> DEBU aab[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:34:32.521 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU aac[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:32.521 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU aad[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:34:32.521 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU aae[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:32.521 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU aaf[0m Closing Client
[36m2019-01-29 10:34:32.521 UTC [orderer/consensus/kafka] try -> DEBU ab0[0m [channel: testchainid] Initial attempt failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:32.521 UTC [orderer/consensus/kafka] try -> DEBU ab1[0m [channel: testchainid] Retrying every 5m0s for a total of 12h0m0s
[36m2019-01-29 10:34:32.553 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU ab2[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.553 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU ab3[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:34:32.553 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU ab4[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:34:32.554 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU ab5[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:34:32.554 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU ab6[0m Closing Client
[36m2019-01-29 10:34:32.554 UTC [orderer/consensus/kafka] try -> DEBU ab7[0m [channel: comunitychannel] Initial attempt failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:34:32.554 UTC [orderer/consensus/kafka] try -> DEBU ab8[0m [channel: comunitychannel] Retrying every 5m0s for a total of 12h0m0s
[36m2019-01-29 10:34:43.987 UTC [orderer/common/server] Deliver -> DEBU ab9[0m Starting new Deliver handler
[36m2019-01-29 10:34:43.987 UTC [common/deliver] Handle -> DEBU aba[0m Starting new deliver loop for 10.0.0.22:52324
[36m2019-01-29 10:34:43.987 UTC [common/deliver] Handle -> DEBU abb[0m Attempting to read seek info message from 10.0.0.22:52324
[33m2019-01-29 10:34:43.987 UTC [common/deliver] deliverBlocks -> WARN abc[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:52324 because of consenter error
[36m2019-01-29 10:34:43.987 UTC [common/deliver] Handle -> DEBU abd[0m Waiting for new SeekInfo from 10.0.0.22:52324
[36m2019-01-29 10:34:43.987 UTC [common/deliver] Handle -> DEBU abe[0m Attempting to read seek info message from 10.0.0.22:52324
[33m2019-01-29 10:34:44.389 UTC [common/deliver] Handle -> WARN abf[0m Error reading from 10.0.0.22:52324: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:34:44.389 UTC [orderer/common/server] func1 -> DEBU ac0[0m Closing Deliver stream
[36m2019-01-29 10:35:05.419 UTC [orderer/common/server] Deliver -> DEBU ac1[0m Starting new Deliver handler
[36m2019-01-29 10:35:05.419 UTC [common/deliver] Handle -> DEBU ac2[0m Starting new deliver loop for 10.0.0.22:52368
[36m2019-01-29 10:35:05.419 UTC [common/deliver] Handle -> DEBU ac3[0m Attempting to read seek info message from 10.0.0.22:52368
[33m2019-01-29 10:35:05.419 UTC [common/deliver] deliverBlocks -> WARN ac4[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:52368 because of consenter error
[36m2019-01-29 10:35:05.419 UTC [common/deliver] Handle -> DEBU ac5[0m Waiting for new SeekInfo from 10.0.0.22:52368
[36m2019-01-29 10:35:05.419 UTC [common/deliver] Handle -> DEBU ac6[0m Attempting to read seek info message from 10.0.0.22:52368
[33m2019-01-29 10:35:06.220 UTC [common/deliver] Handle -> WARN ac7[0m Error reading from 10.0.0.22:52368: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:35:06.220 UTC [orderer/common/server] func1 -> DEBU ac8[0m Closing Deliver stream
[36m2019-01-29 10:35:26.678 UTC [orderer/common/server] Deliver -> DEBU ac9[0m Starting new Deliver handler
[36m2019-01-29 10:35:26.678 UTC [common/deliver] Handle -> DEBU aca[0m Starting new deliver loop for 10.0.0.22:52406
[36m2019-01-29 10:35:26.678 UTC [common/deliver] Handle -> DEBU acb[0m Attempting to read seek info message from 10.0.0.22:52406
[33m2019-01-29 10:35:26.678 UTC [common/deliver] deliverBlocks -> WARN acc[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:52406 because of consenter error
[36m2019-01-29 10:35:26.679 UTC [common/deliver] Handle -> DEBU acd[0m Waiting for new SeekInfo from 10.0.0.22:52406
[36m2019-01-29 10:35:26.679 UTC [common/deliver] Handle -> DEBU ace[0m Attempting to read seek info message from 10.0.0.22:52406
[33m2019-01-29 10:35:28.280 UTC [common/deliver] Handle -> WARN acf[0m Error reading from 10.0.0.22:52406: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:35:28.280 UTC [orderer/common/server] func1 -> DEBU ad0[0m Closing Deliver stream
[36m2019-01-29 10:35:46.302 UTC [orderer/common/server] Deliver -> DEBU ad1[0m Starting new Deliver handler
[36m2019-01-29 10:35:46.302 UTC [common/deliver] Handle -> DEBU ad2[0m Starting new deliver loop for 10.0.0.22:52466
[36m2019-01-29 10:35:46.302 UTC [common/deliver] Handle -> DEBU ad3[0m Attempting to read seek info message from 10.0.0.22:52466
[33m2019-01-29 10:35:46.302 UTC [common/deliver] deliverBlocks -> WARN ad4[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:52466 because of consenter error
[36m2019-01-29 10:35:46.302 UTC [common/deliver] Handle -> DEBU ad5[0m Waiting for new SeekInfo from 10.0.0.22:52466
[36m2019-01-29 10:35:46.303 UTC [common/deliver] Handle -> DEBU ad6[0m Attempting to read seek info message from 10.0.0.22:52466
[33m2019-01-29 10:35:49.504 UTC [common/deliver] Handle -> WARN ad7[0m Error reading from 10.0.0.22:52466: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:35:49.504 UTC [orderer/common/server] func1 -> DEBU ad8[0m Closing Deliver stream
[36m2019-01-29 10:36:10.526 UTC [orderer/common/server] Deliver -> DEBU ad9[0m Starting new Deliver handler
[36m2019-01-29 10:36:10.526 UTC [common/deliver] Handle -> DEBU ada[0m Starting new deliver loop for 10.0.0.22:52626
[36m2019-01-29 10:36:10.526 UTC [common/deliver] Handle -> DEBU adb[0m Attempting to read seek info message from 10.0.0.22:52626
[33m2019-01-29 10:36:10.526 UTC [common/deliver] deliverBlocks -> WARN adc[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:52626 because of consenter error
[36m2019-01-29 10:36:10.526 UTC [common/deliver] Handle -> DEBU add[0m Waiting for new SeekInfo from 10.0.0.22:52626
[36m2019-01-29 10:36:10.526 UTC [common/deliver] Handle -> DEBU ade[0m Attempting to read seek info message from 10.0.0.22:52626
[33m2019-01-29 10:36:16.927 UTC [common/deliver] Handle -> WARN adf[0m Error reading from 10.0.0.22:52626: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:36:16.927 UTC [orderer/common/server] func1 -> DEBU ae0[0m Closing Deliver stream
[36m2019-01-29 10:36:34.949 UTC [orderer/common/server] Deliver -> DEBU ae1[0m Starting new Deliver handler
[36m2019-01-29 10:36:34.949 UTC [common/deliver] Handle -> DEBU ae2[0m Starting new deliver loop for 10.0.0.22:52702
[36m2019-01-29 10:36:34.949 UTC [common/deliver] Handle -> DEBU ae3[0m Attempting to read seek info message from 10.0.0.22:52702
[33m2019-01-29 10:36:34.949 UTC [common/deliver] deliverBlocks -> WARN ae4[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:52702 because of consenter error
[36m2019-01-29 10:36:34.950 UTC [common/deliver] Handle -> DEBU ae5[0m Waiting for new SeekInfo from 10.0.0.22:52702
[36m2019-01-29 10:36:34.950 UTC [common/deliver] Handle -> DEBU ae6[0m Attempting to read seek info message from 10.0.0.22:52702
[33m2019-01-29 10:36:44.951 UTC [common/deliver] Handle -> WARN ae7[0m Error reading from 10.0.0.22:52702: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:36:44.951 UTC [orderer/common/server] func1 -> DEBU ae8[0m Closing Deliver stream
[36m2019-01-29 10:37:00.653 UTC [orderer/common/server] Deliver -> DEBU ae9[0m Starting new Deliver handler
[36m2019-01-29 10:37:00.653 UTC [common/deliver] Handle -> DEBU aea[0m Starting new deliver loop for 10.0.0.22:52732
[36m2019-01-29 10:37:00.653 UTC [common/deliver] Handle -> DEBU aeb[0m Attempting to read seek info message from 10.0.0.22:52732
[33m2019-01-29 10:37:00.654 UTC [common/deliver] deliverBlocks -> WARN aec[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:52732 because of consenter error
[36m2019-01-29 10:37:00.654 UTC [common/deliver] Handle -> DEBU aed[0m Waiting for new SeekInfo from 10.0.0.22:52732
[36m2019-01-29 10:37:00.654 UTC [common/deliver] Handle -> DEBU aee[0m Attempting to read seek info message from 10.0.0.22:52732
[33m2019-01-29 10:37:10.654 UTC [common/deliver] Handle -> WARN aef[0m Error reading from 10.0.0.22:52732: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:37:10.655 UTC [orderer/common/server] func1 -> DEBU af0[0m Closing Deliver stream
[36m2019-01-29 10:37:28.674 UTC [orderer/common/server] Deliver -> DEBU af1[0m Starting new Deliver handler
[36m2019-01-29 10:37:28.674 UTC [common/deliver] Handle -> DEBU af2[0m Starting new deliver loop for 10.0.0.22:52824
[36m2019-01-29 10:37:28.674 UTC [common/deliver] Handle -> DEBU af3[0m Attempting to read seek info message from 10.0.0.22:52824
[33m2019-01-29 10:37:28.674 UTC [common/deliver] deliverBlocks -> WARN af4[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:52824 because of consenter error
[36m2019-01-29 10:37:28.674 UTC [common/deliver] Handle -> DEBU af5[0m Waiting for new SeekInfo from 10.0.0.22:52824
[36m2019-01-29 10:37:28.674 UTC [common/deliver] Handle -> DEBU af6[0m Attempting to read seek info message from 10.0.0.22:52824
[33m2019-01-29 10:37:38.676 UTC [common/deliver] Handle -> WARN af7[0m Error reading from 10.0.0.22:52824: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:37:38.677 UTC [orderer/common/server] func1 -> DEBU af8[0m Closing Deliver stream
[36m2019-01-29 10:37:53.693 UTC [orderer/common/server] Deliver -> DEBU af9[0m Starting new Deliver handler
[36m2019-01-29 10:37:53.693 UTC [common/deliver] Handle -> DEBU afa[0m Starting new deliver loop for 10.0.0.22:52856
[36m2019-01-29 10:37:53.694 UTC [common/deliver] Handle -> DEBU afb[0m Attempting to read seek info message from 10.0.0.22:52856
[33m2019-01-29 10:37:53.694 UTC [common/deliver] deliverBlocks -> WARN afc[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:52856 because of consenter error
[36m2019-01-29 10:37:53.694 UTC [common/deliver] Handle -> DEBU afd[0m Waiting for new SeekInfo from 10.0.0.22:52856
[36m2019-01-29 10:37:53.694 UTC [common/deliver] Handle -> DEBU afe[0m Attempting to read seek info message from 10.0.0.22:52856
[33m2019-01-29 10:38:03.695 UTC [common/deliver] Handle -> WARN aff[0m Error reading from 10.0.0.22:52856: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:38:03.695 UTC [orderer/common/server] func1 -> DEBU b00[0m Closing Deliver stream
[36m2019-01-29 10:38:24.725 UTC [orderer/common/server] Deliver -> DEBU b01[0m Starting new Deliver handler
[36m2019-01-29 10:38:24.726 UTC [common/deliver] Handle -> DEBU b02[0m Starting new deliver loop for 10.0.0.22:52894
[36m2019-01-29 10:38:24.726 UTC [common/deliver] Handle -> DEBU b03[0m Attempting to read seek info message from 10.0.0.22:52894
[33m2019-01-29 10:38:24.726 UTC [common/deliver] deliverBlocks -> WARN b04[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:52894 because of consenter error
[36m2019-01-29 10:38:24.726 UTC [common/deliver] Handle -> DEBU b05[0m Waiting for new SeekInfo from 10.0.0.22:52894
[36m2019-01-29 10:38:24.726 UTC [common/deliver] Handle -> DEBU b06[0m Attempting to read seek info message from 10.0.0.22:52894
[33m2019-01-29 10:38:34.727 UTC [common/deliver] Handle -> WARN b07[0m Error reading from 10.0.0.22:52894: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:38:34.727 UTC [orderer/common/server] func1 -> DEBU b08[0m Closing Deliver stream
[36m2019-01-29 10:38:55.755 UTC [orderer/common/server] Deliver -> DEBU b09[0m Starting new Deliver handler
[36m2019-01-29 10:38:55.755 UTC [common/deliver] Handle -> DEBU b0a[0m Starting new deliver loop for 10.0.0.22:52932
[36m2019-01-29 10:38:55.755 UTC [common/deliver] Handle -> DEBU b0b[0m Attempting to read seek info message from 10.0.0.22:52932
[33m2019-01-29 10:38:55.755 UTC [common/deliver] deliverBlocks -> WARN b0c[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:52932 because of consenter error
[36m2019-01-29 10:38:55.755 UTC [common/deliver] Handle -> DEBU b0d[0m Waiting for new SeekInfo from 10.0.0.22:52932
[36m2019-01-29 10:38:55.755 UTC [common/deliver] Handle -> DEBU b0e[0m Attempting to read seek info message from 10.0.0.22:52932
[33m2019-01-29 10:39:05.757 UTC [common/deliver] Handle -> WARN b0f[0m Error reading from 10.0.0.22:52932: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:39:05.757 UTC [orderer/common/server] func1 -> DEBU b10[0m Closing Deliver stream
[36m2019-01-29 10:39:23.784 UTC [orderer/common/server] Deliver -> DEBU b11[0m Starting new Deliver handler
[36m2019-01-29 10:39:23.784 UTC [common/deliver] Handle -> DEBU b12[0m Starting new deliver loop for 10.0.0.22:52964
[36m2019-01-29 10:39:23.784 UTC [common/deliver] Handle -> DEBU b13[0m Attempting to read seek info message from 10.0.0.22:52964
[33m2019-01-29 10:39:23.784 UTC [common/deliver] deliverBlocks -> WARN b14[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:52964 because of consenter error
[36m2019-01-29 10:39:23.784 UTC [common/deliver] Handle -> DEBU b15[0m Waiting for new SeekInfo from 10.0.0.22:52964
[36m2019-01-29 10:39:23.784 UTC [common/deliver] Handle -> DEBU b16[0m Attempting to read seek info message from 10.0.0.22:52964
[36m2019-01-29 10:39:32.631 UTC [orderer/consensus/kafka] try -> DEBU b17[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:39:32.632 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU b18[0m Initializing new client
[36m2019-01-29 10:39:32.632 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU b19[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:32.632 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b1a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:32.632 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU b1b[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:39:32.660 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b1c[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:32.660 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU b1d[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:32.660 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b1e[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:32.660 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU b1f[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:39:32.664 UTC [orderer/consensus/kafka] try -> DEBU b20[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:39:32.664 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU b21[0m Initializing new client
[36m2019-01-29 10:39:32.664 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU b22[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:32.664 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b23[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:32.664 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU b24[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:39:32.689 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b25[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:32.689 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU b26[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:32.690 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b27[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:32.690 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU b28[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:39:32.690 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b29[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:32.691 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU b2a[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:32.691 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b2b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:32.691 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU b2c[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:39:32.716 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b2d[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:32.716 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b2e[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:32.716 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU b2f[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:32.716 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b31[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:32.716 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU b32[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:39:32.716 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU b30[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:32.716 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU b33[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:39:32.716 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU b34[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:39:32.716 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU b35[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:39:32.745 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b36[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:32.746 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU b37[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:32.746 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU b38[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:39:32.746 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU b39[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:39:32.746 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU b3a[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 10:39:32.967 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b3b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:32.967 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b3c[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:39:32.996 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b3d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:32.996 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b3e[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:39:32.998 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b40[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:32.998 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b41[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:32.998 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b42[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:32.998 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b43[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:39:32.998 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b3f[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:32.999 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b44[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:32.999 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b45[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:32.999 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b46[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:39:33.027 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b47[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:33.027 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b48[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:33.027 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b49[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:33.028 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b4a[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:39:33.028 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b4b[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:33.028 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b4c[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:33.028 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b4d[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:33.028 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b4f[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:39:33.028 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b4e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:33.028 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b51[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:33.028 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b52[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:39:33.028 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU b50[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:39:33.029 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU b53[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:39:33.279 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b54[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:33.279 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b55[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:39:33.357 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b56[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:33.357 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b57[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:33.357 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b58[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:33.357 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b59[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[33m2019-01-29 10:39:33.785 UTC [common/deliver] Handle -> WARN b5a[0m Error reading from 10.0.0.22:52964: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:39:33.785 UTC [orderer/common/server] func1 -> DEBU b5b[0m Closing Deliver stream
[36m2019-01-29 10:39:33.872 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b5c[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:33.872 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b5e[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:33.872 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b5f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:33.873 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b60[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:39:33.872 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b5d[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:33.873 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b61[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:33.873 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b62[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:39:33.873 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU b63[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:39:33.873 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU b64[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 10:39:33.903 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b65[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:33.903 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b66[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:33.903 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b67[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:39:33.903 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU b68[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:39:33.903 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU b69[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:39:34.123 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b6a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:34.124 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b6b[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:39:34.153 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b6c[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:34.154 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b6d[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:39:34.154 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b6e[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.157 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b6f[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.157 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b70[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:34.157 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b71[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:39:34.184 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b72[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.184 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b73[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.184 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b74[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:34.184 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b75[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:39:34.187 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b76[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.187 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b77[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.187 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b78[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:34.187 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b79[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:39:34.208 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b7b[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.208 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b7a[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.208 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b7c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.208 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b7d[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.208 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b7e[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:39:34.208 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b7f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:34.208 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU b80[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:39:34.208 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b81[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:39:34.208 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU b82[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 10:39:34.253 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b83[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.253 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b84[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.253 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b85[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:39:34.253 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU b86[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:39:34.253 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU b87[0m Closing Client
[36m2019-01-29 10:39:34.253 UTC [orderer/consensus/kafka] try -> DEBU b88[0m [channel: testchainid] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:39:34.458 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b89[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:34.458 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b8a[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:39:34.496 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b8b[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.496 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b8c[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka2.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.496 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b8d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:34.496 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b8e[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:39:34.532 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b8f[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.532 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b90[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka0.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.532 UTC [orderer/consensus/kafka/sarama] Open -> DEBU b91[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:39:34.532 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b92[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:39:34.566 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU b93[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.566 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b94[0m client/metadata got error from broker while fetching metadata: dial tcp: lookup kafka1.switch2logic.co.za on 127.0.0.11:53: no such host
[36m2019-01-29 10:39:34.566 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU b95[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 10:39:34.566 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU b96[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 10:39:34.566 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU b97[0m Closing Client
[36m2019-01-29 10:39:34.566 UTC [orderer/consensus/kafka] try -> DEBU b98[0m [channel: comunitychannel] Need to retry because process failed = kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[36m2019-01-29 10:39:54.806 UTC [orderer/common/server] Deliver -> DEBU b99[0m Starting new Deliver handler
[36m2019-01-29 10:39:54.806 UTC [common/deliver] Handle -> DEBU b9a[0m Starting new deliver loop for 10.0.0.22:53022
[36m2019-01-29 10:39:54.806 UTC [common/deliver] Handle -> DEBU b9b[0m Attempting to read seek info message from 10.0.0.22:53022
[33m2019-01-29 10:39:54.806 UTC [common/deliver] deliverBlocks -> WARN b9c[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:53022 because of consenter error
[36m2019-01-29 10:39:54.806 UTC [common/deliver] Handle -> DEBU b9d[0m Waiting for new SeekInfo from 10.0.0.22:53022
[36m2019-01-29 10:39:54.807 UTC [common/deliver] Handle -> DEBU b9e[0m Attempting to read seek info message from 10.0.0.22:53022
[33m2019-01-29 10:40:04.808 UTC [common/deliver] Handle -> WARN b9f[0m Error reading from 10.0.0.22:53022: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:40:04.808 UTC [orderer/common/server] func1 -> DEBU ba0[0m Closing Deliver stream
[36m2019-01-29 10:40:25.836 UTC [orderer/common/server] Deliver -> DEBU ba1[0m Starting new Deliver handler
[36m2019-01-29 10:40:25.836 UTC [common/deliver] Handle -> DEBU ba2[0m Starting new deliver loop for 10.0.0.22:53062
[36m2019-01-29 10:40:25.836 UTC [common/deliver] Handle -> DEBU ba3[0m Attempting to read seek info message from 10.0.0.22:53062
[33m2019-01-29 10:40:25.836 UTC [common/deliver] deliverBlocks -> WARN ba4[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:53062 because of consenter error
[36m2019-01-29 10:40:25.836 UTC [common/deliver] Handle -> DEBU ba5[0m Waiting for new SeekInfo from 10.0.0.22:53062
[36m2019-01-29 10:40:25.836 UTC [common/deliver] Handle -> DEBU ba6[0m Attempting to read seek info message from 10.0.0.22:53062
[33m2019-01-29 10:40:35.838 UTC [common/deliver] Handle -> WARN ba7[0m Error reading from 10.0.0.22:53062: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:40:35.838 UTC [orderer/common/server] func1 -> DEBU ba8[0m Closing Deliver stream
[36m2019-01-29 10:40:50.861 UTC [orderer/common/server] Deliver -> DEBU ba9[0m Starting new Deliver handler
[36m2019-01-29 10:40:50.861 UTC [common/deliver] Handle -> DEBU baa[0m Starting new deliver loop for 10.0.0.22:53090
[36m2019-01-29 10:40:50.861 UTC [common/deliver] Handle -> DEBU bab[0m Attempting to read seek info message from 10.0.0.22:53090
[33m2019-01-29 10:40:50.861 UTC [common/deliver] deliverBlocks -> WARN bac[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:53090 because of consenter error
[36m2019-01-29 10:40:50.861 UTC [common/deliver] Handle -> DEBU bad[0m Waiting for new SeekInfo from 10.0.0.22:53090
[36m2019-01-29 10:40:50.861 UTC [common/deliver] Handle -> DEBU bae[0m Attempting to read seek info message from 10.0.0.22:53090
[33m2019-01-29 10:41:00.863 UTC [common/deliver] Handle -> WARN baf[0m Error reading from 10.0.0.22:53090: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:41:00.863 UTC [orderer/common/server] func1 -> DEBU bb0[0m Closing Deliver stream
[36m2019-01-29 10:41:15.884 UTC [orderer/common/server] Deliver -> DEBU bb1[0m Starting new Deliver handler
[36m2019-01-29 10:41:15.884 UTC [common/deliver] Handle -> DEBU bb2[0m Starting new deliver loop for 10.0.0.22:53148
[36m2019-01-29 10:41:15.884 UTC [common/deliver] Handle -> DEBU bb3[0m Attempting to read seek info message from 10.0.0.22:53148
[33m2019-01-29 10:41:15.884 UTC [common/deliver] deliverBlocks -> WARN bb4[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:53148 because of consenter error
[36m2019-01-29 10:41:15.884 UTC [common/deliver] Handle -> DEBU bb5[0m Waiting for new SeekInfo from 10.0.0.22:53148
[36m2019-01-29 10:41:15.885 UTC [common/deliver] Handle -> DEBU bb6[0m Attempting to read seek info message from 10.0.0.22:53148
[33m2019-01-29 10:41:25.886 UTC [common/deliver] Handle -> WARN bb7[0m Error reading from 10.0.0.22:53148: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:41:25.886 UTC [orderer/common/server] func1 -> DEBU bb8[0m Closing Deliver stream
[36m2019-01-29 10:41:32.767 UTC [grpc] Printf -> DEBU bb9[0m grpc: Server.Serve failed to complete security handshake from "10.0.0.25:52112": tls: first record does not look like a TLS handshake
[36m2019-01-29 10:41:33.769 UTC [grpc] Printf -> DEBU bba[0m grpc: Server.Serve failed to complete security handshake from "10.0.0.25:52116": tls: first record does not look like a TLS handshake
[36m2019-01-29 10:41:40.910 UTC [orderer/common/server] Deliver -> DEBU bbb[0m Starting new Deliver handler
[36m2019-01-29 10:41:40.910 UTC [common/deliver] Handle -> DEBU bbc[0m Starting new deliver loop for 10.0.0.22:53190
[36m2019-01-29 10:41:40.910 UTC [common/deliver] Handle -> DEBU bbd[0m Attempting to read seek info message from 10.0.0.22:53190
[33m2019-01-29 10:41:40.911 UTC [common/deliver] deliverBlocks -> WARN bbe[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:53190 because of consenter error
[36m2019-01-29 10:41:40.911 UTC [common/deliver] Handle -> DEBU bbf[0m Waiting for new SeekInfo from 10.0.0.22:53190
[36m2019-01-29 10:41:40.911 UTC [common/deliver] Handle -> DEBU bc0[0m Attempting to read seek info message from 10.0.0.22:53190
[33m2019-01-29 10:41:50.912 UTC [common/deliver] Handle -> WARN bc1[0m Error reading from 10.0.0.22:53190: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:41:50.912 UTC [orderer/common/server] func1 -> DEBU bc2[0m Closing Deliver stream
[36m2019-01-29 10:42:11.941 UTC [orderer/common/server] Deliver -> DEBU bc3[0m Starting new Deliver handler
[36m2019-01-29 10:42:11.941 UTC [common/deliver] Handle -> DEBU bc4[0m Starting new deliver loop for 10.0.0.22:53234
[36m2019-01-29 10:42:11.941 UTC [common/deliver] Handle -> DEBU bc5[0m Attempting to read seek info message from 10.0.0.22:53234
[33m2019-01-29 10:42:11.942 UTC [common/deliver] deliverBlocks -> WARN bc6[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:53234 because of consenter error
[36m2019-01-29 10:42:11.942 UTC [common/deliver] Handle -> DEBU bc7[0m Waiting for new SeekInfo from 10.0.0.22:53234
[36m2019-01-29 10:42:11.942 UTC [common/deliver] Handle -> DEBU bc8[0m Attempting to read seek info message from 10.0.0.22:53234
[33m2019-01-29 10:42:21.943 UTC [common/deliver] Handle -> WARN bc9[0m Error reading from 10.0.0.22:53234: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:42:21.944 UTC [orderer/common/server] func1 -> DEBU bca[0m Closing Deliver stream
[36m2019-01-29 10:42:42.974 UTC [orderer/common/server] Deliver -> DEBU bcb[0m Starting new Deliver handler
[36m2019-01-29 10:42:42.974 UTC [common/deliver] Handle -> DEBU bcc[0m Starting new deliver loop for 10.0.0.22:53272
[36m2019-01-29 10:42:42.974 UTC [common/deliver] Handle -> DEBU bcd[0m Attempting to read seek info message from 10.0.0.22:53272
[33m2019-01-29 10:42:42.975 UTC [common/deliver] deliverBlocks -> WARN bce[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:53272 because of consenter error
[36m2019-01-29 10:42:42.975 UTC [common/deliver] Handle -> DEBU bcf[0m Waiting for new SeekInfo from 10.0.0.22:53272
[36m2019-01-29 10:42:42.975 UTC [common/deliver] Handle -> DEBU bd0[0m Attempting to read seek info message from 10.0.0.22:53272
[33m2019-01-29 10:42:52.976 UTC [common/deliver] Handle -> WARN bd1[0m Error reading from 10.0.0.22:53272: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:42:52.976 UTC [orderer/common/server] func1 -> DEBU bd2[0m Closing Deliver stream
[36m2019-01-29 10:43:03.258 UTC [grpc] Printf -> DEBU bd3[0m grpc: Server.Serve failed to complete security handshake from "10.0.0.25:52240": tls: first record does not look like a TLS handshake
[36m2019-01-29 10:43:04.260 UTC [grpc] Printf -> DEBU bd4[0m grpc: Server.Serve failed to complete security handshake from "10.0.0.25:52244": tls: first record does not look like a TLS handshake
[36m2019-01-29 10:43:05.964 UTC [grpc] Printf -> DEBU bd5[0m grpc: Server.Serve failed to complete security handshake from "10.0.0.25:52248": tls: first record does not look like a TLS handshake
[36m2019-01-29 10:43:13.997 UTC [orderer/common/server] Deliver -> DEBU bd6[0m Starting new Deliver handler
[36m2019-01-29 10:43:13.997 UTC [common/deliver] Handle -> DEBU bd7[0m Starting new deliver loop for 10.0.0.22:53324
[36m2019-01-29 10:43:13.997 UTC [common/deliver] Handle -> DEBU bd8[0m Attempting to read seek info message from 10.0.0.22:53324
[33m2019-01-29 10:43:13.997 UTC [common/deliver] deliverBlocks -> WARN bd9[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:53324 because of consenter error
[36m2019-01-29 10:43:13.998 UTC [common/deliver] Handle -> DEBU bda[0m Waiting for new SeekInfo from 10.0.0.22:53324
[36m2019-01-29 10:43:13.998 UTC [common/deliver] Handle -> DEBU bdb[0m Attempting to read seek info message from 10.0.0.22:53324
[33m2019-01-29 10:43:23.999 UTC [common/deliver] Handle -> WARN bdc[0m Error reading from 10.0.0.22:53324: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:43:24.000 UTC [orderer/common/server] func1 -> DEBU bdd[0m Closing Deliver stream
[36m2019-01-29 10:43:42.023 UTC [orderer/common/server] Deliver -> DEBU bde[0m Starting new Deliver handler
[36m2019-01-29 10:43:42.023 UTC [common/deliver] Handle -> DEBU bdf[0m Starting new deliver loop for 10.0.0.22:53364
[36m2019-01-29 10:43:42.023 UTC [common/deliver] Handle -> DEBU be0[0m Attempting to read seek info message from 10.0.0.22:53364
[33m2019-01-29 10:43:42.023 UTC [common/deliver] deliverBlocks -> WARN be1[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:53364 because of consenter error
[36m2019-01-29 10:43:42.023 UTC [common/deliver] Handle -> DEBU be2[0m Waiting for new SeekInfo from 10.0.0.22:53364
[36m2019-01-29 10:43:42.023 UTC [common/deliver] Handle -> DEBU be3[0m Attempting to read seek info message from 10.0.0.22:53364
[33m2019-01-29 10:43:52.025 UTC [common/deliver] Handle -> WARN be4[0m Error reading from 10.0.0.22:53364: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:43:52.025 UTC [orderer/common/server] func1 -> DEBU be5[0m Closing Deliver stream
[36m2019-01-29 10:44:07.047 UTC [orderer/common/server] Deliver -> DEBU be6[0m Starting new Deliver handler
[36m2019-01-29 10:44:07.047 UTC [common/deliver] Handle -> DEBU be7[0m Starting new deliver loop for 10.0.0.22:53390
[36m2019-01-29 10:44:07.047 UTC [common/deliver] Handle -> DEBU be8[0m Attempting to read seek info message from 10.0.0.22:53390
[33m2019-01-29 10:44:07.047 UTC [common/deliver] deliverBlocks -> WARN be9[0m [channel: comunitychannel] Rejecting deliver request for 10.0.0.22:53390 because of consenter error
[36m2019-01-29 10:44:07.047 UTC [common/deliver] Handle -> DEBU bea[0m Waiting for new SeekInfo from 10.0.0.22:53390
[36m2019-01-29 10:44:07.047 UTC [common/deliver] Handle -> DEBU beb[0m Attempting to read seek info message from 10.0.0.22:53390
[33m2019-01-29 10:44:17.049 UTC [common/deliver] Handle -> WARN bec[0m Error reading from 10.0.0.22:53390: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 10:44:17.049 UTC [orderer/common/server] func1 -> DEBU bed[0m Closing Deliver stream
[36m2019-01-29 10:44:32.631 UTC [orderer/consensus/kafka] try -> DEBU bee[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:32.631 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU bef[0m Initializing new client
[36m2019-01-29 10:44:32.631 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU bf0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:44:32.631 UTC [orderer/consensus/kafka/sarama] Open -> DEBU bf1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:44:32.632 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU bf2[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:44:32.633 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU bf3[0m Connected to broker at kafka0.switch2logic.co.za:9092 (unregistered)
[36m2019-01-29 10:44:32.636 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU bf4[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 10:44:32.636 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU bf5[0m client/brokers registered new broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 10:44:32.636 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU bf6[0m client/brokers registered new broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:44:32.636 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU bf7[0m Successfully initialized new client
[36m2019-01-29 10:44:32.636 UTC [orderer/consensus/kafka] try -> DEBU bf8[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 10:44:32.636 UTC [orderer/consensus/kafka] startThread -> INFO bf9[0m [channel: testchainid] Producer set up successfully
2019-01-29 10:44:32.636 UTC [orderer/consensus/kafka] sendConnectMessage -> INFO bfa[0m [channel: testchainid] About to post the CONNECT message...
[36m2019-01-29 10:44:32.636 UTC [orderer/consensus/kafka] try -> DEBU bfb[0m [channel: testchainid] Attempting to post the CONNECT message...
[36m2019-01-29 10:44:32.636 UTC [orderer/consensus/kafka/sarama] Open -> DEBU bfc[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:44:32.637 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU bfd[0m producer/broker/1 starting up
[36m2019-01-29 10:44:32.637 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU bfe[0m producer/broker/1 state change to [open] on testchainid/0
[36m2019-01-29 10:44:32.638 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU bff[0m Connected to broker at kafka1.switch2logic.co.za:9092 (registered as #1)
[36m2019-01-29 10:44:32.643 UTC [orderer/consensus/kafka] try -> DEBU c00[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 10:44:32.643 UTC [orderer/consensus/kafka] startThread -> INFO c01[0m [channel: testchainid] CONNECT message posted successfully
2019-01-29 10:44:32.643 UTC [orderer/consensus/kafka] setupParentConsumerForChannel -> INFO c02[0m [channel: testchainid] Setting up the parent consumer for this channel...
[36m2019-01-29 10:44:32.643 UTC [orderer/consensus/kafka] try -> DEBU c03[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:32.643 UTC [orderer/consensus/kafka/sarama] NewConsumer -> DEBU c04[0m Initializing new client
[36m2019-01-29 10:44:32.643 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU c05[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:44:32.643 UTC [orderer/consensus/kafka/sarama] Open -> DEBU c06[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:44:32.643 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU c07[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:44:32.644 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU c08[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.29:10092: connect: connection refused
[36m2019-01-29 10:44:32.644 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU c09[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.29:10092: connect: connection refused
[36m2019-01-29 10:44:32.644 UTC [orderer/consensus/kafka/sarama] Open -> DEBU c0a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:44:32.644 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU c0b[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:44:32.645 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU c0c[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.28:11092: connect: connection refused
[36m2019-01-29 10:44:32.645 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU c0d[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.28:11092: connect: connection refused
[36m2019-01-29 10:44:32.645 UTC [orderer/consensus/kafka/sarama] Open -> DEBU c0e[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:44:32.645 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU c0f[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:44:32.645 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU c10[0m Connected to broker at kafka0.switch2logic.co.za:9092 (unregistered)
[36m2019-01-29 10:44:32.647 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU c11[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 10:44:32.647 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU c12[0m client/brokers registered new broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 10:44:32.647 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU c13[0m client/brokers registered new broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:44:32.647 UTC [orderer/consensus/kafka/sarama] NewConsumer -> DEBU c14[0m Successfully initialized new client
[36m2019-01-29 10:44:32.647 UTC [orderer/consensus/kafka] try -> DEBU c15[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 10:44:32.647 UTC [orderer/consensus/kafka] startThread -> INFO c16[0m [channel: testchainid] Parent consumer set up successfully
2019-01-29 10:44:32.647 UTC [orderer/consensus/kafka] setupChannelConsumerForChannel -> INFO c17[0m [channel: testchainid] Setting up the channel consumer for this channel (start offset: 6)...
[36m2019-01-29 10:44:32.647 UTC [orderer/consensus/kafka] try -> DEBU c18[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:32.647 UTC [orderer/consensus/kafka/sarama] Open -> DEBU c19[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:44:32.648 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU c1a[0m Connected to broker at kafka1.switch2logic.co.za:9092 (registered as #1)
[36m2019-01-29 10:44:32.649 UTC [orderer/consensus/kafka] try -> DEBU c1b[0m [channel: testchainid] Initial attempt failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:32.649 UTC [orderer/consensus/kafka] try -> DEBU c1c[0m [channel: testchainid] Retrying every 1s for a total of 30s
[36m2019-01-29 10:44:32.664 UTC [orderer/consensus/kafka] try -> DEBU c1d[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:44:32.664 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU c1e[0m Initializing new client
[36m2019-01-29 10:44:32.664 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU c1f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:44:32.664 UTC [orderer/consensus/kafka/sarama] Open -> DEBU c20[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:44:32.664 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU c21[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:44:32.665 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU c22[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.28:11092: connect: connection refused
[36m2019-01-29 10:44:32.665 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU c23[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.28:11092: connect: connection refused
[36m2019-01-29 10:44:32.665 UTC [orderer/consensus/kafka/sarama] Open -> DEBU c24[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:44:32.665 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU c25[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:44:32.666 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU c26[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.29:10092: connect: connection refused
[36m2019-01-29 10:44:32.666 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU c27[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.29:10092: connect: connection refused
[36m2019-01-29 10:44:32.666 UTC [orderer/consensus/kafka/sarama] Open -> DEBU c28[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:44:32.666 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU c29[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:44:32.666 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU c2a[0m Connected to broker at kafka0.switch2logic.co.za:9092 (unregistered)
[36m2019-01-29 10:44:32.669 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU c2b[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 10:44:32.669 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU c2c[0m client/brokers registered new broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 10:44:32.669 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU c2d[0m client/brokers registered new broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:44:32.669 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU c2e[0m Successfully initialized new client
[36m2019-01-29 10:44:32.669 UTC [orderer/consensus/kafka] try -> DEBU c2f[0m [channel: comunitychannel] Error is nil, breaking the retry loop
2019-01-29 10:44:32.669 UTC [orderer/consensus/kafka] startThread -> INFO c30[0m [channel: comunitychannel] Producer set up successfully
2019-01-29 10:44:32.669 UTC [orderer/consensus/kafka] sendConnectMessage -> INFO c31[0m [channel: comunitychannel] About to post the CONNECT message...
[36m2019-01-29 10:44:32.669 UTC [orderer/consensus/kafka] try -> DEBU c32[0m [channel: comunitychannel] Attempting to post the CONNECT message...
[36m2019-01-29 10:44:32.669 UTC [orderer/consensus/kafka/sarama] Open -> DEBU c33[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:44:32.669 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU c34[0m producer/broker/0 starting up
[36m2019-01-29 10:44:32.670 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU c35[0m producer/broker/0 state change to [open] on comunitychannel/0
[36m2019-01-29 10:44:32.670 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU c36[0m Connected to broker at kafka0.switch2logic.co.za:9092 (registered as #0)
[36m2019-01-29 10:44:32.677 UTC [orderer/consensus/kafka] try -> DEBU c37[0m [channel: comunitychannel] Error is nil, breaking the retry loop
2019-01-29 10:44:32.677 UTC [orderer/consensus/kafka] startThread -> INFO c38[0m [channel: comunitychannel] CONNECT message posted successfully
2019-01-29 10:44:32.677 UTC [orderer/consensus/kafka] setupParentConsumerForChannel -> INFO c39[0m [channel: comunitychannel] Setting up the parent consumer for this channel...
[36m2019-01-29 10:44:32.678 UTC [orderer/consensus/kafka] try -> DEBU c3a[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:44:32.678 UTC [orderer/consensus/kafka/sarama] NewConsumer -> DEBU c3b[0m Initializing new client
[36m2019-01-29 10:44:32.678 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU c3c[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:44:32.678 UTC [orderer/consensus/kafka/sarama] Open -> DEBU c3d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:44:32.678 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU c3e[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:44:32.679 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU c3f[0m Connected to broker at kafka0.switch2logic.co.za:9092 (unregistered)
[36m2019-01-29 10:44:32.680 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU c40[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 10:44:32.680 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU c41[0m client/brokers registered new broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 10:44:32.680 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU c42[0m client/brokers registered new broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:44:32.680 UTC [orderer/consensus/kafka/sarama] NewConsumer -> DEBU c43[0m Successfully initialized new client
[36m2019-01-29 10:44:32.681 UTC [orderer/consensus/kafka] try -> DEBU c44[0m [channel: comunitychannel] Error is nil, breaking the retry loop
2019-01-29 10:44:32.681 UTC [orderer/consensus/kafka] startThread -> INFO c45[0m [channel: comunitychannel] Parent consumer set up successfully
2019-01-29 10:44:32.681 UTC [orderer/consensus/kafka] setupChannelConsumerForChannel -> INFO c46[0m [channel: comunitychannel] Setting up the channel consumer for this channel (start offset: -2)...
[36m2019-01-29 10:44:32.681 UTC [orderer/consensus/kafka] try -> DEBU c47[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:44:32.681 UTC [orderer/consensus/kafka/sarama] Open -> DEBU c48[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:44:32.681 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU c49[0m Connected to broker at kafka0.switch2logic.co.za:9092 (registered as #0)
[36m2019-01-29 10:44:32.687 UTC [orderer/consensus/kafka] try -> DEBU c4a[0m [channel: comunitychannel] Error is nil, breaking the retry loop
2019-01-29 10:44:32.687 UTC [orderer/consensus/kafka] startThread -> INFO c4c[0m [channel: comunitychannel] Channel consumer set up successfully
2019-01-29 10:44:32.687 UTC [orderer/consensus/kafka] startThread -> INFO c4d[0m [channel: comunitychannel] Start phase completed successfully
[36m2019-01-29 10:44:32.687 UTC [orderer/consensus/kafka/sarama] subscriptionConsumer -> DEBU c4b[0m consumer/broker/0 added subscription to comunitychannel/0
[36m2019-01-29 10:44:32.688 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU c4e[0m [channel: comunitychannel] Successfully unmarshalled consumed message, offset is 0. Inspecting type...
[36m2019-01-29 10:44:32.688 UTC [orderer/consensus/kafka] processConnect -> DEBU c4f[0m [channel: comunitychannel] It's a connect message - ignoring
[36m2019-01-29 10:44:32.688 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU c50[0m [channel: comunitychannel] Successfully unmarshalled consumed message, offset is 1. Inspecting type...
[36m2019-01-29 10:44:32.689 UTC [orderer/consensus/kafka] processConnect -> DEBU c51[0m [channel: comunitychannel] It's a connect message - ignoring
[36m2019-01-29 10:44:32.747 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU c52[0m [channel: comunitychannel] Successfully unmarshalled consumed message, offset is 2. Inspecting type...
[36m2019-01-29 10:44:32.747 UTC [orderer/consensus/kafka] processConnect -> DEBU c53[0m [channel: comunitychannel] It's a connect message - ignoring
[36m2019-01-29 10:44:33.649 UTC [orderer/consensus/kafka] try -> DEBU c54[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:33.652 UTC [orderer/consensus/kafka] try -> DEBU c55[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:34.650 UTC [orderer/consensus/kafka] try -> DEBU c56[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:34.653 UTC [orderer/consensus/kafka] try -> DEBU c57[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:35.073 UTC [orderer/common/server] Deliver -> DEBU c58[0m Starting new Deliver handler
[36m2019-01-29 10:44:35.073 UTC [common/deliver] Handle -> DEBU c59[0m Starting new deliver loop for 10.0.0.22:53506
[36m2019-01-29 10:44:35.073 UTC [common/deliver] Handle -> DEBU c5a[0m Attempting to read seek info message from 10.0.0.22:53506
[36m2019-01-29 10:44:35.074 UTC [policies] Evaluate -> DEBU c5b[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers ==
[36m2019-01-29 10:44:35.074 UTC [policies] Evaluate -> DEBU c5c[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 10:44:35.074 UTC [policies] Evaluate -> DEBU c5d[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers ==
[36m2019-01-29 10:44:35.074 UTC [policies] Evaluate -> DEBU c5e[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 10:44:35.074 UTC [policies] Evaluate -> DEBU c5f[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers ==
2019-01-29 10:44:35.074 UTC [msp] DeserializeIdentity -> INFO c60[0m Obtaining identity
[36m2019-01-29 10:44:35.074 UTC [msp/identity] newIdentity -> DEBU c61[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTTCCAfOgAwIBAgIQDPIhkZbBMzAPRV2mdTJ2BTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB2MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzENMAsGA1UECxME
cGVlcjErMCkGA1UEAxMicGVlcjAuaG9zcGl0YWwxLnN3aXRjaDJsb2dpYy5jby56
YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABMHSfENe+q4SvhxLb6FxLr2SLYg+
qJnL3whOrSDaEyQgmDz7fwbLhWr0Mapiq8N3Xg1vudI6XbNZxyMTBmZbd3ujTTBL
MA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIFVTsOg/
0yx/+c1M2XrkXHABdnzZ5oYFPvNZh3KHhBsGMAoGCCqGSM49BAMCA0gAMEUCIQDn
gu8tRu0NvsaTNG17i4QtjRftqg56ptadF5YHXBQCGgIgeowGfoCWCIW5T09110Gv
cu4tVgSe4rBzE4yjal6cQVQ=
-----END CERTIFICATE-----
[36m2019-01-29 10:44:35.075 UTC [cauthdsl] func1 -> DEBU c62[0m 0xc420192218 gate 1548758675075333927 evaluation starts
[36m2019-01-29 10:44:35.075 UTC [cauthdsl] func2 -> DEBU c63[0m 0xc420192218 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 10:44:35.075 UTC [cauthdsl] func2 -> DEBU c64[0m 0xc420192218 processing identity 0 with bytes of 0a0c486f73706974616c314d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435454434341664f674177494241674951445049686b5a62424d7a41505256326d64544a324254414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777784c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424d485366454e652b7134537668784c623646784c7232534c59672b0a714a6e4c3377684f72534461457951676d447a376677624c685772304d61706971384e33586731767564493658624e5a78794d54426d5a626433756a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149465654734f672f0a3079782f2b63314d3258726b58484142646e7a5a356f594650764e5a68334b48684273474d416f4743437147534d343942414d43413067414d4555434951446e0a677538745275304e767361544e473137693451746a5266747167353670746164463559485842514347674967656f7747666f43574349573554303931313047760a63753474566753653472427a4534796a616c36635156513d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 10:44:35.075 UTC [cauthdsl] func2 -> DEBU c65[0m 0xc420192218 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital3MSP, got Hospital1MSP)
[36m2019-01-29 10:44:35.075 UTC [cauthdsl] func2 -> DEBU c66[0m 0xc420192218 principal evaluation fails
[36m2019-01-29 10:44:35.075 UTC [cauthdsl] func1 -> DEBU c67[0m 0xc420192218 gate 1548758675075333927 evaluation fails
[36m2019-01-29 10:44:35.075 UTC [policies] Evaluate -> DEBU c68[0m Signature set did not satisfy policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 10:44:35.075 UTC [policies] Evaluate -> DEBU c69[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 10:44:35.075 UTC [policies] Evaluate -> DEBU c6a[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers ==
[36m2019-01-29 10:44:35.076 UTC [cauthdsl] func1 -> DEBU c6b[0m 0xc420192230 gate 1548758675076009215 evaluation starts
[36m2019-01-29 10:44:35.076 UTC [cauthdsl] func2 -> DEBU c6c[0m 0xc420192230 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 10:44:35.076 UTC [cauthdsl] func2 -> DEBU c6d[0m 0xc420192230 processing identity 0 with bytes of 0a0c486f73706974616c314d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435454434341664f674177494241674951445049686b5a62424d7a41505256326d64544a324254414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777784c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424d485366454e652b7134537668784c623646784c7232534c59672b0a714a6e4c3377684f72534461457951676d447a376677624c685772304d61706971384e33586731767564493658624e5a78794d54426d5a626433756a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149465654734f672f0a3079782f2b63314d3258726b58484142646e7a5a356f594650764e5a68334b48684273474d416f4743437147534d343942414d43413067414d4555434951446e0a677538745275304e767361544e473137693451746a5266747167353670746164463559485842514347674967656f7747666f43574349573554303931313047760a63753474566753653472427a4534796a616c36635156513d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 10:44:35.076 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU c6e[0m Checking if identity satisfies MEMBER role for Hospital1MSP
[36m2019-01-29 10:44:35.076 UTC [msp] Validate -> DEBU c6f[0m MSP Hospital1MSP validating identity
[36m2019-01-29 10:44:35.076 UTC [msp] getCertificationChain -> DEBU c70[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 10:44:35.077 UTC [cauthdsl] func2 -> DEBU c71[0m 0xc420192230 principal matched by identity 0
[36m2019-01-29 10:44:35.077 UTC [msp/identity] Verify -> DEBU c72[0m Verify: digest = 00000000  23 a7 ed f4 8f 46 b8 8e  98 30 45 b6 c4 24 ad 27  |#....F...0E..$.'|
00000010  b2 16 99 bf f4 72 f5 f1  8a 02 8e 46 52 ce 5c 04  |.....r.....FR.\.|
[36m2019-01-29 10:44:35.077 UTC [msp/identity] Verify -> DEBU c73[0m Verify: sig = 00000000  30 44 02 20 40 d0 b7 29  77 bb 0f 62 56 a4 bc c0  |0D. @..)w..bV...|
00000010  65 b2 d3 7a e2 0c 48 35  ba 76 56 a0 af db 91 8c  |e..z..H5.vV.....|
00000020  70 0d 9b 96 02 20 73 f9  58 65 e7 0b 4c 45 44 b4  |p.... s.Xe..LED.|
00000030  72 83 f9 c3 01 a1 2e cf  ff 4b f2 d1 74 c5 96 d2  |r........K..t...|
00000040  01 cf e3 4e e1 10                                 |...N..|
[36m2019-01-29 10:44:35.077 UTC [cauthdsl] func2 -> DEBU c74[0m 0xc420192230 principal evaluation succeeds for identity 0
[36m2019-01-29 10:44:35.077 UTC [cauthdsl] func1 -> DEBU c75[0m 0xc420192230 gate 1548758675076009215 evaluation succeeds
[36m2019-01-29 10:44:35.077 UTC [policies] Evaluate -> DEBU c76[0m Signature set satisfies policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 10:44:35.077 UTC [policies] Evaluate -> DEBU c77[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 10:44:35.077 UTC [policies] Evaluate -> DEBU c78[0m Signature set satisfies policy /Channel/Application/Readers
[36m2019-01-29 10:44:35.077 UTC [policies] Evaluate -> DEBU c79[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers
[36m2019-01-29 10:44:35.077 UTC [policies] Evaluate -> DEBU c7a[0m Signature set satisfies policy /Channel/Readers
[36m2019-01-29 10:44:35.077 UTC [policies] Evaluate -> DEBU c7b[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers
[36m2019-01-29 10:44:35.078 UTC [common/deliver] deliverBlocks -> DEBU c7c[0m [channel: comunitychannel] Received seekInfo (0xc420166880) start:<specified:<number:1 > > stop:<specified:<number:18446744073709551615 > >  from 10.0.0.22:53506
[36m2019-01-29 10:44:35.078 UTC [fsblkstorage] waitForBlock -> DEBU c7d[0m Going to wait for newer blocks. maxAvailaBlockNumber=[0], waitForBlockNum=[1]
[36m2019-01-29 10:44:35.649 UTC [orderer/consensus/kafka] try -> DEBU c7e[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:35.652 UTC [orderer/consensus/kafka] try -> DEBU c7f[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:36.649 UTC [orderer/consensus/kafka] try -> DEBU c80[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:36.652 UTC [orderer/consensus/kafka] try -> DEBU c81[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:37.649 UTC [orderer/consensus/kafka] try -> DEBU c82[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:37.653 UTC [orderer/consensus/kafka] try -> DEBU c83[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:38.649 UTC [orderer/consensus/kafka] try -> DEBU c84[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:38.651 UTC [orderer/consensus/kafka] try -> DEBU c85[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:39.650 UTC [orderer/consensus/kafka] try -> DEBU c86[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:39.652 UTC [orderer/consensus/kafka] try -> DEBU c87[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:40.650 UTC [orderer/consensus/kafka] try -> DEBU c88[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:40.653 UTC [orderer/consensus/kafka] try -> DEBU c89[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:41.650 UTC [orderer/consensus/kafka] try -> DEBU c8a[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:41.653 UTC [orderer/consensus/kafka] try -> DEBU c8b[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:42.650 UTC [orderer/consensus/kafka] try -> DEBU c8c[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:42.653 UTC [orderer/consensus/kafka] try -> DEBU c8d[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:43.650 UTC [orderer/consensus/kafka] try -> DEBU c8e[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:43.652 UTC [orderer/consensus/kafka] try -> DEBU c8f[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:44.650 UTC [orderer/consensus/kafka] try -> DEBU c90[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:44.652 UTC [orderer/consensus/kafka] try -> DEBU c91[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:45.650 UTC [orderer/consensus/kafka] try -> DEBU c92[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:45.652 UTC [orderer/consensus/kafka] try -> DEBU c93[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:46.650 UTC [orderer/consensus/kafka] try -> DEBU c94[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:46.652 UTC [orderer/consensus/kafka] try -> DEBU c95[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:47.650 UTC [orderer/consensus/kafka] try -> DEBU c96[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:47.653 UTC [orderer/consensus/kafka] try -> DEBU c97[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:48.650 UTC [orderer/consensus/kafka] try -> DEBU c98[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:48.653 UTC [orderer/consensus/kafka] try -> DEBU c99[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:49.649 UTC [orderer/consensus/kafka] try -> DEBU c9a[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:49.651 UTC [orderer/consensus/kafka] try -> DEBU c9b[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:50.650 UTC [orderer/consensus/kafka] try -> DEBU c9c[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:50.652 UTC [orderer/consensus/kafka] try -> DEBU c9d[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:51.650 UTC [orderer/consensus/kafka] try -> DEBU c9e[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:51.652 UTC [orderer/consensus/kafka] try -> DEBU c9f[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:52.650 UTC [orderer/consensus/kafka] try -> DEBU ca0[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:52.652 UTC [orderer/consensus/kafka] try -> DEBU ca1[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:53.650 UTC [orderer/consensus/kafka] try -> DEBU ca2[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:53.653 UTC [orderer/consensus/kafka] try -> DEBU ca3[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:54.650 UTC [orderer/consensus/kafka] try -> DEBU ca4[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:54.653 UTC [orderer/consensus/kafka] try -> DEBU ca5[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:55.650 UTC [orderer/consensus/kafka] try -> DEBU ca6[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:55.651 UTC [orderer/consensus/kafka] try -> DEBU ca7[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:56.650 UTC [orderer/consensus/kafka] try -> DEBU ca8[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:56.653 UTC [orderer/consensus/kafka] try -> DEBU ca9[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:57.650 UTC [orderer/consensus/kafka] try -> DEBU caa[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:57.652 UTC [orderer/consensus/kafka] try -> DEBU cab[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:58.650 UTC [orderer/consensus/kafka] try -> DEBU cac[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:58.652 UTC [orderer/consensus/kafka] try -> DEBU cad[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:44:59.650 UTC [orderer/consensus/kafka] try -> DEBU cae[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:44:59.652 UTC [orderer/consensus/kafka] try -> DEBU caf[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:45:00.649 UTC [orderer/consensus/kafka] try -> DEBU cb0[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:45:00.652 UTC [orderer/consensus/kafka] try -> DEBU cb1[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:45:01.649 UTC [orderer/consensus/kafka] try -> DEBU cb2[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:45:01.651 UTC [orderer/consensus/kafka] try -> DEBU cb3[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:45:02.650 UTC [orderer/consensus/kafka] try -> DEBU cb4[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:45:02.652 UTC [orderer/consensus/kafka] try -> DEBU cb5[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:45:02.652 UTC [orderer/consensus/kafka] retry -> DEBU cb6[0m [channel: testchainid] Switching to the long retry interval
[36m2019-01-29 10:45:02.653 UTC [orderer/consensus/kafka] try -> DEBU cb7[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:45:02.655 UTC [orderer/consensus/kafka] try -> DEBU cb8[0m [channel: testchainid] Initial attempt failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:45:02.655 UTC [orderer/consensus/kafka] try -> DEBU cb9[0m [channel: testchainid] Retrying every 5m0s for a total of 12h0m0s
[36m2019-01-29 10:50:02.655 UTC [orderer/consensus/kafka] try -> DEBU cba[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:50:02.657 UTC [orderer/consensus/kafka] try -> DEBU cbb[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:30.111 UTC [common/deliver] deliverBlocks -> DEBU cbc[0m Context canceled, aborting wait for next block
[36m2019-01-29 10:51:30.111 UTC [orderer/common/server] func1 -> DEBU cbd[0m Closing Deliver stream
[36m2019-01-29 10:51:30.111 UTC [fsblkstorage] waitForBlock -> DEBU cbe[0m Came out of wait. maxAvailaBlockNumber=[0]
2019-01-29 10:51:32.048 UTC [localconfig] completeInitialization -> INFO 001[0m Kafka.Version unset, setting to 0.10.2.0
[36m2019-01-29 10:51:32.048 UTC [bccsp_sw] openKeyStore -> DEBU 002[0m KeyStore opened at [/etc/hyperledger/fabric/orderer/msp/keystore]...done
[36m2019-01-29 10:51:32.049 UTC [bccsp] initBCCSP -> DEBU 003[0m Initialize BCCSP [SW]
[36m2019-01-29 10:51:32.049 UTC [msp] getPemMaterialFromDir -> DEBU 004[0m Reading directory /etc/hyperledger/fabric/orderer/msp/signcerts
[36m2019-01-29 10:51:32.049 UTC [msp] getPemMaterialFromDir -> DEBU 005[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/signcerts/orderer0.hospital1.switch2logic.co.za-cert.pem
[36m2019-01-29 10:51:32.049 UTC [msp] getPemMaterialFromDir -> DEBU 006[0m Reading directory /etc/hyperledger/fabric/orderer/msp/cacerts
[36m2019-01-29 10:51:32.049 UTC [msp] getPemMaterialFromDir -> DEBU 007[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/cacerts/ca.switch2logic.co.za-cert.pem
[36m2019-01-29 10:51:32.049 UTC [msp] getPemMaterialFromDir -> DEBU 008[0m Reading directory /etc/hyperledger/fabric/orderer/msp/admincerts
[36m2019-01-29 10:51:32.049 UTC [msp] getPemMaterialFromDir -> DEBU 009[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/admincerts/Admin@switch2logic.co.za-cert.pem
[36m2019-01-29 10:51:32.049 UTC [msp] getPemMaterialFromDir -> DEBU 00a[0m Reading directory /etc/hyperledger/fabric/orderer/msp/intermediatecerts
[36m2019-01-29 10:51:32.049 UTC [msp] getMspConfig -> DEBU 00b[0m Intermediate certs folder not found at [/etc/hyperledger/fabric/orderer/msp/intermediatecerts]. Skipping. [stat /etc/hyperledger/fabric/orderer/msp/intermediatecerts: no such file or directory]
[36m2019-01-29 10:51:32.049 UTC [msp] getPemMaterialFromDir -> DEBU 00c[0m Reading directory /etc/hyperledger/fabric/orderer/msp/tlscacerts
[36m2019-01-29 10:51:32.049 UTC [msp] getPemMaterialFromDir -> DEBU 00d[0m Inspecting file /etc/hyperledger/fabric/orderer/msp/tlscacerts/tlsca.switch2logic.co.za-cert.pem
[36m2019-01-29 10:51:32.049 UTC [msp] getPemMaterialFromDir -> DEBU 00e[0m Reading directory /etc/hyperledger/fabric/orderer/msp/tlsintermediatecerts
[36m2019-01-29 10:51:32.049 UTC [msp] getMspConfig -> DEBU 00f[0m TLS intermediate certs folder not found at [/etc/hyperledger/fabric/orderer/msp/tlsintermediatecerts]. Skipping. [stat /etc/hyperledger/fabric/orderer/msp/tlsintermediatecerts: no such file or directory]
[36m2019-01-29 10:51:32.049 UTC [msp] getPemMaterialFromDir -> DEBU 010[0m Reading directory /etc/hyperledger/fabric/orderer/msp/crls
[36m2019-01-29 10:51:32.049 UTC [msp] getMspConfig -> DEBU 011[0m crls folder not found at [/etc/hyperledger/fabric/orderer/msp/crls]. Skipping. [stat /etc/hyperledger/fabric/orderer/msp/crls: no such file or directory]
[36m2019-01-29 10:51:32.049 UTC [msp] getMspConfig -> DEBU 012[0m MSP configuration file not found at [/etc/hyperledger/fabric/orderer/msp/config.yaml]: [stat /etc/hyperledger/fabric/orderer/msp/config.yaml: no such file or directory]
[36m2019-01-29 10:51:32.049 UTC [msp] newBccspMsp -> DEBU 013[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:51:32.049 UTC [msp] New -> DEBU 014[0m Creating Cache-MSP instance
[36m2019-01-29 10:51:32.049 UTC [msp] loadLocaMSP -> DEBU 015[0m Created new local MSP
[36m2019-01-29 10:51:32.049 UTC [msp] Setup -> DEBU 016[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 10:51:32.050 UTC [msp/identity] newIdentity -> DEBU 017[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.050 UTC [msp/identity] newIdentity -> DEBU 018[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.072 UTC [msp/identity] newIdentity -> DEBU 019[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICLDCCAdKgAwIBAgIQIoN1EApgEjYu/O7unXVqYzAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBqMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEuMCwGA1UEAxMlb3JkZXJlcjAuaG9zcGl0YWwxLnN3
aXRjaDJsb2dpYy5jby56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABH2/gH3f
8uhcO2CKglQg53cAJL79dENZNdRWfaRikU5j3X60lh9Bx5YpznQ5l0qMaCK1Ogec
Nw5LGgie9ESxrHyjTTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsG
A1UdIwQkMCKAIBiGNntD5tBlG9C8QNsdXde3ItTRRa+p/+3yJwRCsbJxMAoGCCqG
SM49BAMCA0gAMEUCIQCBtzCy6LE3S8b/XG08Qcs01A8QjmCwqmZJQrDvRA8ZhgIg
aAwP9tYtEZs1DBv2bH7HwYIwmi8nlgD3+Xp6CWwvAkg=
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.073 UTC [bccsp_sw] loadPrivateKey -> DEBU 01a[0m Loading private key [f9564620d77ba2b2650b4916b06387b38f212a58c63268d4c08f9b019eaa0f87] at [/etc/hyperledger/fabric/orderer/msp/keystore/f9564620d77ba2b2650b4916b06387b38f212a58c63268d4c08f9b019eaa0f87_sk]...
[36m2019-01-29 10:51:32.073 UTC [msp/identity] newIdentity -> DEBU 01b[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICLDCCAdKgAwIBAgIQIoN1EApgEjYu/O7unXVqYzAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBqMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEuMCwGA1UEAxMlb3JkZXJlcjAuaG9zcGl0YWwxLnN3
aXRjaDJsb2dpYy5jby56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABH2/gH3f
8uhcO2CKglQg53cAJL79dENZNdRWfaRikU5j3X60lh9Bx5YpznQ5l0qMaCK1Ogec
Nw5LGgie9ESxrHyjTTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsG
A1UdIwQkMCKAIBiGNntD5tBlG9C8QNsdXde3ItTRRa+p/+3yJwRCsbJxMAoGCCqG
SM49BAMCA0gAMEUCIQCBtzCy6LE3S8b/XG08Qcs01A8QjmCwqmZJQrDvRA8ZhgIg
aAwP9tYtEZs1DBv2bH7HwYIwmi8nlgD3+Xp6CWwvAkg=
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.073 UTC [msp] setupSigningIdentity -> DEBU 01c[0m Signing identity expires at 2029-01-26 07:41:54 +0000 UTC
[36m2019-01-29 10:51:32.073 UTC [msp] Validate -> DEBU 01d[0m MSP OrdererMSP validating identity
2019-01-29 10:51:32.074 UTC [orderer/common/server] prettyPrintStruct -> INFO 01e[0m Orderer config values:
	General.LedgerType = "file"
	General.ListenAddress = "0.0.0.0"
	General.ListenPort = 7050
	General.TLS.Enabled = true
	General.TLS.PrivateKey = "/etc/hyperledger/fabric/orderer/tls/server.key"
	General.TLS.Certificate = "/etc/hyperledger/fabric/orderer/tls/server.crt"
	General.TLS.RootCAs = [/etc/hyperledger/fabric/orderer/tls/ca.crt]
	General.TLS.ClientAuthRequired = false
	General.TLS.ClientRootCAs = []
	General.Keepalive.ServerMinInterval = 1m0s
	General.Keepalive.ServerInterval = 2h0m0s
	General.Keepalive.ServerTimeout = 20s
	General.GenesisMethod = "file"
	General.GenesisProfile = "SampleInsecureSolo"
	General.SystemChannel = "test-system-channel-name"
	General.GenesisFile = "/etc/hyperledger/fabric/orderer/channel-artifacts/genesis.block"
	General.Profile.Enabled = false
	General.Profile.Address = "0.0.0.0:6060"
	General.LogLevel = "debug"
	General.LogFormat = "%{color}%{time:2006-01-02 15:04:05.000 MST} [%{module}] %{shortfunc} -> %{level:.4s} %{id:03x}%{color:reset} %{message}"
	General.LocalMSPDir = "/etc/hyperledger/fabric/orderer/msp"
	General.LocalMSPID = "OrdererMSP"
	General.BCCSP.ProviderName = "SW"
	General.BCCSP.SwOpts.SecLevel = 256
	General.BCCSP.SwOpts.HashFamily = "SHA2"
	General.BCCSP.SwOpts.Ephemeral = false
	General.BCCSP.SwOpts.FileKeystore.KeyStorePath = "/etc/hyperledger/fabric/orderer/msp/keystore"
	General.BCCSP.SwOpts.DummyKeystore =
	General.BCCSP.PluginOpts =
	General.Authentication.TimeWindow = 15m0s
	FileLedger.Location = "/var/hyperledger/production/orderer"
	FileLedger.Prefix = "hyperledger-fabric-ordererledger"
	RAMLedger.HistorySize = 1000
	Kafka.Retry.ShortInterval = 1s
	Kafka.Retry.ShortTotal = 30s
	Kafka.Retry.LongInterval = 5m0s
	Kafka.Retry.LongTotal = 12h0m0s
	Kafka.Retry.NetworkTimeouts.DialTimeout = 10s
	Kafka.Retry.NetworkTimeouts.ReadTimeout = 10s
	Kafka.Retry.NetworkTimeouts.WriteTimeout = 10s
	Kafka.Retry.Metadata.RetryMax = 3
	Kafka.Retry.Metadata.RetryBackoff = 250ms
	Kafka.Retry.Producer.RetryMax = 3
	Kafka.Retry.Producer.RetryBackoff = 100ms
	Kafka.Retry.Consumer.RetryBackoff = 2s
	Kafka.Verbose = true
	Kafka.Version = 0.10.2.0
	Kafka.TLS.Enabled = false
	Kafka.TLS.PrivateKey = ""
	Kafka.TLS.Certificate = ""
	Kafka.TLS.RootCAs = []
	Kafka.TLS.ClientAuthRequired = false
	Kafka.TLS.ClientRootCAs = []
	Debug.BroadcastTraceDir = ""
	Debug.DeliverTraceDir = ""
2019-01-29 10:51:32.074 UTC [orderer/common/server] initializeServerConfig -> INFO 01f[0m Starting orderer with TLS enabled
[36m2019-01-29 10:51:32.074 UTC [orderer/common/server] createLedgerFactory -> DEBU 020[0m Ledger dir: /var/hyperledger/production/orderer
[36m2019-01-29 10:51:32.074 UTC [kvledger.util] CreateDirIfMissing -> DEBU 021[0m CreateDirIfMissing [/var/hyperledger/production/orderer/index/]
[36m2019-01-29 10:51:32.074 UTC [kvledger.util] logDirStatus -> DEBU 022[0m Before creating dir - [/var/hyperledger/production/orderer/index/] exists
[36m2019-01-29 10:51:32.074 UTC [kvledger.util] logDirStatus -> DEBU 023[0m After creating dir - [/var/hyperledger/production/orderer/index/] exists
[36m2019-01-29 10:51:32.084 UTC [orderer/common/server] createSubDir -> DEBU 024[0m Found chains sub-dir and using it
2019-01-29 10:51:32.084 UTC [orderer/common/server] initializeMultichannelRegistrar -> INFO 025[0m Not bootstrapping because of existing chains
[36m2019-01-29 10:51:32.085 UTC [fsblkstorage] newBlockfileMgr -> DEBU 026[0m newBlockfileMgr() initializing file-based block storage for ledger: comunitychannel 
[36m2019-01-29 10:51:32.085 UTC [kvledger.util] CreateDirIfMissing -> DEBU 027[0m CreateDirIfMissing [/var/hyperledger/production/orderer/chains/comunitychannel/]
[36m2019-01-29 10:51:32.085 UTC [kvledger.util] logDirStatus -> DEBU 028[0m Before creating dir - [/var/hyperledger/production/orderer/chains/comunitychannel/] exists
[36m2019-01-29 10:51:32.085 UTC [kvledger.util] logDirStatus -> DEBU 029[0m After creating dir - [/var/hyperledger/production/orderer/chains/comunitychannel/] exists
[36m2019-01-29 10:51:32.085 UTC [fsblkstorage] loadCurrentInfo -> DEBU 02a[0m loaded checkpointInfo:latestFileChunkSuffixNum=[0], latestFileChunksize=[21745], isChainEmpty=[false], lastBlockNumber=[0]
[36m2019-01-29 10:51:32.085 UTC [fsblkstorage] newBlockfileMgr -> DEBU 02b[0m Synching block information from block storage (if needed)
[36m2019-01-29 10:51:32.085 UTC [fsblkstorage] syncCPInfoFromFS -> DEBU 02c[0m Starting checkpoint=latestFileChunkSuffixNum=[0], latestFileChunksize=[21745], isChainEmpty=[false], lastBlockNumber=[0]
[36m2019-01-29 10:51:32.085 UTC [fsblkstorage] syncCPInfoFromFS -> DEBU 02d[0m status of file [/var/hyperledger/production/orderer/chains/comunitychannel/blockfile_000000]: exists=[true], size=[21745]
[36m2019-01-29 10:51:32.091 UTC [fsblkstorage] newBlockIndex -> DEBU 02e[0m newBlockIndex() - indexItems:[[BlockNum]]
[36m2019-01-29 10:51:32.091 UTC [fsblkstorage] syncIndex -> DEBU 02f[0m Both the block files and indices are in sync.
[36m2019-01-29 10:51:32.091 UTC [fsblkstorage] retrieveBlockHeaderByNumber -> DEBU 030[0m retrieveBlockHeaderByNumber() - blockNum = [0]
[36m2019-01-29 10:51:32.091 UTC [fsblkstorage] newBlockfileStream -> DEBU 031[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/comunitychannel/blockfile_000000], startOffset=[0]
[36m2019-01-29 10:51:32.091 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 032[0m Remaining bytes=[21745], Going to peek [8] bytes
[36m2019-01-29 10:51:32.091 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 033[0m Returning blockbytes - length=[21742], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 10:51:32.092 UTC [fsblkstorage] Next -> DEBU 034[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 10:51:32.092 UTC [fsblkstorage] newBlockfileStream -> DEBU 035[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/comunitychannel/blockfile_000000], startOffset=[0]
[36m2019-01-29 10:51:32.092 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 036[0m Remaining bytes=[21745], Going to peek [8] bytes
[36m2019-01-29 10:51:32.092 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 037[0m Returning blockbytes - length=[21742], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 10:51:32.092 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 038[0m blockbytes [21742] read from file [0]
[36m2019-01-29 10:51:32.092 UTC [fsblkstorage] Next -> DEBU 039[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 10:51:32.092 UTC [fsblkstorage] newBlockfileStream -> DEBU 03a[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/comunitychannel/blockfile_000000], startOffset=[0]
[36m2019-01-29 10:51:32.092 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 03b[0m Remaining bytes=[21745], Going to peek [8] bytes
[36m2019-01-29 10:51:32.092 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 03c[0m Returning blockbytes - length=[21742], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 10:51:32.092 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 03d[0m blockbytes [21742] read from file [0]
[36m2019-01-29 10:51:32.092 UTC [common/channelconfig] NewStandardValues -> DEBU 03e[0m Initializing protos for *channelconfig.ChannelProtos
[36m2019-01-29 10:51:32.092 UTC [common/channelconfig] initializeProtosStruct -> DEBU 03f[0m Processing field: HashingAlgorithm
[36m2019-01-29 10:51:32.092 UTC [common/channelconfig] initializeProtosStruct -> DEBU 040[0m Processing field: BlockDataHashingStructure
[36m2019-01-29 10:51:32.092 UTC [common/channelconfig] initializeProtosStruct -> DEBU 041[0m Processing field: OrdererAddresses
[36m2019-01-29 10:51:32.092 UTC [common/channelconfig] initializeProtosStruct -> DEBU 042[0m Processing field: Consortium
[36m2019-01-29 10:51:32.092 UTC [common/channelconfig] initializeProtosStruct -> DEBU 043[0m Processing field: Capabilities
[36m2019-01-29 10:51:32.092 UTC [common/channelconfig] NewStandardValues -> DEBU 044[0m Initializing protos for *channelconfig.ApplicationProtos
[36m2019-01-29 10:51:32.092 UTC [common/channelconfig] initializeProtosStruct -> DEBU 045[0m Processing field: ACLs
[36m2019-01-29 10:51:32.092 UTC [common/channelconfig] initializeProtosStruct -> DEBU 046[0m Processing field: Capabilities
[36m2019-01-29 10:51:32.092 UTC [common/channelconfig] NewStandardValues -> DEBU 047[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 10:51:32.092 UTC [common/channelconfig] initializeProtosStruct -> DEBU 048[0m Processing field: AnchorPeers
[36m2019-01-29 10:51:32.093 UTC [common/channelconfig] NewStandardValues -> DEBU 049[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 10:51:32.093 UTC [common/channelconfig] initializeProtosStruct -> DEBU 04a[0m Processing field: MSP
[36m2019-01-29 10:51:32.093 UTC [common/channelconfig] Validate -> DEBU 04b[0m Anchor peers for org Hospital1MSP are 
[36m2019-01-29 10:51:32.093 UTC [common/channelconfig] validateMSP -> DEBU 04c[0m Setting up MSP for org Hospital1MSP
[36m2019-01-29 10:51:32.093 UTC [msp] newBccspMsp -> DEBU 04d[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:51:32.093 UTC [msp] New -> DEBU 04e[0m Creating Cache-MSP instance
[36m2019-01-29 10:51:32.093 UTC [msp] Setup -> DEBU 04f[0m Setting up MSP instance Hospital1MSP
[36m2019-01-29 10:51:32.093 UTC [msp/identity] newIdentity -> DEBU 050[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdDCCAhugAwIBAgIQZxH8VsnceZxXgKxlURLgVTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMS5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQX9V4k/h/3pFxByKDscjgQrC0YS/bSbKvzSG2Z2PvltEWp40qB9OaKUN0GQYzo
yK2Etdvyjfd1OFwaTjpGSvv9o18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCBVU7DoP9Msf/nNTNl6
5FxwAXZ82eaGBT7zWYdyh4QbBjAKBggqhkjOPQQDAgNHADBEAiBmXGWQbxCGRp0c
5/y9qJyciB1CDHsknRZDjNzt5f563wIgMy7VPi1R8DpIIIRN9B3mfhDpRa+J+8PA
S4DOJy7rjp4=
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.093 UTC [msp/identity] newIdentity -> DEBU 051[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.094 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 052[0m Checking if identity satisfies role [CLIENT] for Hospital1MSP
[36m2019-01-29 10:51:32.094 UTC [msp] Validate -> DEBU 053[0m MSP Hospital1MSP validating identity
[36m2019-01-29 10:51:32.095 UTC [msp] getCertificationChain -> DEBU 054[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 10:51:32.095 UTC [msp] hasOURole -> DEBU 055[0m MSP Hospital1MSP checking if the identity is a client
[36m2019-01-29 10:51:32.095 UTC [msp] getCertificationChain -> DEBU 056[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 10:51:32.096 UTC [common/channelconfig] NewStandardValues -> DEBU 057[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 10:51:32.096 UTC [common/channelconfig] initializeProtosStruct -> DEBU 058[0m Processing field: AnchorPeers
[36m2019-01-29 10:51:32.096 UTC [common/channelconfig] NewStandardValues -> DEBU 059[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 10:51:32.096 UTC [common/channelconfig] initializeProtosStruct -> DEBU 05a[0m Processing field: MSP
[36m2019-01-29 10:51:32.096 UTC [common/channelconfig] Validate -> DEBU 05b[0m Anchor peers for org Hospital2MSP are 
[36m2019-01-29 10:51:32.096 UTC [common/channelconfig] validateMSP -> DEBU 05c[0m Setting up MSP for org Hospital2MSP
[36m2019-01-29 10:51:32.096 UTC [msp] newBccspMsp -> DEBU 05d[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:51:32.096 UTC [msp] New -> DEBU 05e[0m Creating Cache-MSP instance
[36m2019-01-29 10:51:32.096 UTC [msp] Setup -> DEBU 05f[0m Setting up MSP instance Hospital2MSP
[36m2019-01-29 10:51:32.096 UTC [msp/identity] newIdentity -> DEBU 060[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdjCCAhygAwIBAgIRALZ3Pffn/xWUtUWPSK6CYVAwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowgYsxCzAJBgNVBAYTAlVTMRMwEQYD
VQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4gRnJhbmNpc2NvMSUwIwYDVQQK
Exxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMSgwJgYDVQQDEx9jYS5ob3Nw
aXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcD
QgAESzVpiz6P4mIb+KwvrttcXJKZR5i137Is4298E7OrkYdAg0tB4rpfKTXzUMwW
vI12cTDujpyaZSiqQ3/zP4fOQqNfMF0wDgYDVR0PAQH/BAQDAgGmMA8GA1UdJQQI
MAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQg8QJqWKPaX24HINwR
S6sINFSwRD+440/ccSnvFVaGF24wCgYIKoZIzj0EAwIDSAAwRQIhAPO2Nng3/nqb
cKPliUqVPVjBMu+D55M90+jexWSOpkjcAiADPx8qv9exZ4BfP3+G49Qzpx0Meblj
53vzUgYFa/0EXw==
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.097 UTC [msp/identity] newIdentity -> DEBU 061[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAJ6XtHMTTW1JzdpD54qCc3wwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA+z1rb9aA1HwHfNLi9V9VZe
bdwT4MXak445zGz4DXzRUsmNeBngzntKb6qpgHQpeFtzbzLx+zwiynRDvfdzORGj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPEC
alij2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQC
IA8A3CCJeCasyy7gYd/M1aHYmwWsecY7UY7cVUGy4XOxAiB0rrmfnGW6JA11PDCu
+5c5Rpn0jF5ddsFn+UIpzBFvmg==
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.098 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 062[0m Checking if identity satisfies role [CLIENT] for Hospital2MSP
[36m2019-01-29 10:51:32.098 UTC [msp] Validate -> DEBU 063[0m MSP Hospital2MSP validating identity
[36m2019-01-29 10:51:32.098 UTC [msp] getCertificationChain -> DEBU 064[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 10:51:32.098 UTC [msp] hasOURole -> DEBU 065[0m MSP Hospital2MSP checking if the identity is a client
[36m2019-01-29 10:51:32.098 UTC [msp] getCertificationChain -> DEBU 066[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 10:51:32.099 UTC [common/channelconfig] NewStandardValues -> DEBU 067[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 10:51:32.099 UTC [common/channelconfig] initializeProtosStruct -> DEBU 068[0m Processing field: AnchorPeers
[36m2019-01-29 10:51:32.099 UTC [common/channelconfig] NewStandardValues -> DEBU 069[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 10:51:32.099 UTC [common/channelconfig] initializeProtosStruct -> DEBU 06a[0m Processing field: MSP
[36m2019-01-29 10:51:32.099 UTC [common/channelconfig] Validate -> DEBU 06b[0m Anchor peers for org Hospital3MSP are 
[36m2019-01-29 10:51:32.099 UTC [common/channelconfig] validateMSP -> DEBU 06c[0m Setting up MSP for org Hospital3MSP
[36m2019-01-29 10:51:32.099 UTC [msp] newBccspMsp -> DEBU 06d[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:51:32.099 UTC [msp] New -> DEBU 06e[0m Creating Cache-MSP instance
[36m2019-01-29 10:51:32.099 UTC [msp] Setup -> DEBU 06f[0m Setting up MSP instance Hospital3MSP
[36m2019-01-29 10:51:32.099 UTC [msp/identity] newIdentity -> DEBU 070[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdTCCAhugAwIBAgIQCCXeHGgObIJo8aq4AdvgVjAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMy5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQm1Oo/AEu7ZZToE15Y5ntlaN128Hs/exX3jNH5nO6r4tvyFtBBNEn1ecXxxPtm
dmQ+nPHONorLwDC42SMAKwfro18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCA7vVWFXWpgZBojQ1ZL
xUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEAlOUoPtpfO8V7
vFRCanjU5zZWGO5ajFlPDJjjDTy3UrwCICrUYV32JrR/m3dslCl15dSThHrKtYCC
Ehvyw1+lNqiG
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.100 UTC [msp/identity] newIdentity -> DEBU 071[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAI8BA/5Jf+5m8ncI+D7aSiMwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwzLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABF1IehIKIjh8tXYZ7rw4QOuc
g/tpBAkJeOt44zr/LepbjhyFW8vIRnkw75i6Wcg+LOoAS+2NFNg2oNjqg//yCHWj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIDu9
VYVdamBkGiNDVkvFS+AEt6Jrn0yK8hFyPGZhqenJMAoGCCqGSM49BAMCA0cAMEQC
IFyc85pwbCAxGNdw4fflsX2I3HrP3h93YAB2zSDC8HtkAiAQxblAZfEnXUiQKrqE
8I10bYJqD6pJz1jb3Pv+4xplTA==
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.101 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 072[0m Checking if identity satisfies role [CLIENT] for Hospital3MSP
[36m2019-01-29 10:51:32.101 UTC [msp] Validate -> DEBU 073[0m MSP Hospital3MSP validating identity
[36m2019-01-29 10:51:32.101 UTC [msp] getCertificationChain -> DEBU 074[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 10:51:32.101 UTC [msp] hasOURole -> DEBU 075[0m MSP Hospital3MSP checking if the identity is a client
[36m2019-01-29 10:51:32.101 UTC [msp] getCertificationChain -> DEBU 076[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 10:51:32.102 UTC [common/channelconfig] NewStandardValues -> DEBU 077[0m Initializing protos for *channelconfig.OrdererProtos
[36m2019-01-29 10:51:32.102 UTC [common/channelconfig] initializeProtosStruct -> DEBU 078[0m Processing field: ConsensusType
[36m2019-01-29 10:51:32.102 UTC [common/channelconfig] initializeProtosStruct -> DEBU 079[0m Processing field: BatchSize
[36m2019-01-29 10:51:32.102 UTC [common/channelconfig] initializeProtosStruct -> DEBU 07a[0m Processing field: BatchTimeout
[36m2019-01-29 10:51:32.102 UTC [common/channelconfig] initializeProtosStruct -> DEBU 07b[0m Processing field: KafkaBrokers
[36m2019-01-29 10:51:32.102 UTC [common/channelconfig] initializeProtosStruct -> DEBU 07c[0m Processing field: ChannelRestrictions
[36m2019-01-29 10:51:32.102 UTC [common/channelconfig] initializeProtosStruct -> DEBU 07d[0m Processing field: Capabilities
[36m2019-01-29 10:51:32.102 UTC [common/channelconfig] NewStandardValues -> DEBU 07e[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 10:51:32.102 UTC [common/channelconfig] initializeProtosStruct -> DEBU 07f[0m Processing field: MSP
[36m2019-01-29 10:51:32.102 UTC [common/channelconfig] validateMSP -> DEBU 080[0m Setting up MSP for org OrdererOrg
[36m2019-01-29 10:51:32.102 UTC [msp] newBccspMsp -> DEBU 081[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:51:32.102 UTC [msp] New -> DEBU 082[0m Creating Cache-MSP instance
[36m2019-01-29 10:51:32.102 UTC [msp] Setup -> DEBU 083[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 10:51:32.103 UTC [msp/identity] newIdentity -> DEBU 084[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.103 UTC [msp/identity] newIdentity -> DEBU 085[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.103 UTC [msp] Validate -> DEBU 086[0m MSP OrdererMSP validating identity
[36m2019-01-29 10:51:32.104 UTC [msp] Setup -> DEBU 087[0m Setting up the MSP manager (4 msps)
[36m2019-01-29 10:51:32.104 UTC [msp] Setup -> DEBU 088[0m MSP manager setup complete, setup 4 msps
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 089[0m Proposed new policy Admins for Channel/Application/Hospital1MSP
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 08a[0m Proposed new policy Readers for Channel/Application/Hospital1MSP
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 08b[0m Proposed new policy Writers for Channel/Application/Hospital1MSP
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 08c[0m Proposed new policy Writers for Channel/Application/Hospital2MSP
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 08d[0m Proposed new policy Admins for Channel/Application/Hospital2MSP
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 08e[0m Proposed new policy Readers for Channel/Application/Hospital2MSP
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 08f[0m Proposed new policy Admins for Channel/Application/Hospital3MSP
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 090[0m Proposed new policy Readers for Channel/Application/Hospital3MSP
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 091[0m Proposed new policy Writers for Channel/Application/Hospital3MSP
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 092[0m Proposed new policy Writers for Channel/Application
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 093[0m Proposed new policy Admins for Channel/Application
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 094[0m Proposed new policy Readers for Channel/Application
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 095[0m Proposed new policy Admins for Channel/Orderer/OrdererOrg
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 096[0m Proposed new policy Readers for Channel/Orderer/OrdererOrg
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 097[0m Proposed new policy Writers for Channel/Orderer/OrdererOrg
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 098[0m Proposed new policy Writers for Channel/Orderer
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 099[0m Proposed new policy BlockValidation for Channel/Orderer
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 09a[0m Proposed new policy Admins for Channel/Orderer
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 09b[0m Proposed new policy Readers for Channel/Orderer
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 09c[0m Proposed new policy Readers for Channel
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 09d[0m Proposed new policy Writers for Channel
[36m2019-01-29 10:51:32.104 UTC [policies] NewManagerImpl -> DEBU 09e[0m Proposed new policy Admins for Channel
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 09f[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0a0[0m Adding to config map: [Group]  /Channel/Orderer
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0a1[0m Adding to config map: [Group]  /Channel/Orderer/OrdererOrg
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0a2[0m Adding to config map: [Value]  /Channel/Orderer/OrdererOrg/MSP
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0a3[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0a4[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0a5[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Admins
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0a6[0m Adding to config map: [Value]  /Channel/Orderer/BatchTimeout
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0a7[0m Adding to config map: [Value]  /Channel/Orderer/ChannelRestrictions
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0a8[0m Adding to config map: [Value]  /Channel/Orderer/Capabilities
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0a9[0m Adding to config map: [Value]  /Channel/Orderer/KafkaBrokers
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0aa[0m Adding to config map: [Value]  /Channel/Orderer/ConsensusType
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0ab[0m Adding to config map: [Value]  /Channel/Orderer/BatchSize
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0ac[0m Adding to config map: [Policy] /Channel/Orderer/Readers
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0ad[0m Adding to config map: [Policy] /Channel/Orderer/Writers
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0ae[0m Adding to config map: [Policy] /Channel/Orderer/BlockValidation
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0af[0m Adding to config map: [Policy] /Channel/Orderer/Admins
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0b0[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0b1[0m Adding to config map: [Group]  /Channel/Application/Hospital2MSP
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0b2[0m Adding to config map: [Value]  /Channel/Application/Hospital2MSP/MSP
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0b3[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0b4[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Writers
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0b5[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Admins
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0b6[0m Adding to config map: [Group]  /Channel/Application/Hospital3MSP
[36m2019-01-29 10:51:32.105 UTC [common/configtx] addToMap -> DEBU 0b7[0m Adding to config map: [Value]  /Channel/Application/Hospital3MSP/MSP
[36m2019-01-29 10:51:32.106 UTC [common/configtx] addToMap -> DEBU 0b8[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Admins
[36m2019-01-29 10:51:32.106 UTC [common/configtx] addToMap -> DEBU 0b9[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 10:51:32.106 UTC [common/configtx] addToMap -> DEBU 0ba[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Writers
[36m2019-01-29 10:51:32.106 UTC [common/configtx] addToMap -> DEBU 0bb[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 10:51:32.106 UTC [common/configtx] addToMap -> DEBU 0bc[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/MSP
[36m2019-01-29 10:51:32.106 UTC [common/configtx] addToMap -> DEBU 0bd[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 10:51:32.106 UTC [common/configtx] addToMap -> DEBU 0be[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 10:51:32.106 UTC [common/configtx] addToMap -> DEBU 0bf[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 10:51:32.106 UTC [common/configtx] addToMap -> DEBU 0c0[0m Adding to config map: [Value]  /Channel/Application/Capabilities
[36m2019-01-29 10:51:32.106 UTC [common/configtx] addToMap -> DEBU 0c1[0m Adding to config map: [Policy] /Channel/Application/Writers
[36m2019-01-29 10:51:32.106 UTC [common/configtx] addToMap -> DEBU 0c2[0m Adding to config map: [Policy] /Channel/Application/Admins
[36m2019-01-29 10:51:32.106 UTC [common/configtx] addToMap -> DEBU 0c3[0m Adding to config map: [Policy] /Channel/Application/Readers
[36m2019-01-29 10:51:32.106 UTC [common/configtx] addToMap -> DEBU 0c4[0m Adding to config map: [Value]  /Channel/Consortium
[36m2019-01-29 10:51:32.111 UTC [common/configtx] addToMap -> DEBU 0c5[0m Adding to config map: [Value]  /Channel/HashingAlgorithm
[36m2019-01-29 10:51:32.111 UTC [common/configtx] addToMap -> DEBU 0c6[0m Adding to config map: [Value]  /Channel/BlockDataHashingStructure
[36m2019-01-29 10:51:32.111 UTC [common/configtx] addToMap -> DEBU 0c7[0m Adding to config map: [Value]  /Channel/OrdererAddresses
[36m2019-01-29 10:51:32.111 UTC [common/configtx] addToMap -> DEBU 0c8[0m Adding to config map: [Value]  /Channel/Capabilities
[36m2019-01-29 10:51:32.111 UTC [common/configtx] addToMap -> DEBU 0c9[0m Adding to config map: [Policy] /Channel/Readers
[36m2019-01-29 10:51:32.111 UTC [common/configtx] addToMap -> DEBU 0ca[0m Adding to config map: [Policy] /Channel/Writers
[36m2019-01-29 10:51:32.111 UTC [common/configtx] addToMap -> DEBU 0cb[0m Adding to config map: [Policy] /Channel/Admins
[36m2019-01-29 10:51:32.111 UTC [common/channelconfig] LogSanityChecks -> DEBU 0cc[0m As expected, current configuration has policy '/Channel/Readers'
[36m2019-01-29 10:51:32.111 UTC [common/channelconfig] LogSanityChecks -> DEBU 0cd[0m As expected, current configuration has policy '/Channel/Writers'
[36m2019-01-29 10:51:32.111 UTC [policies] Manager -> DEBU 0ce[0m Manager Channel looking up path [Application]
[36m2019-01-29 10:51:32.111 UTC [policies] Manager -> DEBU 0cf[0m Manager Channel has managers Application
[36m2019-01-29 10:51:32.112 UTC [policies] Manager -> DEBU 0d0[0m Manager Channel has managers Orderer
[36m2019-01-29 10:51:32.112 UTC [policies] Manager -> DEBU 0d1[0m Manager Channel/Application looking up path []
[36m2019-01-29 10:51:32.112 UTC [policies] Manager -> DEBU 0d2[0m Manager Channel/Application has managers Hospital1MSP
[36m2019-01-29 10:51:32.112 UTC [policies] Manager -> DEBU 0d3[0m Manager Channel/Application has managers Hospital2MSP
[36m2019-01-29 10:51:32.112 UTC [policies] Manager -> DEBU 0d4[0m Manager Channel/Application has managers Hospital3MSP
[36m2019-01-29 10:51:32.112 UTC [common/channelconfig] LogSanityChecks -> DEBU 0d5[0m As expected, current configuration has policy '/Channel/Application/Readers'
[36m2019-01-29 10:51:32.112 UTC [common/channelconfig] LogSanityChecks -> DEBU 0d6[0m As expected, current configuration has policy '/Channel/Application/Writers'
[36m2019-01-29 10:51:32.112 UTC [common/channelconfig] LogSanityChecks -> DEBU 0d7[0m As expected, current configuration has policy '/Channel/Application/Admins'
[36m2019-01-29 10:51:32.112 UTC [policies] Manager -> DEBU 0d8[0m Manager Channel looking up path [Orderer]
[36m2019-01-29 10:51:32.112 UTC [policies] Manager -> DEBU 0d9[0m Manager Channel has managers Application
[36m2019-01-29 10:51:32.112 UTC [policies] Manager -> DEBU 0da[0m Manager Channel has managers Orderer
[36m2019-01-29 10:51:32.112 UTC [policies] Manager -> DEBU 0db[0m Manager Channel/Orderer looking up path []
[36m2019-01-29 10:51:32.112 UTC [policies] Manager -> DEBU 0dc[0m Manager Channel/Orderer has managers OrdererOrg
[36m2019-01-29 10:51:32.112 UTC [common/channelconfig] LogSanityChecks -> DEBU 0dd[0m As expected, current configuration has policy '/Channel/Orderer/BlockValidation'
[36m2019-01-29 10:51:32.112 UTC [common/capabilities] Supported -> DEBU 0de[0m Orderer capability V1_1 is supported and is enabled
[36m2019-01-29 10:51:32.112 UTC [common/capabilities] Supported -> DEBU 0df[0m Channel capability V1_1 is supported and is enabled
[36m2019-01-29 10:51:32.112 UTC [orderer/commmon/multichannel] NewRegistrar -> DEBU 0e0[0m Starting chain: comunitychannel
[36m2019-01-29 10:51:32.112 UTC [fsblkstorage] Next -> DEBU 0e1[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=0
[36m2019-01-29 10:51:32.112 UTC [fsblkstorage] newBlockfileStream -> DEBU 0e2[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/comunitychannel/blockfile_000000], startOffset=[0]
[36m2019-01-29 10:51:32.112 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0e3[0m Remaining bytes=[21745], Going to peek [8] bytes
[36m2019-01-29 10:51:32.112 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0e4[0m Returning blockbytes - length=[21742], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 10:51:32.112 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 0e5[0m blockbytes [21742] read from file [0]
[36m2019-01-29 10:51:32.112 UTC [orderer/commmon/multichannel] newBlockWriter -> DEBU 0e6[0m [channel: comunitychannel] Creating block writer for tip of chain (blockNumber=0, lastConfigBlockNum=0, lastConfigSeq=1)
2019-01-29 10:51:32.112 UTC [orderer/consensus/kafka] newChain -> INFO 0e7[0m [channel: comunitychannel] Starting chain with last persisted offset -3 and last recorded block 0
[36m2019-01-29 10:51:32.112 UTC [orderer/commmon/multichannel] newChainSupport -> DEBU 0e8[0m [channel: comunitychannel] Done creating channel support resources
[36m2019-01-29 10:51:32.112 UTC [fsblkstorage] newBlockfileMgr -> DEBU 0e9[0m newBlockfileMgr() initializing file-based block storage for ledger: testchainid 
[36m2019-01-29 10:51:32.112 UTC [kvledger.util] CreateDirIfMissing -> DEBU 0ea[0m CreateDirIfMissing [/var/hyperledger/production/orderer/chains/testchainid/]
[36m2019-01-29 10:51:32.113 UTC [kvledger.util] logDirStatus -> DEBU 0ec[0m Before creating dir - [/var/hyperledger/production/orderer/chains/testchainid/] exists
2019-01-29 10:51:32.113 UTC [orderer/consensus/kafka] setupProducerForChannel -> INFO 0eb[0m [channel: comunitychannel] Setting up the producer for this channel...
[36m2019-01-29 10:51:32.113 UTC [kvledger.util] logDirStatus -> DEBU 0ed[0m After creating dir - [/var/hyperledger/production/orderer/chains/testchainid/] exists
[36m2019-01-29 10:51:32.113 UTC [orderer/consensus/kafka] try -> DEBU 0ee[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:51:32.113 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 0ef[0m Initializing new client
[36m2019-01-29 10:51:32.113 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 0f0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:51:32.113 UTC [fsblkstorage] loadCurrentInfo -> DEBU 0f1[0m loaded checkpointInfo:latestFileChunkSuffixNum=[0], latestFileChunksize=[43120], isChainEmpty=[false], lastBlockNumber=[1]
[36m2019-01-29 10:51:32.113 UTC [fsblkstorage] newBlockfileMgr -> DEBU 0f3[0m Synching block information from block storage (if needed)
[36m2019-01-29 10:51:32.113 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 0f2[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:51:32.113 UTC [fsblkstorage] syncCPInfoFromFS -> DEBU 0f4[0m Starting checkpoint=latestFileChunkSuffixNum=[0], latestFileChunksize=[43120], isChainEmpty=[false], lastBlockNumber=[1]
[36m2019-01-29 10:51:32.113 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0f5[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:51:32.113 UTC [fsblkstorage] syncCPInfoFromFS -> DEBU 0f6[0m status of file [/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000]: exists=[true], size=[43120]
[36m2019-01-29 10:51:32.115 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 0f7[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.29:10092: connect: connection refused
[36m2019-01-29 10:51:32.115 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0f8[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.29:10092: connect: connection refused
[36m2019-01-29 10:51:32.115 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 0f9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:51:32.115 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0fa[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:51:32.115 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 0fb[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.28:11092: connect: connection refused
[36m2019-01-29 10:51:32.115 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0fc[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.28:11092: connect: connection refused
[36m2019-01-29 10:51:32.115 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 0fd[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:51:32.115 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 0fe[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:51:32.116 UTC [fsblkstorage] newBlockIndex -> DEBU 0ff[0m newBlockIndex() - indexItems:[[BlockNum]]
[36m2019-01-29 10:51:32.116 UTC [fsblkstorage] syncIndex -> DEBU 100[0m Both the block files and indices are in sync.
[36m2019-01-29 10:51:32.116 UTC [fsblkstorage] retrieveBlockHeaderByNumber -> DEBU 101[0m retrieveBlockHeaderByNumber() - blockNum = [1]
[36m2019-01-29 10:51:32.116 UTC [fsblkstorage] newBlockfileStream -> DEBU 102[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[18495]
[36m2019-01-29 10:51:32.116 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 103[0m Remaining bytes=[24625], Going to peek [8] bytes
[36m2019-01-29 10:51:32.116 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 104[0m Returning blockbytes - length=[24622], placementInfo={fileNum=[0], startOffset=[18495], bytesOffset=[18498]}
[36m2019-01-29 10:51:32.116 UTC [fsblkstorage] Next -> DEBU 105[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=1
[36m2019-01-29 10:51:32.116 UTC [fsblkstorage] newBlockfileStream -> DEBU 107[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[18495]
[36m2019-01-29 10:51:32.116 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 106[0m Connected to broker at kafka0.switch2logic.co.za:9092 (unregistered)
[36m2019-01-29 10:51:32.116 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 108[0m Remaining bytes=[24625], Going to peek [8] bytes
[36m2019-01-29 10:51:32.117 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 109[0m Returning blockbytes - length=[24622], placementInfo={fileNum=[0], startOffset=[18495], bytesOffset=[18498]}
[36m2019-01-29 10:51:32.117 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 10a[0m blockbytes [24622] read from file [0]
[36m2019-01-29 10:51:32.117 UTC [fsblkstorage] Next -> DEBU 10b[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=1
[36m2019-01-29 10:51:32.117 UTC [fsblkstorage] newBlockfileStream -> DEBU 10c[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 10:51:32.117 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 10d[0m Remaining bytes=[43120], Going to peek [8] bytes
[36m2019-01-29 10:51:32.117 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 10e[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 10:51:32.117 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 10f[0m blockbytes [18492] read from file [0]
[36m2019-01-29 10:51:32.117 UTC [common/channelconfig] NewStandardValues -> DEBU 110[0m Initializing protos for *channelconfig.ChannelProtos
[36m2019-01-29 10:51:32.117 UTC [common/channelconfig] initializeProtosStruct -> DEBU 111[0m Processing field: HashingAlgorithm
[36m2019-01-29 10:51:32.117 UTC [common/channelconfig] initializeProtosStruct -> DEBU 112[0m Processing field: BlockDataHashingStructure
[36m2019-01-29 10:51:32.117 UTC [common/channelconfig] initializeProtosStruct -> DEBU 113[0m Processing field: OrdererAddresses
[36m2019-01-29 10:51:32.117 UTC [common/channelconfig] initializeProtosStruct -> DEBU 114[0m Processing field: Consortium
[36m2019-01-29 10:51:32.117 UTC [common/channelconfig] initializeProtosStruct -> DEBU 115[0m Processing field: Capabilities
[36m2019-01-29 10:51:32.117 UTC [common/channelconfig] NewStandardValues -> DEBU 116[0m Initializing protos for *channelconfig.ConsortiumProtos
[36m2019-01-29 10:51:32.117 UTC [common/channelconfig] initializeProtosStruct -> DEBU 117[0m Processing field: ChannelCreationPolicy
[36m2019-01-29 10:51:32.117 UTC [common/channelconfig] NewStandardValues -> DEBU 118[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 10:51:32.117 UTC [common/channelconfig] initializeProtosStruct -> DEBU 119[0m Processing field: MSP
[36m2019-01-29 10:51:32.117 UTC [common/channelconfig] validateMSP -> DEBU 11a[0m Setting up MSP for org Hospital1MSP
[36m2019-01-29 10:51:32.117 UTC [msp] newBccspMsp -> DEBU 11b[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:51:32.117 UTC [msp] New -> DEBU 11c[0m Creating Cache-MSP instance
[36m2019-01-29 10:51:32.117 UTC [msp] Setup -> DEBU 11d[0m Setting up MSP instance Hospital1MSP
[36m2019-01-29 10:51:32.118 UTC [msp/identity] newIdentity -> DEBU 11e[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdDCCAhugAwIBAgIQZxH8VsnceZxXgKxlURLgVTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMS5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQX9V4k/h/3pFxByKDscjgQrC0YS/bSbKvzSG2Z2PvltEWp40qB9OaKUN0GQYzo
yK2Etdvyjfd1OFwaTjpGSvv9o18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCBVU7DoP9Msf/nNTNl6
5FxwAXZ82eaGBT7zWYdyh4QbBjAKBggqhkjOPQQDAgNHADBEAiBmXGWQbxCGRp0c
5/y9qJyciB1CDHsknRZDjNzt5f563wIgMy7VPi1R8DpIIIRN9B3mfhDpRa+J+8PA
S4DOJy7rjp4=
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.118 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 11f[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 10:51:32.118 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 120[0m client/brokers registered new broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 10:51:32.118 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 121[0m client/brokers registered new broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:51:32.118 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 122[0m Successfully initialized new client
[36m2019-01-29 10:51:32.118 UTC [orderer/consensus/kafka] try -> DEBU 123[0m [channel: comunitychannel] Error is nil, breaking the retry loop
2019-01-29 10:51:32.118 UTC [orderer/consensus/kafka] startThread -> INFO 124[0m [channel: comunitychannel] Producer set up successfully
2019-01-29 10:51:32.118 UTC [orderer/consensus/kafka] sendConnectMessage -> INFO 126[0m [channel: comunitychannel] About to post the CONNECT message...
[36m2019-01-29 10:51:32.118 UTC [msp/identity] newIdentity -> DEBU 125[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.118 UTC [orderer/consensus/kafka] try -> DEBU 127[0m [channel: comunitychannel] Attempting to post the CONNECT message...
[36m2019-01-29 10:51:32.118 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 128[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:51:32.119 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 12a[0m Checking if identity satisfies role [CLIENT] for Hospital1MSP
[36m2019-01-29 10:51:32.119 UTC [msp] Validate -> DEBU 12b[0m MSP Hospital1MSP validating identity
[36m2019-01-29 10:51:32.119 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 12c[0m Connected to broker at kafka0.switch2logic.co.za:9092 (registered as #0)
[36m2019-01-29 10:51:32.118 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU 129[0m producer/broker/0 starting up
[36m2019-01-29 10:51:32.119 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU 12d[0m producer/broker/0 state change to [open] on comunitychannel/0
[36m2019-01-29 10:51:32.120 UTC [msp] getCertificationChain -> DEBU 12e[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 10:51:32.120 UTC [msp] hasOURole -> DEBU 12f[0m MSP Hospital1MSP checking if the identity is a client
[36m2019-01-29 10:51:32.120 UTC [msp] getCertificationChain -> DEBU 130[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 10:51:32.120 UTC [common/channelconfig] NewStandardValues -> DEBU 131[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 10:51:32.120 UTC [common/channelconfig] initializeProtosStruct -> DEBU 132[0m Processing field: MSP
[36m2019-01-29 10:51:32.120 UTC [common/channelconfig] validateMSP -> DEBU 133[0m Setting up MSP for org Hospital2MSP
[36m2019-01-29 10:51:32.120 UTC [msp] newBccspMsp -> DEBU 134[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:51:32.120 UTC [msp] New -> DEBU 135[0m Creating Cache-MSP instance
[36m2019-01-29 10:51:32.120 UTC [msp] Setup -> DEBU 136[0m Setting up MSP instance Hospital2MSP
[36m2019-01-29 10:51:32.121 UTC [msp/identity] newIdentity -> DEBU 137[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdjCCAhygAwIBAgIRALZ3Pffn/xWUtUWPSK6CYVAwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowgYsxCzAJBgNVBAYTAlVTMRMwEQYD
VQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4gRnJhbmNpc2NvMSUwIwYDVQQK
Exxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMSgwJgYDVQQDEx9jYS5ob3Nw
aXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcD
QgAESzVpiz6P4mIb+KwvrttcXJKZR5i137Is4298E7OrkYdAg0tB4rpfKTXzUMwW
vI12cTDujpyaZSiqQ3/zP4fOQqNfMF0wDgYDVR0PAQH/BAQDAgGmMA8GA1UdJQQI
MAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQg8QJqWKPaX24HINwR
S6sINFSwRD+440/ccSnvFVaGF24wCgYIKoZIzj0EAwIDSAAwRQIhAPO2Nng3/nqb
cKPliUqVPVjBMu+D55M90+jexWSOpkjcAiADPx8qv9exZ4BfP3+G49Qzpx0Meblj
53vzUgYFa/0EXw==
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.121 UTC [msp/identity] newIdentity -> DEBU 138[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAJ6XtHMTTW1JzdpD54qCc3wwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA+z1rb9aA1HwHfNLi9V9VZe
bdwT4MXak445zGz4DXzRUsmNeBngzntKb6qpgHQpeFtzbzLx+zwiynRDvfdzORGj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPEC
alij2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQC
IA8A3CCJeCasyy7gYd/M1aHYmwWsecY7UY7cVUGy4XOxAiB0rrmfnGW6JA11PDCu
+5c5Rpn0jF5ddsFn+UIpzBFvmg==
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.122 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 139[0m Checking if identity satisfies role [CLIENT] for Hospital2MSP
[36m2019-01-29 10:51:32.122 UTC [msp] Validate -> DEBU 13a[0m MSP Hospital2MSP validating identity
[36m2019-01-29 10:51:32.122 UTC [msp] getCertificationChain -> DEBU 13b[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 10:51:32.122 UTC [msp] hasOURole -> DEBU 13c[0m MSP Hospital2MSP checking if the identity is a client
[36m2019-01-29 10:51:32.122 UTC [msp] getCertificationChain -> DEBU 13d[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 10:51:32.122 UTC [common/channelconfig] NewStandardValues -> DEBU 13e[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 10:51:32.122 UTC [common/channelconfig] initializeProtosStruct -> DEBU 13f[0m Processing field: MSP
[36m2019-01-29 10:51:32.122 UTC [common/channelconfig] validateMSP -> DEBU 140[0m Setting up MSP for org Hospital3MSP
[36m2019-01-29 10:51:32.122 UTC [msp] newBccspMsp -> DEBU 141[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:51:32.122 UTC [msp] New -> DEBU 142[0m Creating Cache-MSP instance
[36m2019-01-29 10:51:32.122 UTC [msp] Setup -> DEBU 143[0m Setting up MSP instance Hospital3MSP
[36m2019-01-29 10:51:32.123 UTC [msp/identity] newIdentity -> DEBU 144[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdTCCAhugAwIBAgIQCCXeHGgObIJo8aq4AdvgVjAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMy5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQm1Oo/AEu7ZZToE15Y5ntlaN128Hs/exX3jNH5nO6r4tvyFtBBNEn1ecXxxPtm
dmQ+nPHONorLwDC42SMAKwfro18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCA7vVWFXWpgZBojQ1ZL
xUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEAlOUoPtpfO8V7
vFRCanjU5zZWGO5ajFlPDJjjDTy3UrwCICrUYV32JrR/m3dslCl15dSThHrKtYCC
Ehvyw1+lNqiG
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.123 UTC [msp/identity] newIdentity -> DEBU 145[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAI8BA/5Jf+5m8ncI+D7aSiMwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwzLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABF1IehIKIjh8tXYZ7rw4QOuc
g/tpBAkJeOt44zr/LepbjhyFW8vIRnkw75i6Wcg+LOoAS+2NFNg2oNjqg//yCHWj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIDu9
VYVdamBkGiNDVkvFS+AEt6Jrn0yK8hFyPGZhqenJMAoGCCqGSM49BAMCA0cAMEQC
IFyc85pwbCAxGNdw4fflsX2I3HrP3h93YAB2zSDC8HtkAiAQxblAZfEnXUiQKrqE
8I10bYJqD6pJz1jb3Pv+4xplTA==
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.124 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 146[0m Checking if identity satisfies role [CLIENT] for Hospital3MSP
[36m2019-01-29 10:51:32.124 UTC [msp] Validate -> DEBU 147[0m MSP Hospital3MSP validating identity
[36m2019-01-29 10:51:32.124 UTC [msp] getCertificationChain -> DEBU 148[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 10:51:32.124 UTC [msp] hasOURole -> DEBU 149[0m MSP Hospital3MSP checking if the identity is a client
[36m2019-01-29 10:51:32.124 UTC [msp] getCertificationChain -> DEBU 14a[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 10:51:32.124 UTC [common/channelconfig] NewStandardValues -> DEBU 14b[0m Initializing protos for *channelconfig.OrdererProtos
[36m2019-01-29 10:51:32.124 UTC [common/channelconfig] initializeProtosStruct -> DEBU 14c[0m Processing field: ConsensusType
[36m2019-01-29 10:51:32.124 UTC [common/channelconfig] initializeProtosStruct -> DEBU 14d[0m Processing field: BatchSize
[36m2019-01-29 10:51:32.124 UTC [common/channelconfig] initializeProtosStruct -> DEBU 14e[0m Processing field: BatchTimeout
[36m2019-01-29 10:51:32.124 UTC [common/channelconfig] initializeProtosStruct -> DEBU 14f[0m Processing field: KafkaBrokers
[36m2019-01-29 10:51:32.124 UTC [common/channelconfig] initializeProtosStruct -> DEBU 150[0m Processing field: ChannelRestrictions
[36m2019-01-29 10:51:32.124 UTC [common/channelconfig] initializeProtosStruct -> DEBU 151[0m Processing field: Capabilities
[36m2019-01-29 10:51:32.125 UTC [common/channelconfig] NewStandardValues -> DEBU 152[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 10:51:32.125 UTC [common/channelconfig] initializeProtosStruct -> DEBU 153[0m Processing field: MSP
[36m2019-01-29 10:51:32.125 UTC [common/channelconfig] validateMSP -> DEBU 154[0m Setting up MSP for org OrdererOrg
[36m2019-01-29 10:51:32.125 UTC [msp] newBccspMsp -> DEBU 155[0m Creating BCCSP-based MSP instance
[36m2019-01-29 10:51:32.125 UTC [msp] New -> DEBU 156[0m Creating Cache-MSP instance
[36m2019-01-29 10:51:32.125 UTC [msp] Setup -> DEBU 157[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 10:51:32.125 UTC [msp/identity] newIdentity -> DEBU 158[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.125 UTC [msp/identity] newIdentity -> DEBU 159[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 10:51:32.125 UTC [msp] Validate -> DEBU 15a[0m MSP OrdererMSP validating identity
[36m2019-01-29 10:51:32.126 UTC [msp] Setup -> DEBU 15b[0m Setting up the MSP manager (4 msps)
[36m2019-01-29 10:51:32.126 UTC [msp] Setup -> DEBU 15c[0m MSP manager setup complete, setup 4 msps
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 15d[0m Proposed new policy Admins for Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 15e[0m Proposed new policy Readers for Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 15f[0m Proposed new policy Writers for Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 160[0m Proposed new policy Writers for Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 161[0m Proposed new policy Admins for Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 162[0m Proposed new policy Readers for Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 163[0m Proposed new policy Admins for Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 164[0m Proposed new policy Readers for Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 165[0m Proposed new policy Writers for Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 166[0m Proposed new policy Admins for Channel/Consortiums
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 167[0m Proposed new policy Readers for Channel/Orderer/OrdererOrg
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 168[0m Proposed new policy Writers for Channel/Orderer/OrdererOrg
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 169[0m Proposed new policy Admins for Channel/Orderer/OrdererOrg
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 16a[0m Proposed new policy Admins for Channel/Orderer
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 16b[0m Proposed new policy Readers for Channel/Orderer
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 16c[0m Proposed new policy Writers for Channel/Orderer
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 16d[0m Proposed new policy BlockValidation for Channel/Orderer
[36m2019-01-29 10:51:32.126 UTC [policies] GetPolicy -> DEBU 16e[0m Returning dummy reject all policy because Readers could not be found in Channel/Consortiums/Readers
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 16f[0m Proposed new policy Readers for Channel
[36m2019-01-29 10:51:32.126 UTC [policies] GetPolicy -> DEBU 170[0m Returning dummy reject all policy because Writers could not be found in Channel/Consortiums/Writers
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 171[0m Proposed new policy Writers for Channel
[36m2019-01-29 10:51:32.126 UTC [policies] NewManagerImpl -> DEBU 172[0m Proposed new policy Admins for Channel
[36m2019-01-29 10:51:32.126 UTC [common/configtx] addToMap -> DEBU 173[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 10:51:32.126 UTC [common/configtx] addToMap -> DEBU 174[0m Adding to config map: [Group]  /Channel/Consortiums
[36m2019-01-29 10:51:32.126 UTC [common/configtx] addToMap -> DEBU 175[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium
[36m2019-01-29 10:51:32.126 UTC [common/configtx] addToMap -> DEBU 176[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium/Hospital3MSP
[36m2019-01-29 10:51:32.126 UTC [common/configtx] addToMap -> DEBU 177[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/Hospital3MSP/MSP
[36m2019-01-29 10:51:32.126 UTC [common/configtx] addToMap -> DEBU 178[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital3MSP/Admins
[36m2019-01-29 10:51:32.126 UTC [common/configtx] addToMap -> DEBU 179[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital3MSP/Readers
[36m2019-01-29 10:51:32.126 UTC [common/configtx] addToMap -> DEBU 17a[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital3MSP/Writers
[36m2019-01-29 10:51:32.126 UTC [common/configtx] addToMap -> DEBU 17b[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium/Hospital1MSP
[36m2019-01-29 10:51:32.126 UTC [common/configtx] addToMap -> DEBU 17c[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/Hospital1MSP/MSP
[36m2019-01-29 10:51:32.126 UTC [common/configtx] addToMap -> DEBU 17d[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital1MSP/Writers
[36m2019-01-29 10:51:32.126 UTC [common/configtx] addToMap -> DEBU 17e[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital1MSP/Admins
[36m2019-01-29 10:51:32.126 UTC [common/configtx] addToMap -> DEBU 17f[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital1MSP/Readers
[36m2019-01-29 10:51:32.126 UTC [common/configtx] addToMap -> DEBU 180[0m Adding to config map: [Group]  /Channel/Consortiums/HospitalConsortium/Hospital2MSP
[36m2019-01-29 10:51:32.126 UTC [common/configtx] addToMap -> DEBU 181[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/Hospital2MSP/MSP
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 182[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital2MSP/Readers
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 183[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital2MSP/Writers
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 184[0m Adding to config map: [Policy] /Channel/Consortiums/HospitalConsortium/Hospital2MSP/Admins
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 185[0m Adding to config map: [Value]  /Channel/Consortiums/HospitalConsortium/ChannelCreationPolicy
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 186[0m Adding to config map: [Policy] /Channel/Consortiums/Admins
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 187[0m Adding to config map: [Group]  /Channel/Orderer
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 188[0m Adding to config map: [Group]  /Channel/Orderer/OrdererOrg
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 189[0m Adding to config map: [Value]  /Channel/Orderer/OrdererOrg/MSP
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 18a[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 18b[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 18c[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Admins
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 18d[0m Adding to config map: [Value]  /Channel/Orderer/ConsensusType
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 18e[0m Adding to config map: [Value]  /Channel/Orderer/BatchSize
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 18f[0m Adding to config map: [Value]  /Channel/Orderer/BatchTimeout
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 190[0m Adding to config map: [Value]  /Channel/Orderer/ChannelRestrictions
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 191[0m Adding to config map: [Value]  /Channel/Orderer/Capabilities
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 192[0m Adding to config map: [Value]  /Channel/Orderer/KafkaBrokers
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 193[0m Adding to config map: [Policy] /Channel/Orderer/Admins
[36m2019-01-29 10:51:32.127 UTC [common/configtx] addToMap -> DEBU 194[0m Adding to config map: [Policy] /Channel/Orderer/Readers
[36m2019-01-29 10:51:32.128 UTC [common/configtx] addToMap -> DEBU 195[0m Adding to config map: [Policy] /Channel/Orderer/Writers
[36m2019-01-29 10:51:32.128 UTC [common/configtx] addToMap -> DEBU 196[0m Adding to config map: [Policy] /Channel/Orderer/BlockValidation
[36m2019-01-29 10:51:32.128 UTC [common/configtx] addToMap -> DEBU 197[0m Adding to config map: [Value]  /Channel/HashingAlgorithm
[36m2019-01-29 10:51:32.128 UTC [common/configtx] addToMap -> DEBU 198[0m Adding to config map: [Value]  /Channel/BlockDataHashingStructure
[36m2019-01-29 10:51:32.128 UTC [common/configtx] addToMap -> DEBU 199[0m Adding to config map: [Value]  /Channel/OrdererAddresses
[36m2019-01-29 10:51:32.128 UTC [common/configtx] addToMap -> DEBU 19a[0m Adding to config map: [Value]  /Channel/Capabilities
[36m2019-01-29 10:51:32.128 UTC [common/configtx] addToMap -> DEBU 19b[0m Adding to config map: [Policy] /Channel/Admins
[36m2019-01-29 10:51:32.128 UTC [common/configtx] addToMap -> DEBU 19c[0m Adding to config map: [Policy] /Channel/Readers
[36m2019-01-29 10:51:32.128 UTC [common/configtx] addToMap -> DEBU 19d[0m Adding to config map: [Policy] /Channel/Writers
[36m2019-01-29 10:51:32.128 UTC [common/channelconfig] LogSanityChecks -> DEBU 19e[0m As expected, current configuration has policy '/Channel/Readers'
[36m2019-01-29 10:51:32.128 UTC [common/channelconfig] LogSanityChecks -> DEBU 19f[0m As expected, current configuration has policy '/Channel/Writers'
[36m2019-01-29 10:51:32.128 UTC [policies] Manager -> DEBU 1a0[0m Manager Channel looking up path [Application]
[36m2019-01-29 10:51:32.128 UTC [policies] Manager -> DEBU 1a1[0m Manager Channel has managers Consortiums
[36m2019-01-29 10:51:32.128 UTC [policies] Manager -> DEBU 1a2[0m Manager Channel has managers Orderer
[36m2019-01-29 10:51:32.128 UTC [policies] Manager -> DEBU 1a3[0m Manager Channel looking up path [Orderer]
[36m2019-01-29 10:51:32.128 UTC [policies] Manager -> DEBU 1a4[0m Manager Channel has managers Consortiums
[36m2019-01-29 10:51:32.128 UTC [policies] Manager -> DEBU 1a5[0m Manager Channel has managers Orderer
[36m2019-01-29 10:51:32.128 UTC [policies] Manager -> DEBU 1a6[0m Manager Channel/Orderer looking up path []
[36m2019-01-29 10:51:32.128 UTC [policies] Manager -> DEBU 1a7[0m Manager Channel/Orderer has managers OrdererOrg
[36m2019-01-29 10:51:32.128 UTC [common/channelconfig] LogSanityChecks -> DEBU 1a8[0m As expected, current configuration has policy '/Channel/Orderer/BlockValidation'
[36m2019-01-29 10:51:32.129 UTC [common/capabilities] Supported -> DEBU 1a9[0m Orderer capability V1_1 is supported and is enabled
[36m2019-01-29 10:51:32.129 UTC [common/capabilities] Supported -> DEBU 1aa[0m Channel capability V1_1 is supported and is enabled
[36m2019-01-29 10:51:32.129 UTC [fsblkstorage] Next -> DEBU 1ab[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=1
[36m2019-01-29 10:51:32.129 UTC [fsblkstorage] newBlockfileStream -> DEBU 1ac[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[18495]
[36m2019-01-29 10:51:32.129 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 1ad[0m Remaining bytes=[24625], Going to peek [8] bytes
[36m2019-01-29 10:51:32.129 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 1ae[0m Returning blockbytes - length=[24622], placementInfo={fileNum=[0], startOffset=[18495], bytesOffset=[18498]}
[36m2019-01-29 10:51:32.129 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 1af[0m blockbytes [24622] read from file [0]
[36m2019-01-29 10:51:32.129 UTC [orderer/commmon/multichannel] newBlockWriter -> DEBU 1b0[0m [channel: testchainid] Creating block writer for tip of chain (blockNumber=1, lastConfigBlockNum=0, lastConfigSeq=0)
2019-01-29 10:51:32.129 UTC [orderer/consensus/kafka] newChain -> INFO 1b1[0m [channel: testchainid] Starting chain with last persisted offset 5 and last recorded block 1
[36m2019-01-29 10:51:32.129 UTC [orderer/commmon/multichannel] newChainSupport -> DEBU 1b2[0m [channel: testchainid] Done creating channel support resources
[36m2019-01-29 10:51:32.129 UTC [orderer/common/msgprocessor] NewSystemChannel -> DEBU 1b3[0m Creating system channel msg processor for channel testchainid
[36m2019-01-29 10:51:32.129 UTC [fsblkstorage] Next -> DEBU 1b4[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=1
[36m2019-01-29 10:51:32.129 UTC [fsblkstorage] newBlockfileStream -> DEBU 1b5[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/testchainid/blockfile_000000], startOffset=[0]
[36m2019-01-29 10:51:32.129 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 1b6[0m Remaining bytes=[43120], Going to peek [8] bytes
[36m2019-01-29 10:51:32.129 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 1b7[0m Returning blockbytes - length=[18492], placementInfo={fileNum=[0], startOffset=[0], bytesOffset=[3]}
[36m2019-01-29 10:51:32.129 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 1b8[0m blockbytes [18492] read from file [0]
2019-01-29 10:51:32.129 UTC [orderer/commmon/multichannel] NewRegistrar -> INFO 1b9[0m Starting system channel 'testchainid' with genesis block hash c84c51e50921c0294405e41d16cb6234273f63b37a240438cc9f32578277d7e3 and orderer type kafka
2019-01-29 10:51:32.129 UTC [orderer/common/server] Start -> INFO 1ba[0m Starting orderer:
 Version: 1.2.0
 Commit SHA: cae2ad4
 Go version: go1.10
 OS/Arch: linux/amd64
 Experimental features: false
2019-01-29 10:51:32.130 UTC [orderer/common/server] Start -> INFO 1bb[0m Beginning to serve requests
2019-01-29 10:51:32.130 UTC [orderer/consensus/kafka] setupProducerForChannel -> INFO 1bc[0m [channel: testchainid] Setting up the producer for this channel...
[36m2019-01-29 10:51:32.130 UTC [orderer/consensus/kafka] try -> DEBU 1bd[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:32.130 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 1be[0m Initializing new client
[36m2019-01-29 10:51:32.130 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 1bf[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:51:32.130 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1c0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:51:32.130 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1c1[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:51:32.132 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1c2[0m Connected to broker at kafka0.switch2logic.co.za:9092 (unregistered)
[36m2019-01-29 10:51:32.142 UTC [orderer/consensus/kafka] try -> DEBU 1c3[0m [channel: comunitychannel] Error is nil, breaking the retry loop
[36m2019-01-29 10:51:32.142 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 1c4[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
2019-01-29 10:51:32.142 UTC [orderer/consensus/kafka] startThread -> INFO 1c5[0m [channel: comunitychannel] CONNECT message posted successfully
2019-01-29 10:51:32.142 UTC [orderer/consensus/kafka] setupParentConsumerForChannel -> INFO 1c7[0m [channel: comunitychannel] Setting up the parent consumer for this channel...
[36m2019-01-29 10:51:32.142 UTC [orderer/consensus/kafka] try -> DEBU 1c8[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:51:32.142 UTC [orderer/consensus/kafka/sarama] NewConsumer -> DEBU 1c9[0m Initializing new client
[36m2019-01-29 10:51:32.142 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 1ca[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:51:32.142 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1cb[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:51:32.142 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1cc[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 10:51:32.142 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 1c6[0m client/brokers registered new broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 10:51:32.142 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 1cd[0m client/brokers registered new broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:51:32.142 UTC [orderer/consensus/kafka/sarama] NewAsyncProducer -> DEBU 1ce[0m Successfully initialized new client
[36m2019-01-29 10:51:32.142 UTC [orderer/consensus/kafka] try -> DEBU 1cf[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 10:51:32.142 UTC [orderer/consensus/kafka] startThread -> INFO 1d0[0m [channel: testchainid] Producer set up successfully
2019-01-29 10:51:32.142 UTC [orderer/consensus/kafka] sendConnectMessage -> INFO 1d1[0m [channel: testchainid] About to post the CONNECT message...
[36m2019-01-29 10:51:32.142 UTC [orderer/consensus/kafka] try -> DEBU 1d2[0m [channel: testchainid] Attempting to post the CONNECT message...
[36m2019-01-29 10:51:32.143 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1d3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:51:32.143 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU 1d4[0m producer/broker/1 starting up
[36m2019-01-29 10:51:32.143 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU 1d5[0m producer/broker/1 state change to [open] on testchainid/0
[36m2019-01-29 10:51:32.143 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1d6[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.29:10092: connect: connection refused
[36m2019-01-29 10:51:32.143 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1d7[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.29:10092: connect: connection refused
[36m2019-01-29 10:51:32.143 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1d8[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:51:32.143 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1d9[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 10:51:32.143 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1da[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.28:11092: connect: connection refused
[36m2019-01-29 10:51:32.143 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1db[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.28:11092: connect: connection refused
[36m2019-01-29 10:51:32.143 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1dc[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:51:32.143 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1dd[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:51:32.144 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1de[0m Connected to broker at kafka1.switch2logic.co.za:9092 (registered as #1)
[36m2019-01-29 10:51:32.144 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1df[0m Connected to broker at kafka0.switch2logic.co.za:9092 (unregistered)
[36m2019-01-29 10:51:32.146 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 1e0[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 10:51:32.146 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 1e1[0m client/brokers registered new broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 10:51:32.146 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 1e2[0m client/brokers registered new broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:51:32.146 UTC [orderer/consensus/kafka/sarama] NewConsumer -> DEBU 1e3[0m Successfully initialized new client
[36m2019-01-29 10:51:32.146 UTC [orderer/consensus/kafka] try -> DEBU 1e4[0m [channel: comunitychannel] Error is nil, breaking the retry loop
2019-01-29 10:51:32.146 UTC [orderer/consensus/kafka] startThread -> INFO 1e5[0m [channel: comunitychannel] Parent consumer set up successfully
2019-01-29 10:51:32.146 UTC [orderer/consensus/kafka] setupChannelConsumerForChannel -> INFO 1e6[0m [channel: comunitychannel] Setting up the channel consumer for this channel (start offset: -2)...
[36m2019-01-29 10:51:32.146 UTC [orderer/consensus/kafka] try -> DEBU 1e7[0m [channel: comunitychannel] Connecting to the Kafka cluster
[36m2019-01-29 10:51:32.146 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1e8[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:51:32.147 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1e9[0m Connected to broker at kafka0.switch2logic.co.za:9092 (registered as #0)
[36m2019-01-29 10:51:32.149 UTC [orderer/consensus/kafka/sarama] subscriptionConsumer -> DEBU 1ea[0m consumer/broker/0 added subscription to comunitychannel/0
[36m2019-01-29 10:51:32.149 UTC [orderer/consensus/kafka] try -> DEBU 1eb[0m [channel: comunitychannel] Error is nil, breaking the retry loop
2019-01-29 10:51:32.149 UTC [orderer/consensus/kafka] startThread -> INFO 1ec[0m [channel: comunitychannel] Channel consumer set up successfully
2019-01-29 10:51:32.149 UTC [orderer/consensus/kafka] startThread -> INFO 1ed[0m [channel: comunitychannel] Start phase completed successfully
[36m2019-01-29 10:51:32.157 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 1ee[0m [channel: comunitychannel] Successfully unmarshalled consumed message, offset is 0. Inspecting type...
[36m2019-01-29 10:51:32.157 UTC [orderer/consensus/kafka] processConnect -> DEBU 1ef[0m [channel: comunitychannel] It's a connect message - ignoring
[36m2019-01-29 10:51:32.157 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 1f0[0m [channel: comunitychannel] Successfully unmarshalled consumed message, offset is 1. Inspecting type...
[36m2019-01-29 10:51:32.157 UTC [orderer/consensus/kafka] processConnect -> DEBU 1f1[0m [channel: comunitychannel] It's a connect message - ignoring
[36m2019-01-29 10:51:32.157 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 1f2[0m [channel: comunitychannel] Successfully unmarshalled consumed message, offset is 2. Inspecting type...
[36m2019-01-29 10:51:32.158 UTC [orderer/consensus/kafka] processConnect -> DEBU 1f3[0m [channel: comunitychannel] It's a connect message - ignoring
[36m2019-01-29 10:51:32.158 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 1f4[0m [channel: comunitychannel] Successfully unmarshalled consumed message, offset is 3. Inspecting type...
[36m2019-01-29 10:51:32.158 UTC [orderer/consensus/kafka] processConnect -> DEBU 1f5[0m [channel: comunitychannel] It's a connect message - ignoring
[36m2019-01-29 10:51:32.161 UTC [orderer/consensus/kafka] try -> DEBU 1f6[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 10:51:32.161 UTC [orderer/consensus/kafka] startThread -> INFO 1f7[0m [channel: testchainid] CONNECT message posted successfully
2019-01-29 10:51:32.161 UTC [orderer/consensus/kafka] setupParentConsumerForChannel -> INFO 1f8[0m [channel: testchainid] Setting up the parent consumer for this channel...
[36m2019-01-29 10:51:32.161 UTC [orderer/consensus/kafka] try -> DEBU 1f9[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:32.161 UTC [orderer/consensus/kafka/sarama] NewConsumer -> DEBU 1fa[0m Initializing new client
[36m2019-01-29 10:51:32.161 UTC [orderer/consensus/kafka/sarama] NewClient -> DEBU 1fb[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:51:32.161 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 1fc[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:51:32.161 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 1fd[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:51:32.162 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 1fe[0m Connected to broker at kafka0.switch2logic.co.za:9092 (unregistered)
[36m2019-01-29 10:51:32.164 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 1ff[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 10:51:32.164 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 200[0m client/brokers registered new broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 10:51:32.164 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 201[0m client/brokers registered new broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 10:51:32.164 UTC [orderer/consensus/kafka/sarama] NewConsumer -> DEBU 202[0m Successfully initialized new client
[36m2019-01-29 10:51:32.164 UTC [orderer/consensus/kafka] try -> DEBU 203[0m [channel: testchainid] Error is nil, breaking the retry loop
2019-01-29 10:51:32.164 UTC [orderer/consensus/kafka] startThread -> INFO 204[0m [channel: testchainid] Parent consumer set up successfully
2019-01-29 10:51:32.165 UTC [orderer/consensus/kafka] setupChannelConsumerForChannel -> INFO 205[0m [channel: testchainid] Setting up the channel consumer for this channel (start offset: 6)...
[36m2019-01-29 10:51:32.165 UTC [orderer/consensus/kafka] try -> DEBU 206[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:32.165 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 207[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 10:51:32.166 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 208[0m Connected to broker at kafka1.switch2logic.co.za:9092 (registered as #1)
[36m2019-01-29 10:51:32.168 UTC [orderer/consensus/kafka] try -> DEBU 209[0m [channel: testchainid] Initial attempt failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:32.168 UTC [orderer/consensus/kafka] try -> DEBU 20a[0m [channel: testchainid] Retrying every 1s for a total of 30s
[36m2019-01-29 10:51:33.168 UTC [orderer/consensus/kafka] try -> DEBU 20b[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:33.170 UTC [orderer/consensus/kafka] try -> DEBU 20c[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:34.168 UTC [orderer/consensus/kafka] try -> DEBU 20d[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:34.170 UTC [orderer/consensus/kafka] try -> DEBU 20e[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:35.168 UTC [orderer/consensus/kafka] try -> DEBU 20f[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:35.170 UTC [orderer/consensus/kafka] try -> DEBU 210[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:36.169 UTC [orderer/consensus/kafka] try -> DEBU 211[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:36.172 UTC [orderer/consensus/kafka] try -> DEBU 212[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:37.169 UTC [orderer/consensus/kafka] try -> DEBU 213[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:37.172 UTC [orderer/consensus/kafka] try -> DEBU 214[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:38.170 UTC [orderer/consensus/kafka] try -> DEBU 215[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:38.172 UTC [orderer/consensus/kafka] try -> DEBU 216[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:39.169 UTC [orderer/consensus/kafka] try -> DEBU 217[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:39.171 UTC [orderer/consensus/kafka] try -> DEBU 218[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:40.169 UTC [orderer/consensus/kafka] try -> DEBU 219[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:40.171 UTC [orderer/consensus/kafka] try -> DEBU 21a[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:41.169 UTC [orderer/consensus/kafka] try -> DEBU 21b[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:41.170 UTC [orderer/consensus/kafka] try -> DEBU 21c[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:42.169 UTC [orderer/consensus/kafka] try -> DEBU 21d[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:42.171 UTC [orderer/consensus/kafka] try -> DEBU 21e[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:43.169 UTC [orderer/consensus/kafka] try -> DEBU 21f[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:43.172 UTC [orderer/consensus/kafka] try -> DEBU 220[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:44.169 UTC [orderer/consensus/kafka] try -> DEBU 221[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:44.171 UTC [orderer/consensus/kafka] try -> DEBU 222[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:44.234 UTC [orderer/common/server] Deliver -> DEBU 223[0m Starting new Deliver handler
[36m2019-01-29 10:51:44.234 UTC [common/deliver] Handle -> DEBU 224[0m Starting new deliver loop for 10.0.0.34:59894
[36m2019-01-29 10:51:44.234 UTC [common/deliver] Handle -> DEBU 225[0m Attempting to read seek info message from 10.0.0.34:59894
[36m2019-01-29 10:51:44.235 UTC [policies] Evaluate -> DEBU 226[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers ==
[36m2019-01-29 10:51:44.235 UTC [policies] Evaluate -> DEBU 227[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 10:51:44.235 UTC [policies] Evaluate -> DEBU 228[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers ==
[36m2019-01-29 10:51:44.235 UTC [policies] Evaluate -> DEBU 229[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 10:51:44.235 UTC [policies] Evaluate -> DEBU 22a[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers ==
2019-01-29 10:51:44.235 UTC [msp] DeserializeIdentity -> INFO 22b[0m Obtaining identity
[36m2019-01-29 10:51:44.235 UTC [msp/identity] newIdentity -> DEBU 22c[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTTCCAfOgAwIBAgIQDPIhkZbBMzAPRV2mdTJ2BTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB2MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzENMAsGA1UECxME
cGVlcjErMCkGA1UEAxMicGVlcjAuaG9zcGl0YWwxLnN3aXRjaDJsb2dpYy5jby56
YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABMHSfENe+q4SvhxLb6FxLr2SLYg+
qJnL3whOrSDaEyQgmDz7fwbLhWr0Mapiq8N3Xg1vudI6XbNZxyMTBmZbd3ujTTBL
MA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIFVTsOg/
0yx/+c1M2XrkXHABdnzZ5oYFPvNZh3KHhBsGMAoGCCqGSM49BAMCA0gAMEUCIQDn
gu8tRu0NvsaTNG17i4QtjRftqg56ptadF5YHXBQCGgIgeowGfoCWCIW5T09110Gv
cu4tVgSe4rBzE4yjal6cQVQ=
-----END CERTIFICATE-----
[36m2019-01-29 10:51:44.236 UTC [cauthdsl] func1 -> DEBU 22d[0m 0xc4202560b0 gate 1548759104236115735 evaluation starts
[36m2019-01-29 10:51:44.236 UTC [cauthdsl] func2 -> DEBU 22e[0m 0xc4202560b0 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 10:51:44.236 UTC [cauthdsl] func2 -> DEBU 22f[0m 0xc4202560b0 processing identity 0 with bytes of 0a0c486f73706974616c314d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435454434341664f674177494241674951445049686b5a62424d7a41505256326d64544a324254414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777784c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424d485366454e652b7134537668784c623646784c7232534c59672b0a714a6e4c3377684f72534461457951676d447a376677624c685772304d61706971384e33586731767564493658624e5a78794d54426d5a626433756a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149465654734f672f0a3079782f2b63314d3258726b58484142646e7a5a356f594650764e5a68334b48684273474d416f4743437147534d343942414d43413067414d4555434951446e0a677538745275304e767361544e473137693451746a5266747167353670746164463559485842514347674967656f7747666f43574349573554303931313047760a63753474566753653472427a4534796a616c36635156513d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 10:51:44.236 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 230[0m Checking if identity satisfies MEMBER role for Hospital1MSP
[36m2019-01-29 10:51:44.236 UTC [msp] Validate -> DEBU 231[0m MSP Hospital1MSP validating identity
[36m2019-01-29 10:51:44.236 UTC [msp] getCertificationChain -> DEBU 232[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 10:51:44.236 UTC [cauthdsl] func2 -> DEBU 233[0m 0xc4202560b0 principal matched by identity 0
[36m2019-01-29 10:51:44.236 UTC [msp/identity] Verify -> DEBU 234[0m Verify: digest = 00000000  32 4a 77 18 98 fe c5 2b  5c 1b b2 b7 16 e6 8b a0  |2Jw....+\.......|
00000010  3a 68 8a 92 b9 2c 2d c1  74 ad f1 95 77 0b 20 c4  |:h...,-.t...w. .|
[36m2019-01-29 10:51:44.236 UTC [msp/identity] Verify -> DEBU 235[0m Verify: sig = 00000000  30 45 02 21 00 f2 71 66  5d d5 91 1f 73 38 68 65  |0E.!..qf]...s8he|
00000010  ef a6 15 ca f0 99 09 57  ac 81 30 6f 20 6c 6d f1  |.......W..0o lm.|
00000020  61 9f 5b c5 a3 02 20 6b  74 ab 2e a3 ea cf 54 16  |a.[... kt.....T.|
00000030  21 36 de 34 b2 84 d8 71  74 fb a7 8b 10 77 f0 de  |!6.4...qt....w..|
00000040  e5 74 af 6c 7d 89 a6                              |.t.l}..|
[36m2019-01-29 10:51:44.237 UTC [cauthdsl] func2 -> DEBU 236[0m 0xc4202560b0 principal evaluation succeeds for identity 0
[36m2019-01-29 10:51:44.237 UTC [cauthdsl] func1 -> DEBU 237[0m 0xc4202560b0 gate 1548759104236115735 evaluation succeeds
[36m2019-01-29 10:51:44.237 UTC [policies] Evaluate -> DEBU 238[0m Signature set satisfies policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 10:51:44.237 UTC [policies] Evaluate -> DEBU 239[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 10:51:44.237 UTC [policies] Evaluate -> DEBU 23a[0m Signature set satisfies policy /Channel/Application/Readers
[36m2019-01-29 10:51:44.237 UTC [policies] Evaluate -> DEBU 23b[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers
[36m2019-01-29 10:51:44.237 UTC [policies] Evaluate -> DEBU 23c[0m Signature set satisfies policy /Channel/Readers
[36m2019-01-29 10:51:44.237 UTC [policies] Evaluate -> DEBU 23d[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers
[36m2019-01-29 10:51:44.237 UTC [common/deliver] deliverBlocks -> DEBU 23e[0m [channel: comunitychannel] Received seekInfo (0xc42024bba0) start:<specified:<number:1 > > stop:<specified:<number:18446744073709551615 > >  from 10.0.0.34:59894
[36m2019-01-29 10:51:44.237 UTC [fsblkstorage] waitForBlock -> DEBU 23f[0m Going to wait for newer blocks. maxAvailaBlockNumber=[0], waitForBlockNum=[1]
[36m2019-01-29 10:51:45.169 UTC [orderer/consensus/kafka] try -> DEBU 240[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:45.172 UTC [orderer/consensus/kafka] try -> DEBU 241[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:46.169 UTC [orderer/consensus/kafka] try -> DEBU 242[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:46.172 UTC [orderer/consensus/kafka] try -> DEBU 243[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:47.169 UTC [orderer/consensus/kafka] try -> DEBU 244[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:47.172 UTC [orderer/consensus/kafka] try -> DEBU 245[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:48.169 UTC [orderer/consensus/kafka] try -> DEBU 246[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:48.170 UTC [orderer/consensus/kafka] try -> DEBU 247[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:49.168 UTC [orderer/consensus/kafka] try -> DEBU 248[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:49.170 UTC [orderer/consensus/kafka] try -> DEBU 249[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:50.169 UTC [orderer/consensus/kafka] try -> DEBU 24a[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:50.175 UTC [orderer/consensus/kafka] try -> DEBU 24b[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:51.169 UTC [orderer/consensus/kafka] try -> DEBU 24c[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:51.172 UTC [orderer/consensus/kafka] try -> DEBU 24d[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:52.084 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 24e[0m [channel: comunitychannel] Successfully unmarshalled consumed message, offset is 4. Inspecting type...
[36m2019-01-29 10:51:52.085 UTC [orderer/consensus/kafka] processConnect -> DEBU 24f[0m [channel: comunitychannel] It's a connect message - ignoring
[36m2019-01-29 10:51:52.168 UTC [orderer/consensus/kafka] try -> DEBU 250[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:52.169 UTC [orderer/consensus/kafka] try -> DEBU 251[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:53.169 UTC [orderer/consensus/kafka] try -> DEBU 252[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:53.170 UTC [orderer/consensus/kafka] try -> DEBU 253[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:54.170 UTC [orderer/consensus/kafka] try -> DEBU 254[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:54.173 UTC [orderer/consensus/kafka] try -> DEBU 255[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:55.169 UTC [orderer/consensus/kafka] try -> DEBU 256[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:55.172 UTC [orderer/consensus/kafka] try -> DEBU 257[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:56.169 UTC [orderer/consensus/kafka] try -> DEBU 258[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:56.171 UTC [orderer/consensus/kafka] try -> DEBU 259[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:57.169 UTC [orderer/consensus/kafka] try -> DEBU 25a[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:57.172 UTC [orderer/consensus/kafka] try -> DEBU 25b[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:58.169 UTC [orderer/consensus/kafka] try -> DEBU 25c[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:58.171 UTC [orderer/consensus/kafka] try -> DEBU 25d[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:51:59.169 UTC [orderer/consensus/kafka] try -> DEBU 25e[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:51:59.171 UTC [orderer/consensus/kafka] try -> DEBU 25f[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:52:00.169 UTC [orderer/consensus/kafka] try -> DEBU 260[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:52:00.170 UTC [orderer/consensus/kafka] try -> DEBU 261[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:52:01.169 UTC [orderer/consensus/kafka] try -> DEBU 262[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:52:01.171 UTC [orderer/consensus/kafka] try -> DEBU 263[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:52:02.169 UTC [orderer/consensus/kafka] try -> DEBU 264[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:52:02.171 UTC [orderer/consensus/kafka] try -> DEBU 265[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:52:02.172 UTC [orderer/consensus/kafka] retry -> DEBU 266[0m [channel: testchainid] Switching to the long retry interval
[36m2019-01-29 10:52:02.172 UTC [orderer/consensus/kafka] try -> DEBU 267[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:52:02.173 UTC [orderer/consensus/kafka] try -> DEBU 268[0m [channel: testchainid] Initial attempt failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 10:52:02.173 UTC [orderer/consensus/kafka] try -> DEBU 269[0m [channel: testchainid] Retrying every 5m0s for a total of 12h0m0s
[36m2019-01-29 10:57:02.173 UTC [orderer/consensus/kafka] try -> DEBU 26a[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 10:57:02.174 UTC [orderer/consensus/kafka] try -> DEBU 26b[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 11:01:32.119 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 26c[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:01:32.143 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 26d[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:01:32.146 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 26e[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:01:32.165 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 26f[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:02:02.173 UTC [orderer/consensus/kafka] try -> DEBU 270[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 11:02:02.176 UTC [orderer/consensus/kafka] try -> DEBU 271[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 11:02:42.553 UTC [orderer/consensus/kafka/sarama] subscriptionConsumer)-fm -> DEBU 272[0m consumer/broker/0 disconnecting due to error processing FetchRequest: EOF
[36m2019-01-29 11:02:42.553 UTC [orderer/consensus/kafka/sarama] abort -> DEBU 273[0m Closed connection to broker kafka0.switch2logic.co.za:9092
[31m2019-01-29 11:02:42.553 UTC [orderer/consensus/kafka] processMessagesToBlocks -> ERRO 274[0m [channel: comunitychannel] Error during consumption: kafka: error while consuming comunitychannel/0: EOF
[33m2019-01-29 11:02:42.553 UTC [orderer/consensus/kafka] processMessagesToBlocks -> WARN 275[0m [channel: comunitychannel] Deliver sessions will be dropped if consumption errors continue.
[36m2019-01-29 11:02:44.554 UTC [orderer/consensus/kafka/sarama] dispatcher)-fm -> DEBU 276[0m consumer/comunitychannel/0 finding new broker
[36m2019-01-29 11:02:44.554 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 277[0m client/metadata fetching metadata for [comunitychannel] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:02:44.554 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 278[0m client/metadata got error from broker while fetching metadata: EOF
[36m2019-01-29 11:02:44.554 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 279[0m Closed connection to broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:02:44.554 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 27a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:44.554 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 27b[0m client/metadata fetching metadata for [comunitychannel] from broker kafka2.switch2logic.co.za:9092
[36m2019-01-29 11:02:44.556 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 27c[0m Failed to connect to broker kafka2.switch2logic.co.za:9092: dial tcp 10.0.0.43:9092: connect: connection refused
[36m2019-01-29 11:02:44.556 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 27d[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.43:9092: connect: connection refused
[36m2019-01-29 11:02:44.556 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 27e[0m client/brokers deregistered broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 11:02:44.556 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 27f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:44.556 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 280[0m client/metadata fetching metadata for [comunitychannel] from broker kafka1.switch2logic.co.za:9092
[36m2019-01-29 11:02:45.563 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 281[0m Failed to connect to broker kafka1.switch2logic.co.za:9092: dial tcp 10.0.0.44:9092: connect: connection refused
[36m2019-01-29 11:02:45.563 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 282[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.44:9092: connect: connection refused
[36m2019-01-29 11:02:45.563 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 283[0m client/brokers deregistered broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 11:02:45.563 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 284[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:45.563 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 285[0m client/metadata fetching metadata for [comunitychannel] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:02:45.568 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 286[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp 10.0.0.45:9092: connect: connection refused
[36m2019-01-29 11:02:45.568 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 287[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.45:9092: connect: connection refused
[36m2019-01-29 11:02:45.568 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 288[0m client/brokers deregistered broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:02:45.568 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 289[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 11:02:45.568 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 28a[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 11:02:45.568 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 28b[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 11:02:45.819 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 28c[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:45.819 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 28d[0m client/metadata fetching metadata for [comunitychannel] from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 11:02:45.823 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 28e[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:02:45.823 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 28f[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:02:45.823 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 290[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:45.823 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 291[0m client/metadata fetching metadata for [comunitychannel] from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 11:02:45.831 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 292[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:02:45.831 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 293[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:02:45.831 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 294[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:45.836 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 295[0m client/metadata fetching metadata for [comunitychannel] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:02:45.847 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 296[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp 10.0.0.45:9092: connect: connection refused
[36m2019-01-29 11:02:45.847 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 297[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.45:9092: connect: connection refused
[36m2019-01-29 11:02:45.847 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 298[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 11:02:45.847 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 299[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 11:02:45.847 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 29a[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 11:02:46.097 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 29b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:46.100 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 29c[0m client/metadata fetching metadata for [comunitychannel] from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 11:02:46.103 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 29d[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:02:46.103 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 29e[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:02:46.103 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 29f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:46.104 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2a0[0m client/metadata fetching metadata for [comunitychannel] from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 11:02:46.105 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2a1[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:02:46.105 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2a2[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:02:46.105 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2a3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:46.105 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2a4[0m client/metadata fetching metadata for [comunitychannel] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:02:46.106 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2a5[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp 10.0.0.45:9092: connect: connection refused
[36m2019-01-29 11:02:46.106 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2a6[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.45:9092: connect: connection refused
[36m2019-01-29 11:02:46.106 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2a7[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 11:02:46.106 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2a8[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 11:02:46.106 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2a9[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 11:02:46.357 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2aa[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:46.365 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2ab[0m client/metadata fetching metadata for [comunitychannel] from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 11:02:46.374 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2ac[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:02:46.374 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2ad[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:02:46.374 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2ae[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:46.374 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2af[0m client/metadata fetching metadata for [comunitychannel] from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 11:02:46.385 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2b0[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:02:46.385 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2b1[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:02:46.385 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2b2[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:46.385 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2b3[0m client/metadata fetching metadata for [comunitychannel] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:02:46.389 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2b4[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp 10.0.0.45:9092: connect: connection refused
[36m2019-01-29 11:02:46.389 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2b5[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.45:9092: connect: connection refused
[36m2019-01-29 11:02:46.389 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2b6[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 11:02:46.390 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2b7[0m client/brokers resurrecting 3 dead seed brokers
[31m2019-01-29 11:02:46.390 UTC [orderer/consensus/kafka] processMessagesToBlocks -> ERRO 2b8[0m [channel: comunitychannel] Error during consumption: kafka: error while consuming comunitychannel/0: kafka: client has run out of available brokers to talk to (Is your cluster reachable?)
[33m2019-01-29 11:02:46.399 UTC [orderer/consensus/kafka] processMessagesToBlocks -> WARN 2b9[0m [channel: comunitychannel] Deliver sessions will be dropped if consumption errors continue.
[36m2019-01-29 11:02:48.390 UTC [orderer/consensus/kafka/sarama] dispatcher)-fm -> DEBU 2ba[0m consumer/comunitychannel/0 finding new broker
[36m2019-01-29 11:02:48.394 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2bb[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:48.397 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2bc[0m client/metadata fetching metadata for [comunitychannel] from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 11:02:48.400 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2bd[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:02:48.401 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2be[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:02:48.401 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2bf[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:48.401 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2c0[0m client/metadata fetching metadata for [comunitychannel] from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 11:02:48.407 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2c1[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:02:48.407 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2c2[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:02:48.407 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2c3[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:48.407 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2c4[0m client/metadata fetching metadata for [comunitychannel] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:02:48.409 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2c5[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp 10.0.0.45:9092: connect: connection refused
[36m2019-01-29 11:02:48.409 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2c6[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.45:9092: connect: connection refused
[36m2019-01-29 11:02:48.409 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2c7[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 11:02:48.409 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2c8[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 11:02:48.410 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2c9[0m client/metadata retrying after 250ms... (3 attempts remaining)
[36m2019-01-29 11:02:48.660 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2ca[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:48.663 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2cb[0m client/metadata fetching metadata for [comunitychannel] from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 11:02:48.664 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2cc[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:02:48.664 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2cd[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:02:48.664 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2ce[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:48.665 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2cf[0m client/metadata fetching metadata for [comunitychannel] from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 11:02:48.665 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2d0[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:02:48.666 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2d1[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:02:48.666 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2d2[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:48.666 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2d3[0m client/metadata fetching metadata for [comunitychannel] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:02:48.667 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2d4[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp 10.0.0.45:9092: connect: connection refused
[36m2019-01-29 11:02:48.667 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2d5[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.45:9092: connect: connection refused
[36m2019-01-29 11:02:48.667 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2d6[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 11:02:48.668 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2d7[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 11:02:48.668 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2d8[0m client/metadata retrying after 250ms... (2 attempts remaining)
[36m2019-01-29 11:02:48.918 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2d9[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:48.922 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2da[0m client/metadata fetching metadata for [comunitychannel] from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 11:02:48.928 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2db[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:02:48.933 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2dc[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:02:48.936 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2dd[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:48.937 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2de[0m client/metadata fetching metadata for [comunitychannel] from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 11:02:48.945 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2df[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:02:48.945 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2e0[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:02:48.945 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2e1[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:48.945 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2e2[0m client/metadata fetching metadata for [comunitychannel] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:02:48.950 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2e3[0m Failed to connect to broker kafka0.switch2logic.co.za:9092: dial tcp 10.0.0.45:9092: connect: connection refused
[36m2019-01-29 11:02:48.950 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2e4[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.45:9092: connect: connection refused
[36m2019-01-29 11:02:48.950 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2e5[0m client/metadata no available broker to send metadata request to
[36m2019-01-29 11:02:48.950 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2e6[0m client/brokers resurrecting 3 dead seed brokers
[36m2019-01-29 11:02:48.950 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2e7[0m client/metadata retrying after 250ms... (1 attempts remaining)
[36m2019-01-29 11:02:49.201 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2e8[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:49.207 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2e9[0m client/metadata fetching metadata for [comunitychannel] from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 11:02:49.215 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2ea[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:02:49.215 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2eb[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:02:49.215 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2ec[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:49.215 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2ed[0m client/metadata fetching metadata for [comunitychannel] from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 11:02:49.217 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2ee[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:02:49.217 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2ef[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:02:49.217 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2f0[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:49.217 UTC [orderer/consensus/kafka/sarama] func1 -> DEBU 2f1[0m client/metadata fetching metadata for [comunitychannel] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:02:49.227 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2f2[0m Connected to broker at kafka0.switch2logic.co.za:9092 (unregistered)
[36m2019-01-29 11:02:49.596 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 2f3[0m Unexpected topic-level metadata error: kafka server: Replication-factor is invalid.
[31m2019-01-29 11:02:49.599 UTC [orderer/consensus/kafka] processMessagesToBlocks -> ERRO 2f4[0m [channel: comunitychannel] Error during consumption: kafka: error while consuming comunitychannel/0: kafka server: Replication-factor is invalid.
[33m2019-01-29 11:02:49.606 UTC [orderer/consensus/kafka] processMessagesToBlocks -> WARN 2f5[0m [channel: comunitychannel] Deliver sessions will be dropped if consumption errors continue.
[36m2019-01-29 11:02:51.599 UTC [orderer/consensus/kafka/sarama] dispatcher)-fm -> DEBU 2f6[0m consumer/comunitychannel/0 finding new broker
[36m2019-01-29 11:02:51.599 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 2f7[0m client/metadata fetching metadata for [comunitychannel] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:02:51.625 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 2f8[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 11:02:51.627 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 2f9[0m client/brokers registered new broker #1 at kafka1.switch2logic.co.za:9092
[36m2019-01-29 11:02:51.627 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 2fa[0m client/brokers registered new broker #0 at kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:02:51.627 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 2fb[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:51.629 UTC [orderer/consensus/kafka/sarama] subscriptionConsumer -> DEBU 2fc[0m consumer/broker/2 added subscription to comunitychannel/0
[33m2019-01-29 11:02:51.629 UTC [orderer/consensus/kafka] processMessagesToBlocks -> WARN 2fd[0m [channel: comunitychannel] Consumption will resume.
[36m2019-01-29 11:02:51.630 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 2fe[0m Connected to broker at kafka2.switch2logic.co.za:9092 (registered as #2)
[36m2019-01-29 11:02:53.911 UTC [orderer/consensus/kafka/sarama] subscriptionConsumer -> DEBU 2ff[0m consumer/broker/2 abandoned subscription to comunitychannel/0 because kafka server: Tried to send a message to a replica that is not the leader for some partition. Your metadata is out of date.
[36m2019-01-29 11:02:55.912 UTC [orderer/consensus/kafka/sarama] dispatcher)-fm -> DEBU 300[0m consumer/comunitychannel/0 finding new broker
[36m2019-01-29 11:02:55.912 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 301[0m client/metadata fetching metadata for [comunitychannel] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:02:55.921 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 302[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:02:55.922 UTC [orderer/consensus/kafka/sarama] subscriptionConsumer -> DEBU 303[0m consumer/broker/0 added subscription to comunitychannel/0
[36m2019-01-29 11:02:55.923 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 304[0m Connected to broker at kafka0.switch2logic.co.za:9092 (registered as #0)
[36m2019-01-29 11:05:21.090 UTC [grpc] Printf -> DEBU 305[0m grpc: Server.Serve failed to complete security handshake from "10.0.0.38:53924": tls: first record does not look like a TLS handshake
[36m2019-01-29 11:07:02.173 UTC [orderer/consensus/kafka] try -> DEBU 306[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 11:07:02.174 UTC [orderer/consensus/kafka/sarama] getOffset -> DEBU 307[0m Closed connection to broker kafka1.switch2logic.co.za:9092
[36m2019-01-29 11:07:02.174 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 308[0m client/metadata fetching metadata for [testchainid] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:07:02.175 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 309[0m client/metadata got error from broker while fetching metadata: EOF
[36m2019-01-29 11:07:02.175 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 30a[0m Closed connection to broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:07:02.175 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 30b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:07:02.175 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 30c[0m client/metadata fetching metadata for [testchainid] from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 11:07:02.176 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 30d[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:07:02.176 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 30e[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:07:02.176 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 30f[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:07:02.176 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 310[0m client/metadata fetching metadata for [testchainid] from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 11:07:02.178 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 311[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:07:02.178 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 312[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:07:02.178 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 313[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:07:02.178 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 314[0m client/metadata fetching metadata for [testchainid] from broker kafka1.switch2logic.co.za:9092
[36m2019-01-29 11:07:02.179 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 315[0m Connected to broker at kafka1.switch2logic.co.za:9092 (registered as #1)
[36m2019-01-29 11:07:02.194 UTC [orderer/consensus/kafka] try -> DEBU 316[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 11:08:04.752 UTC [grpc] Printf -> DEBU 317[0m grpc: Server.Serve failed to complete security handshake from "10.0.0.38:53960": tls: first record does not look like a TLS handshake
[36m2019-01-29 11:11:32.118 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 318[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:11:32.119 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 319[0m client/metadata got error from broker while fetching metadata: EOF
[36m2019-01-29 11:11:32.119 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 31a[0m Closed connection to broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:11:32.119 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 31b[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:11:32.119 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 31c[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:9092
[36m2019-01-29 11:11:32.121 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 31d[0m Connected to broker at kafka2.switch2logic.co.za:9092 (registered as #2)
[36m2019-01-29 11:11:32.143 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 31e[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:11:32.143 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 31f[0m client/metadata got error from broker while fetching metadata: EOF
[36m2019-01-29 11:11:32.143 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 320[0m Closed connection to broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:11:32.143 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 321[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:11:32.143 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 322[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:10092
[36m2019-01-29 11:11:32.144 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 323[0m Failed to connect to broker kafka1.switch2logic.co.za:10092: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:11:32.144 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 324[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.44:10092: connect: connection refused
[36m2019-01-29 11:11:32.144 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 325[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:11:32.144 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 326[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:11092
[36m2019-01-29 11:11:32.146 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 327[0m Failed to connect to broker kafka2.switch2logic.co.za:11092: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:11:32.146 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 328[0m client/metadata got error from broker while fetching metadata: dial tcp 10.0.0.43:11092: connect: connection refused
[36m2019-01-29 11:11:32.146 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 329[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:11:32.146 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 32a[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:11:32.147 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 32b[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:9092
[36m2019-01-29 11:11:32.149 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 32c[0m Connected to broker at kafka2.switch2logic.co.za:9092 (registered as #2)
[36m2019-01-29 11:11:32.165 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 32d[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:11:32.165 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 32e[0m client/metadata fetching metadata for all topics from broker kafka2.switch2logic.co.za:9092
[36m2019-01-29 11:11:32.166 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 32f[0m Connected to broker at kafka2.switch2logic.co.za:9092 (registered as #2)
[36m2019-01-29 11:12:02.173 UTC [orderer/consensus/kafka] try -> DEBU 330[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 11:12:02.176 UTC [orderer/consensus/kafka] try -> DEBU 331[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 11:15:30.195 UTC [orderer/common/server] Deliver -> DEBU 332[0m Starting new Deliver handler
[36m2019-01-29 11:15:30.195 UTC [common/deliver] Handle -> DEBU 333[0m Starting new deliver loop for 10.0.0.39:47750
[36m2019-01-29 11:15:30.195 UTC [common/deliver] Handle -> DEBU 334[0m Attempting to read seek info message from 10.0.0.39:47750
[36m2019-01-29 11:15:30.195 UTC [policies] Evaluate -> DEBU 335[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers ==
[36m2019-01-29 11:15:30.195 UTC [policies] Evaluate -> DEBU 336[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:15:30.195 UTC [policies] Evaluate -> DEBU 337[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers ==
[36m2019-01-29 11:15:30.195 UTC [policies] Evaluate -> DEBU 338[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:15:30.195 UTC [policies] Evaluate -> DEBU 339[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers ==
2019-01-29 11:15:30.195 UTC [msp] DeserializeIdentity -> INFO 33a[0m Obtaining identity
[36m2019-01-29 11:15:30.196 UTC [msp/identity] newIdentity -> DEBU 33b[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTDCCAfOgAwIBAgIQYZ1E6CL3TI5r9J0X5CY/6zAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMi5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMi5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB2MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzENMAsGA1UECxME
cGVlcjErMCkGA1UEAxMicGVlcjAuaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5jby56
YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABOoEzz0FFdYNv89IgW1kmLaRhSux
ZZyS6FfMn/s+eLJInhN8XZRcfo5jAL5wHQus9ka5err+7PFzn9+DQlEIb46jTTBL
MA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPECalij
2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQCIESI
6Ro/uHS6tC9ccjZumBNE+ffnKjI2UoZeL3zGRXVXAiBeirbdVLCZ46LhSPayifiC
LJj/EF+z8kPVLYe/SoDSRg==
-----END CERTIFICATE-----
[36m2019-01-29 11:15:30.196 UTC [cauthdsl] func1 -> DEBU 33c[0m 0xc420166060 gate 1548760530196703712 evaluation starts
[36m2019-01-29 11:15:30.196 UTC [cauthdsl] func2 -> DEBU 33d[0m 0xc420166060 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:15:30.196 UTC [cauthdsl] func2 -> DEBU 33e[0m 0xc420166060 processing identity 0 with bytes of 0a0c486f73706974616c324d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435444434341664f674177494241674951595a314536434c3354493572394a30583543592f367a414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777794c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424f6f457a7a30464664594e763839496757316b6d4c6152685375780a5a5a79533646664d6e2f732b654c4a496e684e38585a5263666f356a414c357748517573396b61356572722b3750467a6e392b44516c45496234366a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149504543616c696a0a326c39754279446345557572434452557345512f754f4e503348457037785657686864754d416f4743437147534d343942414d43413063414d455143494553490a36526f2f7548533674433963636a5a756d424e452b66666e4b6a4932556f5a654c337a47525856584169426569726264564c435a34364c6853506179696669430a4c4a6a2f45462b7a386b50564c59652f536f445352673d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:15:30.196 UTC [cauthdsl] func2 -> DEBU 33f[0m 0xc420166060 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital1MSP, got Hospital2MSP)
[36m2019-01-29 11:15:30.196 UTC [cauthdsl] func2 -> DEBU 340[0m 0xc420166060 principal evaluation fails
[36m2019-01-29 11:15:30.196 UTC [cauthdsl] func1 -> DEBU 341[0m 0xc420166060 gate 1548760530196703712 evaluation fails
[36m2019-01-29 11:15:30.196 UTC [policies] Evaluate -> DEBU 342[0m Signature set did not satisfy policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:15:30.196 UTC [policies] Evaluate -> DEBU 343[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:15:30.196 UTC [policies] Evaluate -> DEBU 344[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Readers ==
[36m2019-01-29 11:15:30.197 UTC [cauthdsl] func1 -> DEBU 345[0m 0xc420166080 gate 1548760530197006761 evaluation starts
[36m2019-01-29 11:15:30.197 UTC [cauthdsl] func2 -> DEBU 346[0m 0xc420166080 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:15:30.197 UTC [cauthdsl] func2 -> DEBU 347[0m 0xc420166080 processing identity 0 with bytes of 0a0c486f73706974616c324d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435444434341664f674177494241674951595a314536434c3354493572394a30583543592f367a414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777794c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424f6f457a7a30464664594e763839496757316b6d4c6152685375780a5a5a79533646664d6e2f732b654c4a496e684e38585a5263666f356a414c357748517573396b61356572722b3750467a6e392b44516c45496234366a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149504543616c696a0a326c39754279446345557572434452557345512f754f4e503348457037785657686864754d416f4743437147534d343942414d43413063414d455143494553490a36526f2f7548533674433963636a5a756d424e452b66666e4b6a4932556f5a654c337a47525856584169426569726264564c435a34364c6853506179696669430a4c4a6a2f45462b7a386b50564c59652f536f445352673d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:15:30.197 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 348[0m Checking if identity satisfies MEMBER role for Hospital2MSP
[36m2019-01-29 11:15:30.197 UTC [msp] Validate -> DEBU 349[0m MSP Hospital2MSP validating identity
[36m2019-01-29 11:15:30.197 UTC [msp] getCertificationChain -> DEBU 34a[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 11:15:30.197 UTC [cauthdsl] func2 -> DEBU 34b[0m 0xc420166080 principal matched by identity 0
[36m2019-01-29 11:15:30.197 UTC [msp/identity] Verify -> DEBU 34c[0m Verify: digest = 00000000  76 23 69 11 6d b0 29 7c  82 09 7e a7 b2 2e e7 03  |v#i.m.)|..~.....|
00000010  96 f4 58 a2 6b 4f 33 b6  d4 0b 90 37 8e b6 d8 fe  |..X.kO3....7....|
[36m2019-01-29 11:15:30.197 UTC [msp/identity] Verify -> DEBU 34d[0m Verify: sig = 00000000  30 45 02 21 00 a5 39 97  6f 8a 30 42 89 db 48 e7  |0E.!..9.o.0B..H.|
00000010  40 0d 68 2c cd 2a 4e e0  55 7d 6d e7 d7 27 bf 97  |@.h,.*N.U}m..'..|
00000020  da 8e 53 6e c4 02 20 1d  41 e3 fc eb 8c 2c e2 db  |..Sn.. .A....,..|
00000030  15 ab a6 e7 bd e2 08 fa  0c 25 c1 03 e7 1e 60 3e  |.........%....`>|
00000040  a5 b1 85 5f 45 1e ea                              |..._E..|
[36m2019-01-29 11:15:30.198 UTC [cauthdsl] func2 -> DEBU 34e[0m 0xc420166080 principal evaluation succeeds for identity 0
[36m2019-01-29 11:15:30.198 UTC [cauthdsl] func1 -> DEBU 34f[0m 0xc420166080 gate 1548760530197006761 evaluation succeeds
[36m2019-01-29 11:15:30.198 UTC [policies] Evaluate -> DEBU 350[0m Signature set satisfies policy /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:15:30.198 UTC [policies] Evaluate -> DEBU 351[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:15:30.198 UTC [policies] Evaluate -> DEBU 352[0m Signature set satisfies policy /Channel/Application/Readers
[36m2019-01-29 11:15:30.198 UTC [policies] Evaluate -> DEBU 353[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers
[36m2019-01-29 11:15:30.198 UTC [policies] Evaluate -> DEBU 354[0m Signature set satisfies policy /Channel/Readers
[36m2019-01-29 11:15:30.198 UTC [policies] Evaluate -> DEBU 355[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers
[36m2019-01-29 11:15:30.198 UTC [common/deliver] deliverBlocks -> DEBU 356[0m [channel: comunitychannel] Received seekInfo (0xc42028ed00) start:<specified:<number:1 > > stop:<specified:<number:18446744073709551615 > >  from 10.0.0.39:47750
[36m2019-01-29 11:15:30.198 UTC [fsblkstorage] waitForBlock -> DEBU 357[0m Going to wait for newer blocks. maxAvailaBlockNumber=[0], waitForBlockNum=[1]
[36m2019-01-29 11:17:02.173 UTC [orderer/consensus/kafka] try -> DEBU 358[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 11:17:02.175 UTC [orderer/consensus/kafka] try -> DEBU 359[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 11:17:20.359 UTC [orderer/common/server] Deliver -> DEBU 35a[0m Starting new Deliver handler
[36m2019-01-29 11:17:20.360 UTC [common/deliver] Handle -> DEBU 35b[0m Starting new deliver loop for 10.0.0.18:59908
[36m2019-01-29 11:17:20.360 UTC [common/deliver] Handle -> DEBU 35c[0m Attempting to read seek info message from 10.0.0.18:59908
[36m2019-01-29 11:17:20.361 UTC [policies] Evaluate -> DEBU 35d[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers ==
[36m2019-01-29 11:17:20.361 UTC [policies] Evaluate -> DEBU 35e[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:17:20.361 UTC [policies] Evaluate -> DEBU 35f[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers ==
[36m2019-01-29 11:17:20.361 UTC [policies] Evaluate -> DEBU 360[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:17:20.361 UTC [policies] Evaluate -> DEBU 361[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers ==
2019-01-29 11:17:20.361 UTC [msp] DeserializeIdentity -> INFO 362[0m Obtaining identity
[36m2019-01-29 11:17:20.362 UTC [msp/identity] newIdentity -> DEBU 363[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTjCCAfSgAwIBAgIRAKAnE4PbDWMeRl0DixdLiVswCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowdjELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDTALBgNVBAsT
BHBlZXIxKzApBgNVBAMTInBlZXIwLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28u
emEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAASAvWW2l9XjwfiMiZ6Pt8Qrk81E
DaIfG8hRpCV7KHzrHmpiy3y88PlgzQhh3sQ5iYqAqw0Qjz6BrEbhedGqpv6Po00w
SzAOBgNVHQ8BAf8EBAMCB4AwDAYDVR0TAQH/BAIwADArBgNVHSMEJDAigCA7vVWF
XWpgZBojQ1ZLxUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEA
2E0537skl5yXkVKSjrjr0UJC3GRPT8czRln3F2NB2ygCIApG3olic8fvYAJspMQH
92FzZlNWQ3BWDB4d6dq3Ae+/
-----END CERTIFICATE-----
[36m2019-01-29 11:17:20.362 UTC [cauthdsl] func1 -> DEBU 364[0m 0xc420166030 gate 1548760640362833211 evaluation starts
[36m2019-01-29 11:17:20.362 UTC [cauthdsl] func2 -> DEBU 365[0m 0xc420166030 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:17:20.362 UTC [cauthdsl] func2 -> DEBU 366[0m 0xc420166030 processing identity 0 with bytes of 0a0c486f73706974616c334d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943546a4343416653674177494241674952414b416e4534506244574d65526c30446978644c69567377436759494b6f5a497a6a304541774977675973780a437a414a42674e5642415954416c56544d524d77455159445651514945777044595778705a6d3979626d6c684d525977464159445651514845773154595734670a526e4a68626d4e7063324e764d535577497759445651514b4578786f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70680a4d5367774a675944565151444578396a5953356f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70684d423458445445350a4d4445794f5441334e4445314e466f58445449354d4445794e6a41334e4445314e466f77646a454c4d416b474131554542684d4356564d78457a415242674e560a42416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e6862694247636d467559326c7a593238784454414c42674e56424173540a4248426c5a5849784b7a417042674e5642414d54496e426c5a5849774c6d687663334270644746734d79357a64326c30593267796247396e61574d75593238750a656d45775754415442676371686b6a4f5051494242676771686b6a4f50514d4242774e4341415341765757326c39586a7766694d695a3650743851726b3831450a4461496647386852704356374b487a72486d70697933793838506c677a5168683373513569597141717730516a7a36427245626865644771707636506f3030770a537a414f42674e56485138424166384542414d434234417744415944565230544151482f424149774144417242674e5648534d454a44416967434137765657460a585770675a426f6a51315a4c78557667424c65696135394d69764952636a786d59616e707954414b42676771686b6a4f5051514441674e4941444246416945410a324530353337736b6c3579586b564b536a726a7230554a43334752505438637a526c6e3346324e423279674349417047336f6c696338667659414a73704d51480a3932467a5a6c4e5751334257444234643664713341652b2f0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:17:20.363 UTC [cauthdsl] func2 -> DEBU 367[0m 0xc420166030 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital1MSP, got Hospital3MSP)
[36m2019-01-29 11:17:20.363 UTC [cauthdsl] func2 -> DEBU 368[0m 0xc420166030 principal evaluation fails
[36m2019-01-29 11:17:20.363 UTC [cauthdsl] func1 -> DEBU 369[0m 0xc420166030 gate 1548760640362833211 evaluation fails
[36m2019-01-29 11:17:20.363 UTC [policies] Evaluate -> DEBU 36a[0m Signature set did not satisfy policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:17:20.363 UTC [policies] Evaluate -> DEBU 36b[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:17:20.363 UTC [policies] Evaluate -> DEBU 36c[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Readers ==
[36m2019-01-29 11:17:20.363 UTC [cauthdsl] func1 -> DEBU 36d[0m 0xc420166080 gate 1548760640363331969 evaluation starts
[36m2019-01-29 11:17:20.363 UTC [cauthdsl] func2 -> DEBU 36e[0m 0xc420166080 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:17:20.363 UTC [cauthdsl] func2 -> DEBU 36f[0m 0xc420166080 processing identity 0 with bytes of 0a0c486f73706974616c334d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943546a4343416653674177494241674952414b416e4534506244574d65526c30446978644c69567377436759494b6f5a497a6a304541774977675973780a437a414a42674e5642415954416c56544d524d77455159445651514945777044595778705a6d3979626d6c684d525977464159445651514845773154595734670a526e4a68626d4e7063324e764d535577497759445651514b4578786f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70680a4d5367774a675944565151444578396a5953356f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70684d423458445445350a4d4445794f5441334e4445314e466f58445449354d4445794e6a41334e4445314e466f77646a454c4d416b474131554542684d4356564d78457a415242674e560a42416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e6862694247636d467559326c7a593238784454414c42674e56424173540a4248426c5a5849784b7a417042674e5642414d54496e426c5a5849774c6d687663334270644746734d79357a64326c30593267796247396e61574d75593238750a656d45775754415442676371686b6a4f5051494242676771686b6a4f50514d4242774e4341415341765757326c39586a7766694d695a3650743851726b3831450a4461496647386852704356374b487a72486d70697933793838506c677a5168683373513569597141717730516a7a36427245626865644771707636506f3030770a537a414f42674e56485138424166384542414d434234417744415944565230544151482f424149774144417242674e5648534d454a44416967434137765657460a585770675a426f6a51315a4c78557667424c65696135394d69764952636a786d59616e707954414b42676771686b6a4f5051514441674e4941444246416945410a324530353337736b6c3579586b564b536a726a7230554a43334752505438637a526c6e3346324e423279674349417047336f6c696338667659414a73704d51480a3932467a5a6c4e5751334257444234643664713341652b2f0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:17:20.363 UTC [cauthdsl] func2 -> DEBU 370[0m 0xc420166080 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital2MSP, got Hospital3MSP)
[36m2019-01-29 11:17:20.363 UTC [cauthdsl] func2 -> DEBU 371[0m 0xc420166080 principal evaluation fails
[36m2019-01-29 11:17:20.363 UTC [cauthdsl] func1 -> DEBU 372[0m 0xc420166080 gate 1548760640363331969 evaluation fails
[36m2019-01-29 11:17:20.363 UTC [policies] Evaluate -> DEBU 373[0m Signature set did not satisfy policy /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:17:20.363 UTC [policies] Evaluate -> DEBU 374[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:17:20.363 UTC [policies] Evaluate -> DEBU 375[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers ==
[36m2019-01-29 11:17:20.363 UTC [cauthdsl] func1 -> DEBU 376[0m 0xc420166098 gate 1548760640363604370 evaluation starts
[36m2019-01-29 11:17:20.363 UTC [cauthdsl] func2 -> DEBU 377[0m 0xc420166098 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:17:20.363 UTC [cauthdsl] func2 -> DEBU 378[0m 0xc420166098 processing identity 0 with bytes of 0a0c486f73706974616c334d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943546a4343416653674177494241674952414b416e4534506244574d65526c30446978644c69567377436759494b6f5a497a6a304541774977675973780a437a414a42674e5642415954416c56544d524d77455159445651514945777044595778705a6d3979626d6c684d525977464159445651514845773154595734670a526e4a68626d4e7063324e764d535577497759445651514b4578786f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70680a4d5367774a675944565151444578396a5953356f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70684d423458445445350a4d4445794f5441334e4445314e466f58445449354d4445794e6a41334e4445314e466f77646a454c4d416b474131554542684d4356564d78457a415242674e560a42416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e6862694247636d467559326c7a593238784454414c42674e56424173540a4248426c5a5849784b7a417042674e5642414d54496e426c5a5849774c6d687663334270644746734d79357a64326c30593267796247396e61574d75593238750a656d45775754415442676371686b6a4f5051494242676771686b6a4f50514d4242774e4341415341765757326c39586a7766694d695a3650743851726b3831450a4461496647386852704356374b487a72486d70697933793838506c677a5168683373513569597141717730516a7a36427245626865644771707636506f3030770a537a414f42674e56485138424166384542414d434234417744415944565230544151482f424149774144417242674e5648534d454a44416967434137765657460a585770675a426f6a51315a4c78557667424c65696135394d69764952636a786d59616e707954414b42676771686b6a4f5051514441674e4941444246416945410a324530353337736b6c3579586b564b536a726a7230554a43334752505438637a526c6e3346324e423279674349417047336f6c696338667659414a73704d51480a3932467a5a6c4e5751334257444234643664713341652b2f0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:17:20.363 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 379[0m Checking if identity satisfies MEMBER role for Hospital3MSP
[36m2019-01-29 11:17:20.363 UTC [msp] Validate -> DEBU 37a[0m MSP Hospital3MSP validating identity
[36m2019-01-29 11:17:20.364 UTC [msp] getCertificationChain -> DEBU 37b[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 11:17:20.364 UTC [cauthdsl] func2 -> DEBU 37c[0m 0xc420166098 principal matched by identity 0
[36m2019-01-29 11:17:20.364 UTC [msp/identity] Verify -> DEBU 37d[0m Verify: digest = 00000000  e8 ed 40 b8 9e af b2 ef  06 7b 8c 10 da 7a e2 8d  |..@......{...z..|
00000010  b6 64 38 ca 51 7f 36 1a  cc a8 45 f2 4d 5d d3 49  |.d8.Q.6...E.M].I|
[36m2019-01-29 11:17:20.364 UTC [msp/identity] Verify -> DEBU 37e[0m Verify: sig = 00000000  30 45 02 21 00 cf 67 37  c5 fb 18 5d e8 8e ff 76  |0E.!..g7...]...v|
00000010  16 89 7b d5 ad fd 0d ea  64 bc fd a0 19 ee 91 80  |..{.....d.......|
00000020  25 3f 3d 8b d3 02 20 40  ee 79 e7 36 16 b4 15 0c  |%?=... @.y.6....|
00000030  be 6e e7 19 98 3a 3a 54  51 5b ec 7e ff ef 8c 43  |.n...::TQ[.~...C|
00000040  98 48 cd f9 75 a5 31                              |.H..u.1|
[36m2019-01-29 11:17:20.364 UTC [cauthdsl] func2 -> DEBU 37f[0m 0xc420166098 principal evaluation succeeds for identity 0
[36m2019-01-29 11:17:20.364 UTC [cauthdsl] func1 -> DEBU 380[0m 0xc420166098 gate 1548760640363604370 evaluation succeeds
[36m2019-01-29 11:17:20.364 UTC [policies] Evaluate -> DEBU 381[0m Signature set satisfies policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:17:20.364 UTC [policies] Evaluate -> DEBU 382[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:17:20.364 UTC [policies] Evaluate -> DEBU 383[0m Signature set satisfies policy /Channel/Application/Readers
[36m2019-01-29 11:17:20.364 UTC [policies] Evaluate -> DEBU 384[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers
[36m2019-01-29 11:17:20.364 UTC [policies] Evaluate -> DEBU 385[0m Signature set satisfies policy /Channel/Readers
[36m2019-01-29 11:17:20.365 UTC [policies] Evaluate -> DEBU 386[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers
[36m2019-01-29 11:17:20.365 UTC [common/deliver] deliverBlocks -> DEBU 387[0m [channel: comunitychannel] Received seekInfo (0xc42024a5a0) start:<specified:<number:1 > > stop:<specified:<number:18446744073709551615 > >  from 10.0.0.18:59908
[36m2019-01-29 11:17:20.365 UTC [fsblkstorage] waitForBlock -> DEBU 388[0m Going to wait for newer blocks. maxAvailaBlockNumber=[0], waitForBlockNum=[1]
[36m2019-01-29 11:21:21.943 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 389[0m [channel: comunitychannel] Successfully unmarshalled consumed message, offset is 5. Inspecting type...
[36m2019-01-29 11:21:21.943 UTC [orderer/consensus/kafka] processRegular -> DEBU 38a[0m [channel: comunitychannel] Processing regular Kafka message of type CONFIG
[36m2019-01-29 11:21:21.943 UTC [orderer/consensus/kafka] func2 -> DEBU 38b[0m [channel: comunitychannel] Received config message
[36m2019-01-29 11:21:21.943 UTC [orderer/consensus/kafka] func2 -> DEBU 38c[0m [channel: comunitychannel] Creating isolated block for config message
[36m2019-01-29 11:21:21.954 UTC [common/configtx] addToMap -> DEBU 38d[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 11:21:21.954 UTC [common/configtx] addToMap -> DEBU 38e[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 11:21:21.954 UTC [common/configtx] addToMap -> DEBU 38f[0m Adding to config map: [Group]  /Channel/Application/Hospital3MSP
[36m2019-01-29 11:21:21.954 UTC [common/configtx] addToMap -> DEBU 390[0m Adding to config map: [Value]  /Channel/Application/Hospital3MSP/MSP
[36m2019-01-29 11:21:21.954 UTC [common/configtx] addToMap -> DEBU 391[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Writers
[36m2019-01-29 11:21:21.954 UTC [common/configtx] addToMap -> DEBU 392[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Admins
[36m2019-01-29 11:21:21.954 UTC [common/configtx] addToMap -> DEBU 393[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:21:21.955 UTC [common/configtx] addToMap -> DEBU 394[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 11:21:21.955 UTC [common/configtx] addToMap -> DEBU 395[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 11:21:21.955 UTC [common/configtx] addToMap -> DEBU 396[0m Adding to config map: [Group]  /Channel/Application/Hospital3MSP
[36m2019-01-29 11:21:21.955 UTC [common/configtx] addToMap -> DEBU 397[0m Adding to config map: [Value]  /Channel/Application/Hospital3MSP/MSP
[36m2019-01-29 11:21:21.955 UTC [common/configtx] addToMap -> DEBU 398[0m Adding to config map: [Value]  /Channel/Application/Hospital3MSP/AnchorPeers
[36m2019-01-29 11:21:21.955 UTC [common/configtx] addToMap -> DEBU 399[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Admins
[36m2019-01-29 11:21:21.955 UTC [common/configtx] addToMap -> DEBU 39a[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:21:21.955 UTC [common/configtx] addToMap -> DEBU 39b[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Writers
[36m2019-01-29 11:21:21.955 UTC [common/configtx] verifyDeltaSet -> DEBU 39c[0m Processing change to key: [Group]  /Channel/Application/Hospital3MSP
[36m2019-01-29 11:21:21.955 UTC [common/configtx] policyForItem -> DEBU 39d[0m Getting policy for item Hospital3MSP with mod_policy Admins
[36m2019-01-29 11:21:21.955 UTC [policies] Manager -> DEBU 39e[0m Manager Channel looking up path [Application]
[36m2019-01-29 11:21:21.955 UTC [policies] Manager -> DEBU 39f[0m Manager Channel has managers Application
[36m2019-01-29 11:21:21.955 UTC [policies] Manager -> DEBU 3a0[0m Manager Channel has managers Orderer
[36m2019-01-29 11:21:21.955 UTC [policies] Manager -> DEBU 3a1[0m Manager Channel/Application looking up path []
[36m2019-01-29 11:21:21.955 UTC [policies] Manager -> DEBU 3a2[0m Manager Channel/Application has managers Hospital1MSP
[36m2019-01-29 11:21:21.955 UTC [policies] Manager -> DEBU 3a3[0m Manager Channel/Application has managers Hospital2MSP
[36m2019-01-29 11:21:21.955 UTC [policies] Manager -> DEBU 3a4[0m Manager Channel/Application has managers Hospital3MSP
[36m2019-01-29 11:21:21.955 UTC [policies] Manager -> DEBU 3a5[0m Manager Channel/Application looking up path [Hospital3MSP]
[36m2019-01-29 11:21:21.955 UTC [policies] Manager -> DEBU 3a6[0m Manager Channel/Application has managers Hospital1MSP
[36m2019-01-29 11:21:21.955 UTC [policies] Manager -> DEBU 3a7[0m Manager Channel/Application has managers Hospital2MSP
[36m2019-01-29 11:21:21.955 UTC [policies] Manager -> DEBU 3a8[0m Manager Channel/Application has managers Hospital3MSP
[36m2019-01-29 11:21:21.955 UTC [policies] Manager -> DEBU 3a9[0m Manager Channel/Application/Hospital3MSP looking up path []
[36m2019-01-29 11:21:21.955 UTC [policies] Evaluate -> DEBU 3aa[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Admins ==
2019-01-29 11:21:21.955 UTC [msp] DeserializeIdentity -> INFO 3ab[0m Obtaining identity
[36m2019-01-29 11:21:21.955 UTC [msp/identity] newIdentity -> DEBU 3ac[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAI8BA/5Jf+5m8ncI+D7aSiMwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwzLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABF1IehIKIjh8tXYZ7rw4QOuc
g/tpBAkJeOt44zr/LepbjhyFW8vIRnkw75i6Wcg+LOoAS+2NFNg2oNjqg//yCHWj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIDu9
VYVdamBkGiNDVkvFS+AEt6Jrn0yK8hFyPGZhqenJMAoGCCqGSM49BAMCA0cAMEQC
IFyc85pwbCAxGNdw4fflsX2I3HrP3h93YAB2zSDC8HtkAiAQxblAZfEnXUiQKrqE
8I10bYJqD6pJz1jb3Pv+4xplTA==
-----END CERTIFICATE-----
[36m2019-01-29 11:21:21.956 UTC [cauthdsl] func1 -> DEBU 3ad[0m 0xc42000e828 gate 1548760881956337847 evaluation starts
[36m2019-01-29 11:21:21.956 UTC [cauthdsl] func2 -> DEBU 3ae[0m 0xc42000e828 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:21:21.956 UTC [cauthdsl] func2 -> DEBU 3af[0m 0xc42000e828 processing identity 0 with bytes of 0a0c486f73706974616c334d535012df062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943547a434341666167417749424167495241493842412f354a662b356d386e63492b44376153694d77436759494b6f5a497a6a304541774977675973780a437a414a42674e5642415954416c56544d524d77455159445651514945777044595778705a6d3979626d6c684d525977464159445651514845773154595734670a526e4a68626d4e7063324e764d535577497759445651514b4578786f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70680a4d5367774a675944565151444578396a5953356f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70684d423458445445350a4d4445794f5441334e4445314e466f58445449354d4445794e6a41334e4445314e466f776544454c4d416b474131554542684d4356564d78457a415242674e560a42416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e6862694247636d467559326c7a59323878447a414e42674e56424173540a426d4e7361575675644445724d436b47413155454177776951575274615735416147397a63476c305957777a4c6e4e336158526a61444a73623264705979356a0a627935365954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424631496568494b496a68387458595a37727734514f75630a672f747042416b4a654f7434347a722f4c6570626a68794657387649526e6b77373569365763672b4c4f6f41532b324e464e67326f4e6a71672f2f794348576a0a5454424c4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b41494475390a56595664616d426b47694e44566b7646532b414574364a726e30794b3868467950475a6871656e4a4d416f4743437147534d343942414d43413063414d4551430a494679633835707762434178474e64773466666c735832493348725033683933594142327a5344433848746b4169415178626c415a66456e585569514b7271450a3849313062594a714436704a7a316a623350762b3478706c54413d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:21:21.956 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 3b0[0m Checking if identity satisfies ADMIN role for Hospital3MSP
[36m2019-01-29 11:21:21.956 UTC [cauthdsl] func2 -> DEBU 3b1[0m 0xc42000e828 principal matched by identity 0
[36m2019-01-29 11:21:21.956 UTC [msp/identity] Verify -> DEBU 3b2[0m Verify: digest = 00000000  5a 4b e6 c6 6d 21 aa c7  ba 9e 82 2a bd 7f 29 b1  |ZK..m!.....*..).|
00000010  0b 0c 11 d7 f3 10 8b 76  2b d5 e6 46 9c 28 fa 43  |.......v+..F.(.C|
[36m2019-01-29 11:21:21.956 UTC [msp/identity] Verify -> DEBU 3b3[0m Verify: sig = 00000000  30 44 02 20 73 ea 38 17  94 6e c4 65 bd 26 45 ea  |0D. s.8..n.e.&E.|
00000010  e1 8c f8 ca 26 92 17 2d  af 7c f8 cf 73 88 e3 94  |....&..-.|..s...|
00000020  eb 32 a9 94 02 20 37 13  e7 ef 73 80 f9 6d 8e 2e  |.2... 7...s..m..|
00000030  69 7c e5 a4 bf e8 39 f2  19 54 fa 6e c4 41 46 78  |i|....9..T.n.AFx|
00000040  0e de 00 86 5a 5b                                 |....Z[|
[36m2019-01-29 11:21:21.956 UTC [cauthdsl] func2 -> DEBU 3b4[0m 0xc42000e828 principal evaluation succeeds for identity 0
[36m2019-01-29 11:21:21.956 UTC [cauthdsl] func1 -> DEBU 3b5[0m 0xc42000e828 gate 1548760881956337847 evaluation succeeds
[36m2019-01-29 11:21:21.956 UTC [policies] Evaluate -> DEBU 3b6[0m Signature set satisfies policy /Channel/Application/Hospital3MSP/Admins
[36m2019-01-29 11:21:21.956 UTC [policies] Evaluate -> DEBU 3b7[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Admins
[36m2019-01-29 11:21:21.956 UTC [common/configtx] verifyDeltaSet -> DEBU 3b8[0m Processing change to key: [Value]  /Channel/Application/Hospital3MSP/AnchorPeers
[36m2019-01-29 11:21:21.957 UTC [common/configtx] recurseConfigMap -> DEBU 3b9[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital1MSP" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.957 UTC [common/configtx] recurseConfigMap -> DEBU 3ba[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital1MSP" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.957 UTC [common/configtx] recurseConfigMap -> DEBU 3bb[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\022\022\020\n\014Hospital1MSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.957 UTC [common/configtx] recurseConfigMap -> DEBU 3bc[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital2MSP" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.957 UTC [common/configtx] recurseConfigMap -> DEBU 3bd[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital2MSP" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.957 UTC [common/configtx] recurseConfigMap -> DEBU 3be[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\022\022\020\n\014Hospital2MSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.957 UTC [common/configtx] recurseConfigMap -> DEBU 3bf[0m Setting policy for key Writers to 
[36m2019-01-29 11:21:21.957 UTC [common/configtx] recurseConfigMap -> DEBU 3c0[0m Setting policy for key Admins to 
[36m2019-01-29 11:21:21.957 UTC [common/configtx] recurseConfigMap -> DEBU 3c1[0m Setting policy for key Readers to 
[36m2019-01-29 11:21:21.957 UTC [common/configtx] recurseConfigMap -> DEBU 3c2[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.957 UTC [common/configtx] recurseConfigMap -> DEBU 3c3[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.957 UTC [common/configtx] recurseConfigMap -> DEBU 3c4[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.958 UTC [common/configtx] recurseConfigMap -> DEBU 3c5[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\016\022\014\n\nOrdererMSP" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.958 UTC [common/configtx] recurseConfigMap -> DEBU 3c6[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\016\022\014\n\nOrdererMSP" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.958 UTC [common/configtx] recurseConfigMap -> DEBU 3c7[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\nOrdererMSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.958 UTC [common/configtx] recurseConfigMap -> DEBU 3c8[0m Setting policy for key BlockValidation to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.958 UTC [common/configtx] recurseConfigMap -> DEBU 3c9[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.958 UTC [common/configtx] recurseConfigMap -> DEBU 3ca[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.958 UTC [common/configtx] recurseConfigMap -> DEBU 3cb[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.958 UTC [common/configtx] recurseConfigMap -> DEBU 3cc[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.958 UTC [common/configtx] recurseConfigMap -> DEBU 3cd[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.958 UTC [common/configtx] recurseConfigMap -> DEBU 3ce[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 11:21:21.958 UTC [common/channelconfig] NewStandardValues -> DEBU 3cf[0m Initializing protos for *channelconfig.ChannelProtos
[36m2019-01-29 11:21:21.958 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3d0[0m Processing field: HashingAlgorithm
[36m2019-01-29 11:21:21.958 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3d1[0m Processing field: BlockDataHashingStructure
[36m2019-01-29 11:21:21.958 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3d2[0m Processing field: OrdererAddresses
[36m2019-01-29 11:21:21.958 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3d3[0m Processing field: Consortium
[36m2019-01-29 11:21:21.958 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3d4[0m Processing field: Capabilities
[36m2019-01-29 11:21:21.958 UTC [common/channelconfig] NewStandardValues -> DEBU 3d5[0m Initializing protos for *channelconfig.ApplicationProtos
[36m2019-01-29 11:21:21.958 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3d6[0m Processing field: ACLs
[36m2019-01-29 11:21:21.958 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3d7[0m Processing field: Capabilities
[36m2019-01-29 11:21:21.958 UTC [common/channelconfig] NewStandardValues -> DEBU 3d8[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 11:21:21.958 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3d9[0m Processing field: AnchorPeers
[36m2019-01-29 11:21:21.958 UTC [common/channelconfig] NewStandardValues -> DEBU 3da[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 11:21:21.958 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3db[0m Processing field: MSP
[36m2019-01-29 11:21:21.958 UTC [common/channelconfig] Validate -> DEBU 3dc[0m Anchor peers for org Hospital3MSP are anchor_peers:<host:"peer0.hospital3.switch2logic.co.za" port:7051 > 
[36m2019-01-29 11:21:21.958 UTC [common/channelconfig] validateMSP -> DEBU 3dd[0m Setting up MSP for org Hospital3MSP
[36m2019-01-29 11:21:21.958 UTC [msp] newBccspMsp -> DEBU 3de[0m Creating BCCSP-based MSP instance
[36m2019-01-29 11:21:21.959 UTC [msp] New -> DEBU 3df[0m Creating Cache-MSP instance
[36m2019-01-29 11:21:21.959 UTC [msp] Setup -> DEBU 3e0[0m Setting up MSP instance Hospital3MSP
[36m2019-01-29 11:21:21.959 UTC [msp/identity] newIdentity -> DEBU 3e1[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdTCCAhugAwIBAgIQCCXeHGgObIJo8aq4AdvgVjAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMy5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQm1Oo/AEu7ZZToE15Y5ntlaN128Hs/exX3jNH5nO6r4tvyFtBBNEn1ecXxxPtm
dmQ+nPHONorLwDC42SMAKwfro18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCA7vVWFXWpgZBojQ1ZL
xUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEAlOUoPtpfO8V7
vFRCanjU5zZWGO5ajFlPDJjjDTy3UrwCICrUYV32JrR/m3dslCl15dSThHrKtYCC
Ehvyw1+lNqiG
-----END CERTIFICATE-----
[36m2019-01-29 11:21:21.960 UTC [msp/identity] newIdentity -> DEBU 3e2[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAI8BA/5Jf+5m8ncI+D7aSiMwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwzLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABF1IehIKIjh8tXYZ7rw4QOuc
g/tpBAkJeOt44zr/LepbjhyFW8vIRnkw75i6Wcg+LOoAS+2NFNg2oNjqg//yCHWj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIDu9
VYVdamBkGiNDVkvFS+AEt6Jrn0yK8hFyPGZhqenJMAoGCCqGSM49BAMCA0cAMEQC
IFyc85pwbCAxGNdw4fflsX2I3HrP3h93YAB2zSDC8HtkAiAQxblAZfEnXUiQKrqE
8I10bYJqD6pJz1jb3Pv+4xplTA==
-----END CERTIFICATE-----
[36m2019-01-29 11:21:21.961 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 3e3[0m Checking if identity satisfies role [CLIENT] for Hospital3MSP
[36m2019-01-29 11:21:21.961 UTC [msp] Validate -> DEBU 3e4[0m MSP Hospital3MSP validating identity
[36m2019-01-29 11:21:21.961 UTC [msp] getCertificationChain -> DEBU 3e5[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 11:21:21.962 UTC [msp] hasOURole -> DEBU 3e6[0m MSP Hospital3MSP checking if the identity is a client
[36m2019-01-29 11:21:21.962 UTC [msp] getCertificationChain -> DEBU 3e7[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 11:21:21.962 UTC [common/channelconfig] NewStandardValues -> DEBU 3e8[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 11:21:21.962 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3e9[0m Processing field: AnchorPeers
[36m2019-01-29 11:21:21.962 UTC [common/channelconfig] NewStandardValues -> DEBU 3ea[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 11:21:21.962 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3eb[0m Processing field: MSP
[36m2019-01-29 11:21:21.962 UTC [common/channelconfig] Validate -> DEBU 3ec[0m Anchor peers for org Hospital1MSP are 
[36m2019-01-29 11:21:21.962 UTC [common/channelconfig] validateMSP -> DEBU 3ed[0m Setting up MSP for org Hospital1MSP
[36m2019-01-29 11:21:21.962 UTC [msp] newBccspMsp -> DEBU 3ee[0m Creating BCCSP-based MSP instance
[36m2019-01-29 11:21:21.963 UTC [msp] New -> DEBU 3ef[0m Creating Cache-MSP instance
[36m2019-01-29 11:21:21.963 UTC [msp] Setup -> DEBU 3f0[0m Setting up MSP instance Hospital1MSP
[36m2019-01-29 11:21:21.966 UTC [msp/identity] newIdentity -> DEBU 3f1[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdDCCAhugAwIBAgIQZxH8VsnceZxXgKxlURLgVTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMS5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQX9V4k/h/3pFxByKDscjgQrC0YS/bSbKvzSG2Z2PvltEWp40qB9OaKUN0GQYzo
yK2Etdvyjfd1OFwaTjpGSvv9o18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCBVU7DoP9Msf/nNTNl6
5FxwAXZ82eaGBT7zWYdyh4QbBjAKBggqhkjOPQQDAgNHADBEAiBmXGWQbxCGRp0c
5/y9qJyciB1CDHsknRZDjNzt5f563wIgMy7VPi1R8DpIIIRN9B3mfhDpRa+J+8PA
S4DOJy7rjp4=
-----END CERTIFICATE-----
[36m2019-01-29 11:21:21.966 UTC [msp/identity] newIdentity -> DEBU 3f2[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 11:21:21.967 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 3f3[0m Checking if identity satisfies role [CLIENT] for Hospital1MSP
[36m2019-01-29 11:21:21.968 UTC [msp] Validate -> DEBU 3f4[0m MSP Hospital1MSP validating identity
[36m2019-01-29 11:21:21.968 UTC [msp] getCertificationChain -> DEBU 3f5[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 11:21:21.968 UTC [msp] hasOURole -> DEBU 3f6[0m MSP Hospital1MSP checking if the identity is a client
[36m2019-01-29 11:21:21.968 UTC [msp] getCertificationChain -> DEBU 3f7[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 11:21:21.969 UTC [common/channelconfig] NewStandardValues -> DEBU 3f8[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 11:21:21.969 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3f9[0m Processing field: AnchorPeers
[36m2019-01-29 11:21:21.969 UTC [common/channelconfig] NewStandardValues -> DEBU 3fa[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 11:21:21.969 UTC [common/channelconfig] initializeProtosStruct -> DEBU 3fb[0m Processing field: MSP
[36m2019-01-29 11:21:21.969 UTC [common/channelconfig] Validate -> DEBU 3fc[0m Anchor peers for org Hospital2MSP are 
[36m2019-01-29 11:21:21.969 UTC [common/channelconfig] validateMSP -> DEBU 3fd[0m Setting up MSP for org Hospital2MSP
[36m2019-01-29 11:21:21.969 UTC [msp] newBccspMsp -> DEBU 3fe[0m Creating BCCSP-based MSP instance
[36m2019-01-29 11:21:21.969 UTC [msp] New -> DEBU 3ff[0m Creating Cache-MSP instance
[36m2019-01-29 11:21:21.969 UTC [msp] Setup -> DEBU 400[0m Setting up MSP instance Hospital2MSP
[36m2019-01-29 11:21:21.970 UTC [msp/identity] newIdentity -> DEBU 401[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdjCCAhygAwIBAgIRALZ3Pffn/xWUtUWPSK6CYVAwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowgYsxCzAJBgNVBAYTAlVTMRMwEQYD
VQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4gRnJhbmNpc2NvMSUwIwYDVQQK
Exxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMSgwJgYDVQQDEx9jYS5ob3Nw
aXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcD
QgAESzVpiz6P4mIb+KwvrttcXJKZR5i137Is4298E7OrkYdAg0tB4rpfKTXzUMwW
vI12cTDujpyaZSiqQ3/zP4fOQqNfMF0wDgYDVR0PAQH/BAQDAgGmMA8GA1UdJQQI
MAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQg8QJqWKPaX24HINwR
S6sINFSwRD+440/ccSnvFVaGF24wCgYIKoZIzj0EAwIDSAAwRQIhAPO2Nng3/nqb
cKPliUqVPVjBMu+D55M90+jexWSOpkjcAiADPx8qv9exZ4BfP3+G49Qzpx0Meblj
53vzUgYFa/0EXw==
-----END CERTIFICATE-----
[36m2019-01-29 11:21:21.970 UTC [msp/identity] newIdentity -> DEBU 402[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAJ6XtHMTTW1JzdpD54qCc3wwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA+z1rb9aA1HwHfNLi9V9VZe
bdwT4MXak445zGz4DXzRUsmNeBngzntKb6qpgHQpeFtzbzLx+zwiynRDvfdzORGj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPEC
alij2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQC
IA8A3CCJeCasyy7gYd/M1aHYmwWsecY7UY7cVUGy4XOxAiB0rrmfnGW6JA11PDCu
+5c5Rpn0jF5ddsFn+UIpzBFvmg==
-----END CERTIFICATE-----
[36m2019-01-29 11:21:21.971 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 403[0m Checking if identity satisfies role [CLIENT] for Hospital2MSP
[36m2019-01-29 11:21:21.971 UTC [msp] Validate -> DEBU 404[0m MSP Hospital2MSP validating identity
[36m2019-01-29 11:21:21.972 UTC [msp] getCertificationChain -> DEBU 405[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 11:21:21.972 UTC [msp] hasOURole -> DEBU 406[0m MSP Hospital2MSP checking if the identity is a client
[36m2019-01-29 11:21:21.972 UTC [msp] getCertificationChain -> DEBU 407[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 11:21:21.973 UTC [common/channelconfig] NewStandardValues -> DEBU 408[0m Initializing protos for *channelconfig.OrdererProtos
[36m2019-01-29 11:21:21.981 UTC [common/channelconfig] initializeProtosStruct -> DEBU 409[0m Processing field: ConsensusType
[36m2019-01-29 11:21:21.981 UTC [common/channelconfig] initializeProtosStruct -> DEBU 40a[0m Processing field: BatchSize
[36m2019-01-29 11:21:21.981 UTC [common/channelconfig] initializeProtosStruct -> DEBU 40b[0m Processing field: BatchTimeout
[36m2019-01-29 11:21:21.981 UTC [common/channelconfig] initializeProtosStruct -> DEBU 40c[0m Processing field: KafkaBrokers
[36m2019-01-29 11:21:21.981 UTC [common/channelconfig] initializeProtosStruct -> DEBU 40d[0m Processing field: ChannelRestrictions
[36m2019-01-29 11:21:21.981 UTC [common/channelconfig] initializeProtosStruct -> DEBU 40e[0m Processing field: Capabilities
[36m2019-01-29 11:21:21.981 UTC [common/channelconfig] NewStandardValues -> DEBU 40f[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 11:21:21.981 UTC [common/channelconfig] initializeProtosStruct -> DEBU 410[0m Processing field: MSP
[36m2019-01-29 11:21:21.981 UTC [common/channelconfig] validateMSP -> DEBU 411[0m Setting up MSP for org OrdererOrg
[36m2019-01-29 11:21:21.981 UTC [msp] newBccspMsp -> DEBU 412[0m Creating BCCSP-based MSP instance
[36m2019-01-29 11:21:21.982 UTC [msp] New -> DEBU 413[0m Creating Cache-MSP instance
[36m2019-01-29 11:21:21.982 UTC [msp] Setup -> DEBU 414[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 11:21:21.982 UTC [msp/identity] newIdentity -> DEBU 415[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 11:21:21.982 UTC [msp/identity] newIdentity -> DEBU 416[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 11:21:21.983 UTC [msp] Validate -> DEBU 417[0m MSP OrdererMSP validating identity
[36m2019-01-29 11:21:21.983 UTC [msp] Setup -> DEBU 418[0m Setting up the MSP manager (4 msps)
[36m2019-01-29 11:21:21.983 UTC [msp] Setup -> DEBU 419[0m MSP manager setup complete, setup 4 msps
[36m2019-01-29 11:21:21.983 UTC [policies] NewManagerImpl -> DEBU 41a[0m Proposed new policy Admins for Channel/Application/Hospital1MSP
[36m2019-01-29 11:21:21.983 UTC [policies] NewManagerImpl -> DEBU 41b[0m Proposed new policy Readers for Channel/Application/Hospital1MSP
[36m2019-01-29 11:21:21.983 UTC [policies] NewManagerImpl -> DEBU 41c[0m Proposed new policy Writers for Channel/Application/Hospital1MSP
[36m2019-01-29 11:21:21.983 UTC [policies] NewManagerImpl -> DEBU 41d[0m Proposed new policy Writers for Channel/Application/Hospital2MSP
[36m2019-01-29 11:21:21.983 UTC [policies] NewManagerImpl -> DEBU 41e[0m Proposed new policy Admins for Channel/Application/Hospital2MSP
[36m2019-01-29 11:21:21.983 UTC [policies] NewManagerImpl -> DEBU 41f[0m Proposed new policy Readers for Channel/Application/Hospital2MSP
[36m2019-01-29 11:21:21.983 UTC [policies] NewManagerImpl -> DEBU 420[0m Proposed new policy Writers for Channel/Application/Hospital3MSP
[36m2019-01-29 11:21:21.984 UTC [policies] NewManagerImpl -> DEBU 421[0m Proposed new policy Admins for Channel/Application/Hospital3MSP
[36m2019-01-29 11:21:21.984 UTC [policies] NewManagerImpl -> DEBU 422[0m Proposed new policy Readers for Channel/Application/Hospital3MSP
[36m2019-01-29 11:21:21.984 UTC [policies] NewManagerImpl -> DEBU 423[0m Proposed new policy Writers for Channel/Application
[36m2019-01-29 11:21:21.984 UTC [policies] NewManagerImpl -> DEBU 424[0m Proposed new policy Admins for Channel/Application
[36m2019-01-29 11:21:21.984 UTC [policies] NewManagerImpl -> DEBU 425[0m Proposed new policy Readers for Channel/Application
[36m2019-01-29 11:21:21.984 UTC [policies] NewManagerImpl -> DEBU 426[0m Proposed new policy Readers for Channel/Orderer/OrdererOrg
[36m2019-01-29 11:21:21.984 UTC [policies] NewManagerImpl -> DEBU 427[0m Proposed new policy Writers for Channel/Orderer/OrdererOrg
[36m2019-01-29 11:21:21.984 UTC [policies] NewManagerImpl -> DEBU 428[0m Proposed new policy Admins for Channel/Orderer/OrdererOrg
[36m2019-01-29 11:21:21.984 UTC [policies] NewManagerImpl -> DEBU 429[0m Proposed new policy BlockValidation for Channel/Orderer
[36m2019-01-29 11:21:21.984 UTC [policies] NewManagerImpl -> DEBU 42a[0m Proposed new policy Admins for Channel/Orderer
[36m2019-01-29 11:21:21.984 UTC [policies] NewManagerImpl -> DEBU 42b[0m Proposed new policy Readers for Channel/Orderer
[36m2019-01-29 11:21:21.984 UTC [policies] NewManagerImpl -> DEBU 42c[0m Proposed new policy Writers for Channel/Orderer
[36m2019-01-29 11:21:21.984 UTC [policies] NewManagerImpl -> DEBU 42d[0m Proposed new policy Readers for Channel
[36m2019-01-29 11:21:21.984 UTC [policies] NewManagerImpl -> DEBU 42e[0m Proposed new policy Writers for Channel
[36m2019-01-29 11:21:21.984 UTC [policies] NewManagerImpl -> DEBU 42f[0m Proposed new policy Admins for Channel
[36m2019-01-29 11:21:21.984 UTC [common/configtx] addToMap -> DEBU 430[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 11:21:21.984 UTC [common/configtx] addToMap -> DEBU 431[0m Adding to config map: [Group]  /Channel/Orderer
[36m2019-01-29 11:21:21.984 UTC [common/configtx] addToMap -> DEBU 432[0m Adding to config map: [Group]  /Channel/Orderer/OrdererOrg
[36m2019-01-29 11:21:21.985 UTC [common/configtx] addToMap -> DEBU 433[0m Adding to config map: [Value]  /Channel/Orderer/OrdererOrg/MSP
[36m2019-01-29 11:21:21.985 UTC [common/configtx] addToMap -> DEBU 434[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 11:21:21.985 UTC [common/configtx] addToMap -> DEBU 435[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 11:21:21.985 UTC [common/configtx] addToMap -> DEBU 436[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Admins
[36m2019-01-29 11:21:21.985 UTC [common/configtx] addToMap -> DEBU 437[0m Adding to config map: [Value]  /Channel/Orderer/BatchSize
[36m2019-01-29 11:21:21.985 UTC [common/configtx] addToMap -> DEBU 438[0m Adding to config map: [Value]  /Channel/Orderer/BatchTimeout
[36m2019-01-29 11:21:21.985 UTC [common/configtx] addToMap -> DEBU 439[0m Adding to config map: [Value]  /Channel/Orderer/ChannelRestrictions
[36m2019-01-29 11:21:21.985 UTC [common/configtx] addToMap -> DEBU 43a[0m Adding to config map: [Value]  /Channel/Orderer/Capabilities
[36m2019-01-29 11:21:21.985 UTC [common/configtx] addToMap -> DEBU 43b[0m Adding to config map: [Value]  /Channel/Orderer/KafkaBrokers
[36m2019-01-29 11:21:21.985 UTC [common/configtx] addToMap -> DEBU 43c[0m Adding to config map: [Value]  /Channel/Orderer/ConsensusType
[36m2019-01-29 11:21:21.985 UTC [common/configtx] addToMap -> DEBU 43d[0m Adding to config map: [Policy] /Channel/Orderer/BlockValidation
[36m2019-01-29 11:21:21.985 UTC [common/configtx] addToMap -> DEBU 43e[0m Adding to config map: [Policy] /Channel/Orderer/Admins
[36m2019-01-29 11:21:21.985 UTC [common/configtx] addToMap -> DEBU 43f[0m Adding to config map: [Policy] /Channel/Orderer/Readers
[36m2019-01-29 11:21:21.985 UTC [common/configtx] addToMap -> DEBU 440[0m Adding to config map: [Policy] /Channel/Orderer/Writers
[36m2019-01-29 11:21:21.986 UTC [common/configtx] addToMap -> DEBU 441[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 11:21:21.986 UTC [common/configtx] addToMap -> DEBU 442[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 11:21:21.986 UTC [common/configtx] addToMap -> DEBU 443[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/MSP
[36m2019-01-29 11:21:21.986 UTC [common/configtx] addToMap -> DEBU 444[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:21:21.986 UTC [common/configtx] addToMap -> DEBU 445[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 11:21:21.986 UTC [common/configtx] addToMap -> DEBU 446[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 11:21:21.986 UTC [common/configtx] addToMap -> DEBU 447[0m Adding to config map: [Group]  /Channel/Application/Hospital2MSP
[36m2019-01-29 11:21:21.986 UTC [common/configtx] addToMap -> DEBU 448[0m Adding to config map: [Value]  /Channel/Application/Hospital2MSP/MSP
[36m2019-01-29 11:21:21.986 UTC [common/configtx] addToMap -> DEBU 449[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Writers
[36m2019-01-29 11:21:21.986 UTC [common/configtx] addToMap -> DEBU 44a[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Admins
[36m2019-01-29 11:21:21.986 UTC [common/configtx] addToMap -> DEBU 44b[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:21:21.986 UTC [common/configtx] addToMap -> DEBU 44c[0m Adding to config map: [Group]  /Channel/Application/Hospital3MSP
[36m2019-01-29 11:21:21.986 UTC [common/configtx] addToMap -> DEBU 44d[0m Adding to config map: [Value]  /Channel/Application/Hospital3MSP/AnchorPeers
[36m2019-01-29 11:21:21.986 UTC [common/configtx] addToMap -> DEBU 44e[0m Adding to config map: [Value]  /Channel/Application/Hospital3MSP/MSP
[36m2019-01-29 11:21:21.986 UTC [common/configtx] addToMap -> DEBU 44f[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Writers
[36m2019-01-29 11:21:21.986 UTC [common/configtx] addToMap -> DEBU 450[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Admins
[36m2019-01-29 11:21:21.987 UTC [common/configtx] addToMap -> DEBU 451[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:21:21.987 UTC [common/configtx] addToMap -> DEBU 452[0m Adding to config map: [Value]  /Channel/Application/Capabilities
[36m2019-01-29 11:21:21.987 UTC [common/configtx] addToMap -> DEBU 453[0m Adding to config map: [Policy] /Channel/Application/Readers
[36m2019-01-29 11:21:21.987 UTC [common/configtx] addToMap -> DEBU 454[0m Adding to config map: [Policy] /Channel/Application/Writers
[36m2019-01-29 11:21:21.987 UTC [common/configtx] addToMap -> DEBU 455[0m Adding to config map: [Policy] /Channel/Application/Admins
[36m2019-01-29 11:21:21.987 UTC [common/configtx] addToMap -> DEBU 456[0m Adding to config map: [Value]  /Channel/HashingAlgorithm
[36m2019-01-29 11:21:21.987 UTC [common/configtx] addToMap -> DEBU 457[0m Adding to config map: [Value]  /Channel/BlockDataHashingStructure
[36m2019-01-29 11:21:21.987 UTC [common/configtx] addToMap -> DEBU 458[0m Adding to config map: [Value]  /Channel/OrdererAddresses
[36m2019-01-29 11:21:21.987 UTC [common/configtx] addToMap -> DEBU 459[0m Adding to config map: [Value]  /Channel/Capabilities
[36m2019-01-29 11:21:21.987 UTC [common/configtx] addToMap -> DEBU 45a[0m Adding to config map: [Value]  /Channel/Consortium
[36m2019-01-29 11:21:21.987 UTC [common/configtx] addToMap -> DEBU 45b[0m Adding to config map: [Policy] /Channel/Readers
[36m2019-01-29 11:21:21.987 UTC [common/configtx] addToMap -> DEBU 45c[0m Adding to config map: [Policy] /Channel/Writers
[36m2019-01-29 11:21:21.987 UTC [common/configtx] addToMap -> DEBU 45d[0m Adding to config map: [Policy] /Channel/Admins
[36m2019-01-29 11:21:21.987 UTC [common/channelconfig] LogSanityChecks -> DEBU 45e[0m As expected, current configuration has policy '/Channel/Readers'
[36m2019-01-29 11:21:21.987 UTC [common/channelconfig] LogSanityChecks -> DEBU 45f[0m As expected, current configuration has policy '/Channel/Writers'
[36m2019-01-29 11:21:21.987 UTC [policies] Manager -> DEBU 460[0m Manager Channel looking up path [Application]
[36m2019-01-29 11:21:21.987 UTC [policies] Manager -> DEBU 461[0m Manager Channel has managers Application
[36m2019-01-29 11:21:21.987 UTC [policies] Manager -> DEBU 462[0m Manager Channel has managers Orderer
[36m2019-01-29 11:21:21.987 UTC [policies] Manager -> DEBU 463[0m Manager Channel/Application looking up path []
[36m2019-01-29 11:21:21.987 UTC [policies] Manager -> DEBU 464[0m Manager Channel/Application has managers Hospital1MSP
[36m2019-01-29 11:21:21.987 UTC [policies] Manager -> DEBU 465[0m Manager Channel/Application has managers Hospital2MSP
[36m2019-01-29 11:21:21.987 UTC [policies] Manager -> DEBU 466[0m Manager Channel/Application has managers Hospital3MSP
[36m2019-01-29 11:21:21.987 UTC [common/channelconfig] LogSanityChecks -> DEBU 467[0m As expected, current configuration has policy '/Channel/Application/Readers'
[36m2019-01-29 11:21:21.987 UTC [common/channelconfig] LogSanityChecks -> DEBU 468[0m As expected, current configuration has policy '/Channel/Application/Writers'
[36m2019-01-29 11:21:21.987 UTC [common/channelconfig] LogSanityChecks -> DEBU 469[0m As expected, current configuration has policy '/Channel/Application/Admins'
[36m2019-01-29 11:21:21.987 UTC [policies] Manager -> DEBU 46a[0m Manager Channel looking up path [Orderer]
[36m2019-01-29 11:21:21.987 UTC [policies] Manager -> DEBU 46b[0m Manager Channel has managers Application
[36m2019-01-29 11:21:21.987 UTC [policies] Manager -> DEBU 46c[0m Manager Channel has managers Orderer
[36m2019-01-29 11:21:21.987 UTC [policies] Manager -> DEBU 46d[0m Manager Channel/Orderer looking up path []
[36m2019-01-29 11:21:21.987 UTC [policies] Manager -> DEBU 46e[0m Manager Channel/Orderer has managers OrdererOrg
[36m2019-01-29 11:21:21.987 UTC [common/channelconfig] LogSanityChecks -> DEBU 46f[0m As expected, current configuration has policy '/Channel/Orderer/BlockValidation'
[36m2019-01-29 11:21:21.987 UTC [common/capabilities] Supported -> DEBU 470[0m Orderer capability V1_1 is supported and is enabled
[36m2019-01-29 11:21:21.988 UTC [common/capabilities] Supported -> DEBU 471[0m Channel capability V1_1 is supported and is enabled
[36m2019-01-29 11:21:21.988 UTC [msp] GetDefaultSigningIdentity -> DEBU 472[0m Obtaining default signing identity
[36m2019-01-29 11:21:21.988 UTC [msp] GetDefaultSigningIdentity -> DEBU 473[0m Obtaining default signing identity
[36m2019-01-29 11:21:21.988 UTC [msp/identity] Sign -> DEBU 474[0m Sign: plaintext: 0ABD060A0A4F7264657265724D535012...2A82DF99490F6297F1B81F05E12A880C 
[36m2019-01-29 11:21:21.988 UTC [msp/identity] Sign -> DEBU 475[0m Sign: digest: C4BF5E99CCAE6B5E7E41309FE1E46A35937E6CB4597819724BFCB3D16DA9AFD8 
[36m2019-01-29 11:21:21.988 UTC [orderer/commmon/multichannel] addLastConfigSignature -> DEBU 476[0m [channel: comunitychannel] Detected lastConfigSeq transitioning from 1 to 2, setting lastConfigBlockNum from 0 to 1
[36m2019-01-29 11:21:21.988 UTC [msp] GetDefaultSigningIdentity -> DEBU 477[0m Obtaining default signing identity
[36m2019-01-29 11:21:21.988 UTC [orderer/commmon/multichannel] addLastConfigSignature -> DEBU 478[0m [channel: comunitychannel] About to write block, setting its LAST_CONFIG to 1
[36m2019-01-29 11:21:21.988 UTC [msp] GetDefaultSigningIdentity -> DEBU 479[0m Obtaining default signing identity
[36m2019-01-29 11:21:21.988 UTC [msp/identity] Sign -> DEBU 47a[0m Sign: plaintext: 08010ABD060A0A4F7264657265724D53...2A82DF99490F6297F1B81F05E12A880C 
[36m2019-01-29 11:21:21.988 UTC [msp/identity] Sign -> DEBU 47b[0m Sign: digest: 676C9C1482EDB48E5A9FD1802B6AF1A678C63F0B6859C33A8BBADF4A97480340 
[36m2019-01-29 11:21:21.991 UTC [fsblkstorage] indexBlock -> DEBU 47c[0m Indexing block [blockNum=1, blockHash=[]byte{0x87, 0x8c, 0x57, 0x6, 0xf3, 0x2d, 0x72, 0xb7, 0x61, 0xc1, 0xd1, 0x44, 0x49, 0x9d, 0x88, 0x8a, 0xaa, 0x34, 0x55, 0x9d, 0x1d, 0xc1, 0x2d, 0x27, 0xb8, 0x2b, 0xe, 0x3d, 0x15, 0x1b, 0x33, 0x92} txOffsets=
txId= locPointer=offset=71, bytesLength=21681
]
[36m2019-01-29 11:21:21.993 UTC [fsblkstorage] updateCheckpoint -> DEBU 47d[0m Broadcasting about update checkpointInfo: latestFileChunkSuffixNum=[0], latestFileChunksize=[45385], isChainEmpty=[false], lastBlockNumber=[1]
[36m2019-01-29 11:21:21.993 UTC [orderer/commmon/multichannel] commitBlock -> DEBU 47e[0m [channel: comunitychannel] Wrote block 1
[36m2019-01-29 11:21:21.993 UTC [fsblkstorage] waitForBlock -> DEBU 47f[0m Came out of wait. maxAvailaBlockNumber=[1]
[36m2019-01-29 11:21:21.993 UTC [fsblkstorage] Next -> DEBU 480[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=1
[36m2019-01-29 11:21:21.993 UTC [fsblkstorage] newBlockfileStream -> DEBU 481[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/comunitychannel/blockfile_000000], startOffset=[21745]
[36m2019-01-29 11:21:21.993 UTC [fsblkstorage] waitForBlock -> DEBU 482[0m Came out of wait. maxAvailaBlockNumber=[1]
[36m2019-01-29 11:21:21.993 UTC [fsblkstorage] Next -> DEBU 483[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=1
[36m2019-01-29 11:21:21.993 UTC [fsblkstorage] newBlockfileStream -> DEBU 484[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/comunitychannel/blockfile_000000], startOffset=[21745]
[36m2019-01-29 11:21:21.993 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 485[0m Remaining bytes=[23640], Going to peek [8] bytes
[36m2019-01-29 11:21:21.993 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 486[0m Returning blockbytes - length=[23637], placementInfo={fileNum=[0], startOffset=[21745], bytesOffset=[21748]}
[36m2019-01-29 11:21:21.993 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 488[0m blockbytes [23637] read from file [0]
[36m2019-01-29 11:21:21.993 UTC [policies] Evaluate -> DEBU 48a[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers ==
[36m2019-01-29 11:21:21.994 UTC [policies] Evaluate -> DEBU 48b[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:21:21.994 UTC [policies] Evaluate -> DEBU 48c[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Orderer/Readers ==
[36m2019-01-29 11:21:21.994 UTC [policies] Evaluate -> DEBU 48d[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:21:21.994 UTC [policies] Evaluate -> DEBU 48e[0m == Evaluating *cauthdsl.policy Policy /Channel/Orderer/OrdererOrg/Readers ==
2019-01-29 11:21:21.994 UTC [msp] DeserializeIdentity -> INFO 48f[0m Obtaining identity
[36m2019-01-29 11:21:21.993 UTC [fsblkstorage] waitForBlock -> DEBU 489[0m Came out of wait. maxAvailaBlockNumber=[1]
[36m2019-01-29 11:21:21.994 UTC [fsblkstorage] Next -> DEBU 490[0m Initializing block stream for iterator. itr.maxBlockNumAvailable=1
[36m2019-01-29 11:21:21.994 UTC [fsblkstorage] newBlockfileStream -> DEBU 491[0m newBlockfileStream(): filePath=[/var/hyperledger/production/orderer/chains/comunitychannel/blockfile_000000], startOffset=[21745]
[36m2019-01-29 11:21:21.994 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 492[0m Remaining bytes=[23640], Going to peek [8] bytes
[36m2019-01-29 11:21:21.994 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 493[0m Returning blockbytes - length=[23637], placementInfo={fileNum=[0], startOffset=[21745], bytesOffset=[21748]}
[36m2019-01-29 11:21:21.994 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 494[0m blockbytes [23637] read from file [0]
[36m2019-01-29 11:21:21.994 UTC [msp/identity] newIdentity -> DEBU 495[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTjCCAfSgAwIBAgIRAKAnE4PbDWMeRl0DixdLiVswCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowdjELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDTALBgNVBAsT
BHBlZXIxKzApBgNVBAMTInBlZXIwLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28u
emEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAASAvWW2l9XjwfiMiZ6Pt8Qrk81E
DaIfG8hRpCV7KHzrHmpiy3y88PlgzQhh3sQ5iYqAqw0Qjz6BrEbhedGqpv6Po00w
SzAOBgNVHQ8BAf8EBAMCB4AwDAYDVR0TAQH/BAIwADArBgNVHSMEJDAigCA7vVWF
XWpgZBojQ1ZLxUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEA
2E0537skl5yXkVKSjrjr0UJC3GRPT8czRln3F2NB2ygCIApG3olic8fvYAJspMQH
92FzZlNWQ3BWDB4d6dq3Ae+/
-----END CERTIFICATE-----
[36m2019-01-29 11:21:21.994 UTC [policies] Evaluate -> DEBU 496[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers ==
[36m2019-01-29 11:21:21.994 UTC [policies] Evaluate -> DEBU 497[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:21:21.994 UTC [policies] Evaluate -> DEBU 498[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Orderer/Readers ==
[36m2019-01-29 11:21:21.994 UTC [policies] Evaluate -> DEBU 499[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:21:21.994 UTC [policies] Evaluate -> DEBU 49a[0m == Evaluating *cauthdsl.policy Policy /Channel/Orderer/OrdererOrg/Readers ==
2019-01-29 11:21:21.994 UTC [msp] DeserializeIdentity -> INFO 49b[0m Obtaining identity
[36m2019-01-29 11:21:21.994 UTC [cauthdsl] func1 -> DEBU 49c[0m 0xc420166078 gate 1548760881994946159 evaluation starts
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func2 -> DEBU 49d[0m 0xc420166078 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func2 -> DEBU 49e[0m 0xc420166078 processing identity 0 with bytes of 0a0c486f73706974616c334d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943546a4343416653674177494241674952414b416e4534506244574d65526c30446978644c69567377436759494b6f5a497a6a304541774977675973780a437a414a42674e5642415954416c56544d524d77455159445651514945777044595778705a6d3979626d6c684d525977464159445651514845773154595734670a526e4a68626d4e7063324e764d535577497759445651514b4578786f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70680a4d5367774a675944565151444578396a5953356f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70684d423458445445350a4d4445794f5441334e4445314e466f58445449354d4445794e6a41334e4445314e466f77646a454c4d416b474131554542684d4356564d78457a415242674e560a42416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e6862694247636d467559326c7a593238784454414c42674e56424173540a4248426c5a5849784b7a417042674e5642414d54496e426c5a5849774c6d687663334270644746734d79357a64326c30593267796247396e61574d75593238750a656d45775754415442676371686b6a4f5051494242676771686b6a4f50514d4242774e4341415341765757326c39586a7766694d695a3650743851726b3831450a4461496647386852704356374b487a72486d70697933793838506c677a5168683373513569597141717730516a7a36427245626865644771707636506f3030770a537a414f42674e56485138424166384542414d434234417744415944565230544151482f424149774144417242674e5648534d454a44416967434137765657460a585770675a426f6a51315a4c78557667424c65696135394d69764952636a786d59616e707954414b42676771686b6a4f5051514441674e4941444246416945410a324530353337736b6c3579586b564b536a726a7230554a43334752505438637a526c6e3346324e423279674349417047336f6c696338667659414a73704d51480a3932467a5a6c4e5751334257444234643664713341652b2f0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:21:21.995 UTC [msp/identity] newIdentity -> DEBU 49f[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTDCCAfOgAwIBAgIQYZ1E6CL3TI5r9J0X5CY/6zAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMi5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMi5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB2MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzENMAsGA1UECxME
cGVlcjErMCkGA1UEAxMicGVlcjAuaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5jby56
YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABOoEzz0FFdYNv89IgW1kmLaRhSux
ZZyS6FfMn/s+eLJInhN8XZRcfo5jAL5wHQus9ka5err+7PFzn9+DQlEIb46jTTBL
MA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPECalij
2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQCIESI
6Ro/uHS6tC9ccjZumBNE+ffnKjI2UoZeL3zGRXVXAiBeirbdVLCZ46LhSPayifiC
LJj/EF+z8kPVLYe/SoDSRg==
-----END CERTIFICATE-----
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func2 -> DEBU 4a0[0m 0xc420166078 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected OrdererMSP, got Hospital3MSP)
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func2 -> DEBU 4a1[0m 0xc420166078 principal evaluation fails
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func1 -> DEBU 4a2[0m 0xc420166078 gate 1548760881994946159 evaluation fails
[36m2019-01-29 11:21:21.995 UTC [policies] Evaluate -> DEBU 4a3[0m Signature set did not satisfy policy /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 11:21:21.995 UTC [policies] Evaluate -> DEBU 4a4[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 11:21:21.995 UTC [policies] func1 -> DEBU 4a5[0m Evaluation Failed: Only 0 policies were satisfied, but needed 1 of [ OrdererOrg.Readers ]
[36m2019-01-29 11:21:21.995 UTC [policies] Evaluate -> DEBU 4a6[0m Signature set did not satisfy policy /Channel/Orderer/Readers
[36m2019-01-29 11:21:21.995 UTC [policies] Evaluate -> DEBU 4a7[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Orderer/Readers
[36m2019-01-29 11:21:21.995 UTC [policies] Evaluate -> DEBU 4a8[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers ==
[36m2019-01-29 11:21:21.995 UTC [policies] Evaluate -> DEBU 4a9[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:21:21.995 UTC [policies] Evaluate -> DEBU 4aa[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers ==
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func1 -> DEBU 4ab[0m 0xc420166098 gate 1548760881995301598 evaluation starts
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func2 -> DEBU 4ac[0m 0xc420166098 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func2 -> DEBU 4ad[0m 0xc420166098 processing identity 0 with bytes of 0a0c486f73706974616c334d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943546a4343416653674177494241674952414b416e4534506244574d65526c30446978644c69567377436759494b6f5a497a6a304541774977675973780a437a414a42674e5642415954416c56544d524d77455159445651514945777044595778705a6d3979626d6c684d525977464159445651514845773154595734670a526e4a68626d4e7063324e764d535577497759445651514b4578786f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70680a4d5367774a675944565151444578396a5953356f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70684d423458445445350a4d4445794f5441334e4445314e466f58445449354d4445794e6a41334e4445314e466f77646a454c4d416b474131554542684d4356564d78457a415242674e560a42416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e6862694247636d467559326c7a593238784454414c42674e56424173540a4248426c5a5849784b7a417042674e5642414d54496e426c5a5849774c6d687663334270644746734d79357a64326c30593267796247396e61574d75593238750a656d45775754415442676371686b6a4f5051494242676771686b6a4f50514d4242774e4341415341765757326c39586a7766694d695a3650743851726b3831450a4461496647386852704356374b487a72486d70697933793838506c677a5168683373513569597141717730516a7a36427245626865644771707636506f3030770a537a414f42674e56485138424166384542414d434234417744415944565230544151482f424149774144417242674e5648534d454a44416967434137765657460a585770675a426f6a51315a4c78557667424c65696135394d69764952636a786d59616e707954414b42676771686b6a4f5051514441674e4941444246416945410a324530353337736b6c3579586b564b536a726a7230554a43334752505438637a526c6e3346324e423279674349417047336f6c696338667659414a73704d51480a3932467a5a6c4e5751334257444234643664713341652b2f0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:21:21.995 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 4ae[0m Checking if identity satisfies MEMBER role for Hospital3MSP
[36m2019-01-29 11:21:21.995 UTC [msp] Validate -> DEBU 4af[0m MSP Hospital3MSP validating identity
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func1 -> DEBU 4b0[0m 0xc42000efc0 gate 1548760881995433290 evaluation starts
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func2 -> DEBU 4b1[0m 0xc42000efc0 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func2 -> DEBU 4b2[0m 0xc42000efc0 processing identity 0 with bytes of 0a0c486f73706974616c324d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435444434341664f674177494241674951595a314536434c3354493572394a30583543592f367a414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777794c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424f6f457a7a30464664594e763839496757316b6d4c6152685375780a5a5a79533646664d6e2f732b654c4a496e684e38585a5263666f356a414c357748517573396b61356572722b3750467a6e392b44516c45496234366a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149504543616c696a0a326c39754279446345557572434452557345512f754f4e503348457037785657686864754d416f4743437147534d343942414d43413063414d455143494553490a36526f2f7548533674433963636a5a756d424e452b66666e4b6a4932556f5a654c337a47525856584169426569726264564c435a34364c6853506179696669430a4c4a6a2f45462b7a386b50564c59652f536f445352673d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func2 -> DEBU 4b3[0m 0xc42000efc0 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected OrdererMSP, got Hospital2MSP)
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func2 -> DEBU 4b4[0m 0xc42000efc0 principal evaluation fails
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func1 -> DEBU 4b5[0m 0xc42000efc0 gate 1548760881995433290 evaluation fails
[36m2019-01-29 11:21:21.995 UTC [policies] Evaluate -> DEBU 4b6[0m Signature set did not satisfy policy /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 11:21:21.995 UTC [policies] Evaluate -> DEBU 4b7[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 11:21:21.995 UTC [policies] func1 -> DEBU 4b8[0m Evaluation Failed: Only 0 policies were satisfied, but needed 1 of [ OrdererOrg.Readers ]
[36m2019-01-29 11:21:21.995 UTC [policies] Evaluate -> DEBU 4b9[0m Signature set did not satisfy policy /Channel/Orderer/Readers
[36m2019-01-29 11:21:21.995 UTC [policies] Evaluate -> DEBU 4ba[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Orderer/Readers
[36m2019-01-29 11:21:21.995 UTC [policies] Evaluate -> DEBU 4bb[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers ==
[36m2019-01-29 11:21:21.995 UTC [policies] Evaluate -> DEBU 4bc[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:21:21.995 UTC [policies] Evaluate -> DEBU 4bd[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers ==
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func1 -> DEBU 4be[0m 0xc42000efd8 gate 1548760881995730827 evaluation starts
[36m2019-01-29 11:21:21.995 UTC [msp] getCertificationChain -> DEBU 4bf[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func2 -> DEBU 4c0[0m 0xc42000efd8 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func2 -> DEBU 4c1[0m 0xc42000efd8 processing identity 0 with bytes of 0a0c486f73706974616c324d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435444434341664f674177494241674951595a314536434c3354493572394a30583543592f367a414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777794c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424f6f457a7a30464664594e763839496757316b6d4c6152685375780a5a5a79533646664d6e2f732b654c4a496e684e38585a5263666f356a414c357748517573396b61356572722b3750467a6e392b44516c45496234366a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149504543616c696a0a326c39754279446345557572434452557345512f754f4e503348457037785657686864754d416f4743437147534d343942414d43413063414d455143494553490a36526f2f7548533674433963636a5a756d424e452b66666e4b6a4932556f5a654c337a47525856584169426569726264564c435a34364c6853506179696669430a4c4a6a2f45462b7a386b50564c59652f536f445352673d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func2 -> DEBU 4c2[0m 0xc42000efd8 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital3MSP, got Hospital2MSP)
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func2 -> DEBU 4c3[0m 0xc42000efd8 principal evaluation fails
[36m2019-01-29 11:21:21.995 UTC [cauthdsl] func1 -> DEBU 4c4[0m 0xc42000efd8 gate 1548760881995730827 evaluation fails
[36m2019-01-29 11:21:21.995 UTC [policies] Evaluate -> DEBU 4c5[0m Signature set did not satisfy policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:21:21.996 UTC [cauthdsl] func2 -> DEBU 4c7[0m 0xc420166098 principal matched by identity 0
[36m2019-01-29 11:21:21.996 UTC [msp/identity] Verify -> DEBU 4c8[0m Verify: digest = 00000000  e8 ed 40 b8 9e af b2 ef  06 7b 8c 10 da 7a e2 8d  |..@......{...z..|
00000010  b6 64 38 ca 51 7f 36 1a  cc a8 45 f2 4d 5d d3 49  |.d8.Q.6...E.M].I|
[36m2019-01-29 11:21:21.996 UTC [msp/identity] Verify -> DEBU 4c9[0m Verify: sig = 00000000  30 45 02 21 00 cf 67 37  c5 fb 18 5d e8 8e ff 76  |0E.!..g7...]...v|
00000010  16 89 7b d5 ad fd 0d ea  64 bc fd a0 19 ee 91 80  |..{.....d.......|
00000020  25 3f 3d 8b d3 02 20 40  ee 79 e7 36 16 b4 15 0c  |%?=... @.y.6....|
00000030  be 6e e7 19 98 3a 3a 54  51 5b ec 7e ff ef 8c 43  |.n...::TQ[.~...C|
00000040  98 48 cd f9 75 a5 31                              |.H..u.1|
[36m2019-01-29 11:21:21.996 UTC [cauthdsl] func2 -> DEBU 4ca[0m 0xc420166098 principal evaluation succeeds for identity 0
[36m2019-01-29 11:21:21.996 UTC [cauthdsl] func1 -> DEBU 4cb[0m 0xc420166098 gate 1548760881995301598 evaluation succeeds
[36m2019-01-29 11:21:21.996 UTC [policies] Evaluate -> DEBU 4cc[0m Signature set satisfies policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:21:21.996 UTC [policies] Evaluate -> DEBU 4cd[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:21:21.996 UTC [policies] Evaluate -> DEBU 4ce[0m Signature set satisfies policy /Channel/Application/Readers
[36m2019-01-29 11:21:21.996 UTC [policies] Evaluate -> DEBU 4cf[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers
[36m2019-01-29 11:21:21.996 UTC [policies] Evaluate -> DEBU 4d0[0m Signature set satisfies policy /Channel/Readers
[36m2019-01-29 11:21:21.996 UTC [policies] Evaluate -> DEBU 4d1[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers
[36m2019-01-29 11:21:21.996 UTC [common/deliver] deliverBlocks -> DEBU 4d2[0m [channel: comunitychannel] Delivering block for (0xc42024a5a0) for 10.0.0.18:59908
[36m2019-01-29 11:21:21.996 UTC [fsblkstorage] waitForBlock -> DEBU 4d3[0m Going to wait for newer blocks. maxAvailaBlockNumber=[1], waitForBlockNum=[2]
[36m2019-01-29 11:21:21.993 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 487[0m Remaining bytes=[23640], Going to peek [8] bytes
[36m2019-01-29 11:21:21.995 UTC [policies] Evaluate -> DEBU 4c6[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:21:21.996 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 4d4[0m Returning blockbytes - length=[23637], placementInfo={fileNum=[0], startOffset=[21745], bytesOffset=[21748]}
[36m2019-01-29 11:21:21.996 UTC [policies] Evaluate -> DEBU 4d5[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers ==
[36m2019-01-29 11:21:21.997 UTC [cauthdsl] func1 -> DEBU 4d7[0m 0xc42000efe8 gate 1548760881997555359 evaluation starts
[36m2019-01-29 11:21:21.997 UTC [cauthdsl] func2 -> DEBU 4d8[0m 0xc42000efe8 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:21:21.997 UTC [cauthdsl] func2 -> DEBU 4d9[0m 0xc42000efe8 processing identity 0 with bytes of 0a0c486f73706974616c324d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435444434341664f674177494241674951595a314536434c3354493572394a30583543592f367a414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777794c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424f6f457a7a30464664594e763839496757316b6d4c6152685375780a5a5a79533646664d6e2f732b654c4a496e684e38585a5263666f356a414c357748517573396b61356572722b3750467a6e392b44516c45496234366a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149504543616c696a0a326c39754279446345557572434452557345512f754f4e503348457037785657686864754d416f4743437147534d343942414d43413063414d455143494553490a36526f2f7548533674433963636a5a756d424e452b66666e4b6a4932556f5a654c337a47525856584169426569726264564c435a34364c6853506179696669430a4c4a6a2f45462b7a386b50564c59652f536f445352673d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:21:21.998 UTC [cauthdsl] func2 -> DEBU 4da[0m 0xc42000efe8 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital1MSP, got Hospital2MSP)
[36m2019-01-29 11:21:21.998 UTC [cauthdsl] func2 -> DEBU 4db[0m 0xc42000efe8 principal evaluation fails
[36m2019-01-29 11:21:21.998 UTC [cauthdsl] func1 -> DEBU 4dc[0m 0xc42000efe8 gate 1548760881997555359 evaluation fails
[36m2019-01-29 11:21:21.998 UTC [policies] Evaluate -> DEBU 4dd[0m Signature set did not satisfy policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:21:21.998 UTC [policies] Evaluate -> DEBU 4de[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:21:21.998 UTC [policies] Evaluate -> DEBU 4df[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Readers ==
[36m2019-01-29 11:21:21.998 UTC [cauthdsl] func1 -> DEBU 4e0[0m 0xc42000eff8 gate 1548760881998595633 evaluation starts
[36m2019-01-29 11:21:21.996 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 4d6[0m blockbytes [23637] read from file [0]
[36m2019-01-29 11:21:21.999 UTC [policies] Evaluate -> DEBU 4e2[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers ==
[36m2019-01-29 11:21:21.999 UTC [policies] Evaluate -> DEBU 4e3[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:21:21.999 UTC [policies] Evaluate -> DEBU 4e4[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Orderer/Readers ==
[36m2019-01-29 11:21:21.999 UTC [policies] Evaluate -> DEBU 4e5[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:21:21.999 UTC [policies] Evaluate -> DEBU 4e6[0m == Evaluating *cauthdsl.policy Policy /Channel/Orderer/OrdererOrg/Readers ==
2019-01-29 11:21:21.999 UTC [msp] DeserializeIdentity -> INFO 4e7[0m Obtaining identity
[36m2019-01-29 11:21:21.999 UTC [msp/identity] newIdentity -> DEBU 4e8[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTTCCAfOgAwIBAgIQDPIhkZbBMzAPRV2mdTJ2BTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB2MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzENMAsGA1UECxME
cGVlcjErMCkGA1UEAxMicGVlcjAuaG9zcGl0YWwxLnN3aXRjaDJsb2dpYy5jby56
YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABMHSfENe+q4SvhxLb6FxLr2SLYg+
qJnL3whOrSDaEyQgmDz7fwbLhWr0Mapiq8N3Xg1vudI6XbNZxyMTBmZbd3ujTTBL
MA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIFVTsOg/
0yx/+c1M2XrkXHABdnzZ5oYFPvNZh3KHhBsGMAoGCCqGSM49BAMCA0gAMEUCIQDn
gu8tRu0NvsaTNG17i4QtjRftqg56ptadF5YHXBQCGgIgeowGfoCWCIW5T09110Gv
cu4tVgSe4rBzE4yjal6cQVQ=
-----END CERTIFICATE-----
[36m2019-01-29 11:21:22.000 UTC [cauthdsl] func1 -> DEBU 4e9[0m 0xc420166140 gate 1548760882000374990 evaluation starts
[36m2019-01-29 11:21:22.000 UTC [cauthdsl] func2 -> DEBU 4ea[0m 0xc420166140 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:21:22.000 UTC [cauthdsl] func2 -> DEBU 4eb[0m 0xc420166140 processing identity 0 with bytes of 0a0c486f73706974616c314d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435454434341664f674177494241674951445049686b5a62424d7a41505256326d64544a324254414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777784c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424d485366454e652b7134537668784c623646784c7232534c59672b0a714a6e4c3377684f72534461457951676d447a376677624c685772304d61706971384e33586731767564493658624e5a78794d54426d5a626433756a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149465654734f672f0a3079782f2b63314d3258726b58484142646e7a5a356f594650764e5a68334b48684273474d416f4743437147534d343942414d43413067414d4555434951446e0a677538745275304e767361544e473137693451746a5266747167353670746164463559485842514347674967656f7747666f43574349573554303931313047760a63753474566753653472427a4534796a616c36635156513d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:21:22.000 UTC [cauthdsl] func2 -> DEBU 4ec[0m 0xc420166140 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected OrdererMSP, got Hospital1MSP)
[36m2019-01-29 11:21:22.000 UTC [cauthdsl] func2 -> DEBU 4ed[0m 0xc420166140 principal evaluation fails
[36m2019-01-29 11:21:22.000 UTC [cauthdsl] func1 -> DEBU 4ee[0m 0xc420166140 gate 1548760882000374990 evaluation fails
[36m2019-01-29 11:21:22.000 UTC [policies] Evaluate -> DEBU 4ef[0m Signature set did not satisfy policy /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 11:21:22.000 UTC [policies] Evaluate -> DEBU 4f0[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 11:21:22.000 UTC [policies] func1 -> DEBU 4f1[0m Evaluation Failed: Only 0 policies were satisfied, but needed 1 of [ OrdererOrg.Readers ]
[36m2019-01-29 11:21:22.000 UTC [policies] Evaluate -> DEBU 4f2[0m Signature set did not satisfy policy /Channel/Orderer/Readers
[36m2019-01-29 11:21:22.000 UTC [policies] Evaluate -> DEBU 4f3[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Orderer/Readers
[36m2019-01-29 11:21:22.000 UTC [policies] Evaluate -> DEBU 4f4[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers ==
[36m2019-01-29 11:21:22.000 UTC [policies] Evaluate -> DEBU 4f5[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:21:22.000 UTC [policies] Evaluate -> DEBU 4f6[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers ==
[36m2019-01-29 11:21:22.000 UTC [cauthdsl] func1 -> DEBU 4f7[0m 0xc420166180 gate 1548760882000715096 evaluation starts
[36m2019-01-29 11:21:22.000 UTC [cauthdsl] func2 -> DEBU 4f8[0m 0xc420166180 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:21:22.000 UTC [cauthdsl] func2 -> DEBU 4f9[0m 0xc420166180 processing identity 0 with bytes of 0a0c486f73706974616c314d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435454434341664f674177494241674951445049686b5a62424d7a41505256326d64544a324254414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777784c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424d485366454e652b7134537668784c623646784c7232534c59672b0a714a6e4c3377684f72534461457951676d447a376677624c685772304d61706971384e33586731767564493658624e5a78794d54426d5a626433756a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149465654734f672f0a3079782f2b63314d3258726b58484142646e7a5a356f594650764e5a68334b48684273474d416f4743437147534d343942414d43413067414d4555434951446e0a677538745275304e767361544e473137693451746a5266747167353670746164463559485842514347674967656f7747666f43574349573554303931313047760a63753474566753653472427a4534796a616c36635156513d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:21:22.000 UTC [cauthdsl] func2 -> DEBU 4fa[0m 0xc420166180 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital3MSP, got Hospital1MSP)
[36m2019-01-29 11:21:22.000 UTC [cauthdsl] func2 -> DEBU 4fb[0m 0xc420166180 principal evaluation fails
[36m2019-01-29 11:21:22.000 UTC [cauthdsl] func1 -> DEBU 4fc[0m 0xc420166180 gate 1548760882000715096 evaluation fails
[36m2019-01-29 11:21:22.000 UTC [policies] Evaluate -> DEBU 4fd[0m Signature set did not satisfy policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:21:22.000 UTC [policies] Evaluate -> DEBU 4fe[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:21:22.000 UTC [policies] Evaluate -> DEBU 4ff[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers ==
[36m2019-01-29 11:21:22.000 UTC [cauthdsl] func1 -> DEBU 500[0m 0xc420166190 gate 1548760882000919207 evaluation starts
[36m2019-01-29 11:21:22.000 UTC [cauthdsl] func2 -> DEBU 501[0m 0xc420166190 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:21:22.000 UTC [cauthdsl] func2 -> DEBU 502[0m 0xc420166190 processing identity 0 with bytes of 0a0c486f73706974616c314d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435454434341664f674177494241674951445049686b5a62424d7a41505256326d64544a324254414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777784c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424d485366454e652b7134537668784c623646784c7232534c59672b0a714a6e4c3377684f72534461457951676d447a376677624c685772304d61706971384e33586731767564493658624e5a78794d54426d5a626433756a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149465654734f672f0a3079782f2b63314d3258726b58484142646e7a5a356f594650764e5a68334b48684273474d416f4743437147534d343942414d43413067414d4555434951446e0a677538745275304e767361544e473137693451746a5266747167353670746164463559485842514347674967656f7747666f43574349573554303931313047760a63753474566753653472427a4534796a616c36635156513d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:21:22.001 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 503[0m Checking if identity satisfies MEMBER role for Hospital1MSP
[36m2019-01-29 11:21:22.001 UTC [msp] Validate -> DEBU 504[0m MSP Hospital1MSP validating identity
[36m2019-01-29 11:21:21.998 UTC [cauthdsl] func2 -> DEBU 4e1[0m 0xc42000eff8 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:21:22.001 UTC [msp] getCertificationChain -> DEBU 506[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 11:21:22.001 UTC [cauthdsl] func2 -> DEBU 505[0m 0xc42000eff8 processing identity 0 with bytes of 0a0c486f73706974616c324d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435444434341664f674177494241674951595a314536434c3354493572394a30583543592f367a414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777794c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424f6f457a7a30464664594e763839496757316b6d4c6152685375780a5a5a79533646664d6e2f732b654c4a496e684e38585a5263666f356a414c357748517573396b61356572722b3750467a6e392b44516c45496234366a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149504543616c696a0a326c39754279446345557572434452557345512f754f4e503348457037785657686864754d416f4743437147534d343942414d43413063414d455143494553490a36526f2f7548533674433963636a5a756d424e452b66666e4b6a4932556f5a654c337a47525856584169426569726264564c435a34364c6853506179696669430a4c4a6a2f45462b7a386b50564c59652f536f445352673d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:21:22.001 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 507[0m Checking if identity satisfies MEMBER role for Hospital2MSP
[36m2019-01-29 11:21:22.001 UTC [msp] Validate -> DEBU 508[0m MSP Hospital2MSP validating identity
[36m2019-01-29 11:21:22.001 UTC [cauthdsl] func2 -> DEBU 509[0m 0xc420166190 principal matched by identity 0
[36m2019-01-29 11:21:22.001 UTC [msp/identity] Verify -> DEBU 50a[0m Verify: digest = 00000000  32 4a 77 18 98 fe c5 2b  5c 1b b2 b7 16 e6 8b a0  |2Jw....+\.......|
00000010  3a 68 8a 92 b9 2c 2d c1  74 ad f1 95 77 0b 20 c4  |:h...,-.t...w. .|
[36m2019-01-29 11:21:22.001 UTC [msp/identity] Verify -> DEBU 50b[0m Verify: sig = 00000000  30 45 02 21 00 f2 71 66  5d d5 91 1f 73 38 68 65  |0E.!..qf]...s8he|
00000010  ef a6 15 ca f0 99 09 57  ac 81 30 6f 20 6c 6d f1  |.......W..0o lm.|
00000020  61 9f 5b c5 a3 02 20 6b  74 ab 2e a3 ea cf 54 16  |a.[... kt.....T.|
00000030  21 36 de 34 b2 84 d8 71  74 fb a7 8b 10 77 f0 de  |!6.4...qt....w..|
00000040  e5 74 af 6c 7d 89 a6                              |.t.l}..|
[36m2019-01-29 11:21:22.002 UTC [cauthdsl] func2 -> DEBU 50c[0m 0xc420166190 principal evaluation succeeds for identity 0
[36m2019-01-29 11:21:22.002 UTC [cauthdsl] func1 -> DEBU 50d[0m 0xc420166190 gate 1548760882000919207 evaluation succeeds
[36m2019-01-29 11:21:22.002 UTC [policies] Evaluate -> DEBU 50e[0m Signature set satisfies policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:21:22.002 UTC [policies] Evaluate -> DEBU 50f[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:21:22.002 UTC [policies] Evaluate -> DEBU 510[0m Signature set satisfies policy /Channel/Application/Readers
[36m2019-01-29 11:21:22.002 UTC [policies] Evaluate -> DEBU 511[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers
[36m2019-01-29 11:21:22.002 UTC [policies] Evaluate -> DEBU 512[0m Signature set satisfies policy /Channel/Readers
[36m2019-01-29 11:21:22.002 UTC [policies] Evaluate -> DEBU 513[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers
[36m2019-01-29 11:21:22.002 UTC [common/deliver] deliverBlocks -> DEBU 514[0m [channel: comunitychannel] Delivering block for (0xc42024bba0) for 10.0.0.34:59894
[36m2019-01-29 11:21:22.002 UTC [fsblkstorage] waitForBlock -> DEBU 515[0m Going to wait for newer blocks. maxAvailaBlockNumber=[1], waitForBlockNum=[2]
[36m2019-01-29 11:21:22.003 UTC [msp] getCertificationChain -> DEBU 516[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 11:21:22.023 UTC [cauthdsl] func2 -> DEBU 517[0m 0xc42000eff8 principal matched by identity 0
[36m2019-01-29 11:21:22.023 UTC [msp/identity] Verify -> DEBU 518[0m Verify: digest = 00000000  76 23 69 11 6d b0 29 7c  82 09 7e a7 b2 2e e7 03  |v#i.m.)|..~.....|
00000010  96 f4 58 a2 6b 4f 33 b6  d4 0b 90 37 8e b6 d8 fe  |..X.kO3....7....|
[36m2019-01-29 11:21:22.023 UTC [msp/identity] Verify -> DEBU 519[0m Verify: sig = 00000000  30 45 02 21 00 a5 39 97  6f 8a 30 42 89 db 48 e7  |0E.!..9.o.0B..H.|
00000010  40 0d 68 2c cd 2a 4e e0  55 7d 6d e7 d7 27 bf 97  |@.h,.*N.U}m..'..|
00000020  da 8e 53 6e c4 02 20 1d  41 e3 fc eb 8c 2c e2 db  |..Sn.. .A....,..|
00000030  15 ab a6 e7 bd e2 08 fa  0c 25 c1 03 e7 1e 60 3e  |.........%....`>|
00000040  a5 b1 85 5f 45 1e ea                              |..._E..|
[36m2019-01-29 11:21:22.024 UTC [cauthdsl] func2 -> DEBU 51a[0m 0xc42000eff8 principal evaluation succeeds for identity 0
[36m2019-01-29 11:21:22.024 UTC [cauthdsl] func1 -> DEBU 51b[0m 0xc42000eff8 gate 1548760881998595633 evaluation succeeds
[36m2019-01-29 11:21:22.024 UTC [policies] Evaluate -> DEBU 51c[0m Signature set satisfies policy /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:21:22.024 UTC [policies] Evaluate -> DEBU 51d[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:21:22.024 UTC [policies] Evaluate -> DEBU 51e[0m Signature set satisfies policy /Channel/Application/Readers
[36m2019-01-29 11:21:22.024 UTC [policies] Evaluate -> DEBU 51f[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers
[36m2019-01-29 11:21:22.024 UTC [policies] Evaluate -> DEBU 520[0m Signature set satisfies policy /Channel/Readers
[36m2019-01-29 11:21:22.024 UTC [policies] Evaluate -> DEBU 521[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers
[36m2019-01-29 11:21:22.024 UTC [common/deliver] deliverBlocks -> DEBU 522[0m [channel: comunitychannel] Delivering block for (0xc42028ed00) for 10.0.0.39:47750
[36m2019-01-29 11:21:22.024 UTC [fsblkstorage] waitForBlock -> DEBU 523[0m Going to wait for newer blocks. maxAvailaBlockNumber=[1], waitForBlockNum=[2]
[36m2019-01-29 11:21:32.118 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 524[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:21:32.119 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 525[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:9092
[36m2019-01-29 11:21:32.121 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 526[0m Connected to broker at kafka1.switch2logic.co.za:9092 (registered as #1)
[36m2019-01-29 11:21:32.143 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 527[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:21:32.143 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 528[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:21:32.144 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 529[0m Connected to broker at kafka0.switch2logic.co.za:9092 (registered as #0)
[36m2019-01-29 11:21:32.146 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 52a[0m client/metadata fetching metadata for all topics from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:21:32.165 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 52b[0m client/metadata fetching metadata for all topics from broker kafka1.switch2logic.co.za:9092
[36m2019-01-29 11:22:02.173 UTC [orderer/consensus/kafka] try -> DEBU 52c[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 11:22:02.175 UTC [orderer/consensus/kafka] try -> DEBU 52d[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
[36m2019-01-29 11:22:06.292 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 52e[0m [channel: comunitychannel] Successfully unmarshalled consumed message, offset is 6. Inspecting type...
[36m2019-01-29 11:22:06.292 UTC [orderer/consensus/kafka] processRegular -> DEBU 52f[0m [channel: comunitychannel] Processing regular Kafka message of type CONFIG
[36m2019-01-29 11:22:06.292 UTC [orderer/consensus/kafka] func2 -> DEBU 530[0m [channel: comunitychannel] Received config message
[36m2019-01-29 11:22:06.292 UTC [orderer/consensus/kafka] func2 -> DEBU 531[0m [channel: comunitychannel] Creating isolated block for config message
[36m2019-01-29 11:22:06.292 UTC [common/configtx] addToMap -> DEBU 532[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 11:22:06.293 UTC [common/configtx] addToMap -> DEBU 533[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 11:22:06.293 UTC [common/configtx] addToMap -> DEBU 534[0m Adding to config map: [Group]  /Channel/Application/Hospital2MSP
[36m2019-01-29 11:22:06.293 UTC [common/configtx] addToMap -> DEBU 535[0m Adding to config map: [Value]  /Channel/Application/Hospital2MSP/MSP
[36m2019-01-29 11:22:06.293 UTC [common/configtx] addToMap -> DEBU 536[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:22:06.293 UTC [common/configtx] addToMap -> DEBU 537[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Writers
[36m2019-01-29 11:22:06.293 UTC [common/configtx] addToMap -> DEBU 538[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Admins
[36m2019-01-29 11:22:06.293 UTC [common/configtx] addToMap -> DEBU 539[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 11:22:06.293 UTC [common/configtx] addToMap -> DEBU 53a[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 11:22:06.293 UTC [common/configtx] addToMap -> DEBU 53b[0m Adding to config map: [Group]  /Channel/Application/Hospital2MSP
[36m2019-01-29 11:22:06.293 UTC [common/configtx] addToMap -> DEBU 53c[0m Adding to config map: [Value]  /Channel/Application/Hospital2MSP/MSP
[36m2019-01-29 11:22:06.293 UTC [common/configtx] addToMap -> DEBU 53d[0m Adding to config map: [Value]  /Channel/Application/Hospital2MSP/AnchorPeers
[36m2019-01-29 11:22:06.293 UTC [common/configtx] addToMap -> DEBU 53e[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:22:06.293 UTC [common/configtx] addToMap -> DEBU 53f[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Writers
[36m2019-01-29 11:22:06.293 UTC [common/configtx] addToMap -> DEBU 540[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Admins
[36m2019-01-29 11:22:06.293 UTC [common/configtx] verifyDeltaSet -> DEBU 541[0m Processing change to key: [Group]  /Channel/Application/Hospital2MSP
[36m2019-01-29 11:22:06.294 UTC [common/configtx] policyForItem -> DEBU 542[0m Getting policy for item Hospital2MSP with mod_policy Admins
[36m2019-01-29 11:22:06.294 UTC [policies] Manager -> DEBU 543[0m Manager Channel looking up path [Application]
[36m2019-01-29 11:22:06.294 UTC [policies] Manager -> DEBU 544[0m Manager Channel has managers Orderer
[36m2019-01-29 11:22:06.294 UTC [policies] Manager -> DEBU 545[0m Manager Channel has managers Application
[36m2019-01-29 11:22:06.294 UTC [policies] Manager -> DEBU 546[0m Manager Channel/Application looking up path []
[36m2019-01-29 11:22:06.294 UTC [policies] Manager -> DEBU 547[0m Manager Channel/Application has managers Hospital2MSP
[36m2019-01-29 11:22:06.294 UTC [policies] Manager -> DEBU 548[0m Manager Channel/Application has managers Hospital3MSP
[36m2019-01-29 11:22:06.294 UTC [policies] Manager -> DEBU 549[0m Manager Channel/Application has managers Hospital1MSP
[36m2019-01-29 11:22:06.294 UTC [policies] Manager -> DEBU 54a[0m Manager Channel/Application looking up path [Hospital2MSP]
[36m2019-01-29 11:22:06.294 UTC [policies] Manager -> DEBU 54b[0m Manager Channel/Application has managers Hospital1MSP
[36m2019-01-29 11:22:06.294 UTC [policies] Manager -> DEBU 54c[0m Manager Channel/Application has managers Hospital2MSP
[36m2019-01-29 11:22:06.294 UTC [policies] Manager -> DEBU 54d[0m Manager Channel/Application has managers Hospital3MSP
[36m2019-01-29 11:22:06.294 UTC [policies] Manager -> DEBU 54e[0m Manager Channel/Application/Hospital2MSP looking up path []
[36m2019-01-29 11:22:06.294 UTC [policies] Evaluate -> DEBU 54f[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Admins ==
2019-01-29 11:22:06.294 UTC [msp] DeserializeIdentity -> INFO 550[0m Obtaining identity
[36m2019-01-29 11:22:06.294 UTC [msp/identity] newIdentity -> DEBU 551[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAJ6XtHMTTW1JzdpD54qCc3wwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA+z1rb9aA1HwHfNLi9V9VZe
bdwT4MXak445zGz4DXzRUsmNeBngzntKb6qpgHQpeFtzbzLx+zwiynRDvfdzORGj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPEC
alij2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQC
IA8A3CCJeCasyy7gYd/M1aHYmwWsecY7UY7cVUGy4XOxAiB0rrmfnGW6JA11PDCu
+5c5Rpn0jF5ddsFn+UIpzBFvmg==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:06.295 UTC [cauthdsl] func1 -> DEBU 552[0m 0xc42000e850 gate 1548760926295406642 evaluation starts
[36m2019-01-29 11:22:06.295 UTC [cauthdsl] func2 -> DEBU 553[0m 0xc42000e850 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:06.295 UTC [cauthdsl] func2 -> DEBU 554[0m 0xc42000e850 processing identity 0 with bytes of 0a0c486f73706974616c324d535012df062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943547a4343416661674177494241674952414a365874484d545457314a7a6470443534714363337777436759494b6f5a497a6a304541774977675973780a437a414a42674e5642415954416c56544d524d77455159445651514945777044595778705a6d3979626d6c684d525977464159445651514845773154595734670a526e4a68626d4e7063324e764d535577497759445651514b4578786f62334e7761585268624449756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70680a4d5367774a675944565151444578396a5953356f62334e7761585268624449756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70684d423458445445350a4d4445794f5441334e4445314e466f58445449354d4445794e6a41334e4445314e466f776544454c4d416b474131554542684d4356564d78457a415242674e560a42416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e6862694247636d467559326c7a59323878447a414e42674e56424173540a426d4e7361575675644445724d436b47413155454177776951575274615735416147397a63476c30595777794c6e4e336158526a61444a73623264705979356a0a627935365954425a4d424d4742797147534d34394167454743437147534d3439417745484130494142412b7a31726239614131487748664e4c69395639565a650a62647754344d58616b3434357a477a3444587a5255736d4e65426e677a6e744b62367170674851706546747a627a4c782b7a7769796e52447666647a4f52476a0a5454424c4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b41495045430a616c696a326c39754279446345557572434452557345512f754f4e503348457037785657686864754d416f4743437147534d343942414d43413063414d4551430a494138413343434a654361737979376759642f4d316148596d77577365635937555937635655477934584f784169423072726d666e4757364a413131504443750a2b35633552706e306a4635646473466e2b5549707a4246766d673d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:06.295 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 555[0m Checking if identity satisfies ADMIN role for Hospital2MSP
[36m2019-01-29 11:22:06.295 UTC [cauthdsl] func2 -> DEBU 556[0m 0xc42000e850 principal matched by identity 0
[36m2019-01-29 11:22:06.295 UTC [msp/identity] Verify -> DEBU 557[0m Verify: digest = 00000000  8a d5 75 1d 1c c2 e6 24  2a 57 ab a4 f6 2e f5 ec  |..u....$*W......|
00000010  5b 3c 08 a2 15 39 d8 3b  09 36 09 a5 d9 fd e3 ed  |[<...9.;.6......|
[36m2019-01-29 11:22:06.295 UTC [msp/identity] Verify -> DEBU 558[0m Verify: sig = 00000000  30 44 02 20 6d 2a 1a 23  06 d6 7f 73 ea b2 4b 36  |0D. m*.#...s..K6|
00000010  b1 d5 e2 49 36 e6 61 25  e2 e5 58 86 3f 5e 03 f7  |...I6.a%..X.?^..|
00000020  cf e5 3d d0 02 20 7b e4  ee b4 5a 91 13 f9 ae 06  |..=.. {...Z.....|
00000030  6f bf 68 b3 84 fb 3a 37  49 98 5d 67 b6 c9 9b 00  |o.h...:7I.]g....|
00000040  54 66 c1 84 87 98                                 |Tf....|
[36m2019-01-29 11:22:06.296 UTC [cauthdsl] func2 -> DEBU 559[0m 0xc42000e850 principal evaluation succeeds for identity 0
[36m2019-01-29 11:22:06.296 UTC [cauthdsl] func1 -> DEBU 55a[0m 0xc42000e850 gate 1548760926295406642 evaluation succeeds
[36m2019-01-29 11:22:06.296 UTC [policies] Evaluate -> DEBU 55b[0m Signature set satisfies policy /Channel/Application/Hospital2MSP/Admins
[36m2019-01-29 11:22:06.296 UTC [policies] Evaluate -> DEBU 55c[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Admins
[36m2019-01-29 11:22:06.296 UTC [common/configtx] verifyDeltaSet -> DEBU 55d[0m Processing change to key: [Value]  /Channel/Application/Hospital2MSP/AnchorPeers
[36m2019-01-29 11:22:06.296 UTC [common/configtx] recurseConfigMap -> DEBU 55e[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital1MSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.296 UTC [common/configtx] recurseConfigMap -> DEBU 55f[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\022\022\020\n\014Hospital1MSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.296 UTC [common/configtx] recurseConfigMap -> DEBU 560[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital1MSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.296 UTC [common/configtx] recurseConfigMap -> DEBU 561[0m Setting policy for key Readers to 
[36m2019-01-29 11:22:06.297 UTC [common/configtx] recurseConfigMap -> DEBU 562[0m Setting policy for key Writers to 
[36m2019-01-29 11:22:06.297 UTC [common/configtx] recurseConfigMap -> DEBU 563[0m Setting policy for key Admins to 
[36m2019-01-29 11:22:06.297 UTC [common/configtx] recurseConfigMap -> DEBU 564[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital3MSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.297 UTC [common/configtx] recurseConfigMap -> DEBU 565[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\022\022\020\n\014Hospital3MSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.297 UTC [common/configtx] recurseConfigMap -> DEBU 566[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital3MSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.297 UTC [common/configtx] recurseConfigMap -> DEBU 567[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.297 UTC [common/configtx] recurseConfigMap -> DEBU 568[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.297 UTC [common/configtx] recurseConfigMap -> DEBU 569[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.297 UTC [common/configtx] recurseConfigMap -> DEBU 56a[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\016\022\014\n\nOrdererMSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.297 UTC [common/configtx] recurseConfigMap -> DEBU 56b[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\016\022\014\n\nOrdererMSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.297 UTC [common/configtx] recurseConfigMap -> DEBU 56c[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\nOrdererMSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.297 UTC [common/configtx] recurseConfigMap -> DEBU 56d[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.297 UTC [common/configtx] recurseConfigMap -> DEBU 56e[0m Setting policy for key BlockValidation to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.298 UTC [common/configtx] recurseConfigMap -> DEBU 56f[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.298 UTC [common/configtx] recurseConfigMap -> DEBU 570[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.298 UTC [common/configtx] recurseConfigMap -> DEBU 571[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.298 UTC [common/configtx] recurseConfigMap -> DEBU 572[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.298 UTC [common/configtx] recurseConfigMap -> DEBU 573[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 11:22:06.298 UTC [common/channelconfig] NewStandardValues -> DEBU 574[0m Initializing protos for *channelconfig.ChannelProtos
[36m2019-01-29 11:22:06.298 UTC [common/channelconfig] initializeProtosStruct -> DEBU 575[0m Processing field: HashingAlgorithm
[36m2019-01-29 11:22:06.298 UTC [common/channelconfig] initializeProtosStruct -> DEBU 576[0m Processing field: BlockDataHashingStructure
[36m2019-01-29 11:22:06.298 UTC [common/channelconfig] initializeProtosStruct -> DEBU 577[0m Processing field: OrdererAddresses
[36m2019-01-29 11:22:06.298 UTC [common/channelconfig] initializeProtosStruct -> DEBU 578[0m Processing field: Consortium
[36m2019-01-29 11:22:06.299 UTC [common/channelconfig] initializeProtosStruct -> DEBU 579[0m Processing field: Capabilities
[36m2019-01-29 11:22:06.299 UTC [common/channelconfig] NewStandardValues -> DEBU 57a[0m Initializing protos for *channelconfig.ApplicationProtos
[36m2019-01-29 11:22:06.299 UTC [common/channelconfig] initializeProtosStruct -> DEBU 57b[0m Processing field: ACLs
[36m2019-01-29 11:22:06.299 UTC [common/channelconfig] initializeProtosStruct -> DEBU 57c[0m Processing field: Capabilities
[36m2019-01-29 11:22:06.299 UTC [common/channelconfig] NewStandardValues -> DEBU 57d[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 11:22:06.299 UTC [common/channelconfig] initializeProtosStruct -> DEBU 57e[0m Processing field: AnchorPeers
[36m2019-01-29 11:22:06.299 UTC [common/channelconfig] NewStandardValues -> DEBU 57f[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 11:22:06.299 UTC [common/channelconfig] initializeProtosStruct -> DEBU 580[0m Processing field: MSP
[36m2019-01-29 11:22:06.299 UTC [common/channelconfig] Validate -> DEBU 581[0m Anchor peers for org Hospital2MSP are anchor_peers:<host:"peer0.hospital2.switch2logic.co.za" port:7051 > 
[36m2019-01-29 11:22:06.299 UTC [common/channelconfig] validateMSP -> DEBU 582[0m Setting up MSP for org Hospital2MSP
[36m2019-01-29 11:22:06.299 UTC [msp] newBccspMsp -> DEBU 583[0m Creating BCCSP-based MSP instance
[36m2019-01-29 11:22:06.299 UTC [msp] New -> DEBU 584[0m Creating Cache-MSP instance
[36m2019-01-29 11:22:06.299 UTC [msp] Setup -> DEBU 585[0m Setting up MSP instance Hospital2MSP
[36m2019-01-29 11:22:06.300 UTC [msp/identity] newIdentity -> DEBU 586[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdjCCAhygAwIBAgIRALZ3Pffn/xWUtUWPSK6CYVAwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowgYsxCzAJBgNVBAYTAlVTMRMwEQYD
VQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4gRnJhbmNpc2NvMSUwIwYDVQQK
Exxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMSgwJgYDVQQDEx9jYS5ob3Nw
aXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcD
QgAESzVpiz6P4mIb+KwvrttcXJKZR5i137Is4298E7OrkYdAg0tB4rpfKTXzUMwW
vI12cTDujpyaZSiqQ3/zP4fOQqNfMF0wDgYDVR0PAQH/BAQDAgGmMA8GA1UdJQQI
MAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQg8QJqWKPaX24HINwR
S6sINFSwRD+440/ccSnvFVaGF24wCgYIKoZIzj0EAwIDSAAwRQIhAPO2Nng3/nqb
cKPliUqVPVjBMu+D55M90+jexWSOpkjcAiADPx8qv9exZ4BfP3+G49Qzpx0Meblj
53vzUgYFa/0EXw==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:06.301 UTC [msp/identity] newIdentity -> DEBU 587[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAJ6XtHMTTW1JzdpD54qCc3wwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA+z1rb9aA1HwHfNLi9V9VZe
bdwT4MXak445zGz4DXzRUsmNeBngzntKb6qpgHQpeFtzbzLx+zwiynRDvfdzORGj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPEC
alij2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQC
IA8A3CCJeCasyy7gYd/M1aHYmwWsecY7UY7cVUGy4XOxAiB0rrmfnGW6JA11PDCu
+5c5Rpn0jF5ddsFn+UIpzBFvmg==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:06.302 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 588[0m Checking if identity satisfies role [CLIENT] for Hospital2MSP
[36m2019-01-29 11:22:06.303 UTC [msp] Validate -> DEBU 589[0m MSP Hospital2MSP validating identity
[36m2019-01-29 11:22:06.303 UTC [msp] getCertificationChain -> DEBU 58a[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 11:22:06.304 UTC [msp] hasOURole -> DEBU 58b[0m MSP Hospital2MSP checking if the identity is a client
[36m2019-01-29 11:22:06.304 UTC [msp] getCertificationChain -> DEBU 58c[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 11:22:06.304 UTC [common/channelconfig] NewStandardValues -> DEBU 58d[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 11:22:06.304 UTC [common/channelconfig] initializeProtosStruct -> DEBU 58e[0m Processing field: AnchorPeers
[36m2019-01-29 11:22:06.304 UTC [common/channelconfig] NewStandardValues -> DEBU 58f[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 11:22:06.304 UTC [common/channelconfig] initializeProtosStruct -> DEBU 590[0m Processing field: MSP
[36m2019-01-29 11:22:06.304 UTC [common/channelconfig] Validate -> DEBU 591[0m Anchor peers for org Hospital3MSP are anchor_peers:<host:"peer0.hospital3.switch2logic.co.za" port:7051 > 
[36m2019-01-29 11:22:06.304 UTC [common/channelconfig] validateMSP -> DEBU 592[0m Setting up MSP for org Hospital3MSP
[36m2019-01-29 11:22:06.304 UTC [msp] newBccspMsp -> DEBU 593[0m Creating BCCSP-based MSP instance
[36m2019-01-29 11:22:06.304 UTC [msp] New -> DEBU 594[0m Creating Cache-MSP instance
[36m2019-01-29 11:22:06.304 UTC [msp] Setup -> DEBU 595[0m Setting up MSP instance Hospital3MSP
[36m2019-01-29 11:22:06.305 UTC [msp/identity] newIdentity -> DEBU 596[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdTCCAhugAwIBAgIQCCXeHGgObIJo8aq4AdvgVjAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMy5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQm1Oo/AEu7ZZToE15Y5ntlaN128Hs/exX3jNH5nO6r4tvyFtBBNEn1ecXxxPtm
dmQ+nPHONorLwDC42SMAKwfro18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCA7vVWFXWpgZBojQ1ZL
xUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEAlOUoPtpfO8V7
vFRCanjU5zZWGO5ajFlPDJjjDTy3UrwCICrUYV32JrR/m3dslCl15dSThHrKtYCC
Ehvyw1+lNqiG
-----END CERTIFICATE-----
[36m2019-01-29 11:22:06.306 UTC [msp/identity] newIdentity -> DEBU 597[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAI8BA/5Jf+5m8ncI+D7aSiMwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwzLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABF1IehIKIjh8tXYZ7rw4QOuc
g/tpBAkJeOt44zr/LepbjhyFW8vIRnkw75i6Wcg+LOoAS+2NFNg2oNjqg//yCHWj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIDu9
VYVdamBkGiNDVkvFS+AEt6Jrn0yK8hFyPGZhqenJMAoGCCqGSM49BAMCA0cAMEQC
IFyc85pwbCAxGNdw4fflsX2I3HrP3h93YAB2zSDC8HtkAiAQxblAZfEnXUiQKrqE
8I10bYJqD6pJz1jb3Pv+4xplTA==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:06.307 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 598[0m Checking if identity satisfies role [CLIENT] for Hospital3MSP
[36m2019-01-29 11:22:06.307 UTC [msp] Validate -> DEBU 599[0m MSP Hospital3MSP validating identity
[36m2019-01-29 11:22:06.308 UTC [msp] getCertificationChain -> DEBU 59a[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 11:22:06.308 UTC [msp] hasOURole -> DEBU 59b[0m MSP Hospital3MSP checking if the identity is a client
[36m2019-01-29 11:22:06.308 UTC [msp] getCertificationChain -> DEBU 59c[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 11:22:06.308 UTC [common/channelconfig] NewStandardValues -> DEBU 59d[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 11:22:06.309 UTC [common/channelconfig] initializeProtosStruct -> DEBU 59e[0m Processing field: AnchorPeers
[36m2019-01-29 11:22:06.309 UTC [common/channelconfig] NewStandardValues -> DEBU 59f[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 11:22:06.309 UTC [common/channelconfig] initializeProtosStruct -> DEBU 5a0[0m Processing field: MSP
[36m2019-01-29 11:22:06.309 UTC [common/channelconfig] Validate -> DEBU 5a1[0m Anchor peers for org Hospital1MSP are 
[36m2019-01-29 11:22:06.309 UTC [common/channelconfig] validateMSP -> DEBU 5a2[0m Setting up MSP for org Hospital1MSP
[36m2019-01-29 11:22:06.309 UTC [msp] newBccspMsp -> DEBU 5a3[0m Creating BCCSP-based MSP instance
[36m2019-01-29 11:22:06.309 UTC [msp] New -> DEBU 5a4[0m Creating Cache-MSP instance
[36m2019-01-29 11:22:06.309 UTC [msp] Setup -> DEBU 5a5[0m Setting up MSP instance Hospital1MSP
[36m2019-01-29 11:22:06.309 UTC [msp/identity] newIdentity -> DEBU 5a6[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdDCCAhugAwIBAgIQZxH8VsnceZxXgKxlURLgVTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMS5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQX9V4k/h/3pFxByKDscjgQrC0YS/bSbKvzSG2Z2PvltEWp40qB9OaKUN0GQYzo
yK2Etdvyjfd1OFwaTjpGSvv9o18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCBVU7DoP9Msf/nNTNl6
5FxwAXZ82eaGBT7zWYdyh4QbBjAKBggqhkjOPQQDAgNHADBEAiBmXGWQbxCGRp0c
5/y9qJyciB1CDHsknRZDjNzt5f563wIgMy7VPi1R8DpIIIRN9B3mfhDpRa+J+8PA
S4DOJy7rjp4=
-----END CERTIFICATE-----
[36m2019-01-29 11:22:06.310 UTC [msp/identity] newIdentity -> DEBU 5a7[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:06.311 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 5a8[0m Checking if identity satisfies role [CLIENT] for Hospital1MSP
[36m2019-01-29 11:22:06.311 UTC [msp] Validate -> DEBU 5a9[0m MSP Hospital1MSP validating identity
[36m2019-01-29 11:22:06.311 UTC [msp] getCertificationChain -> DEBU 5aa[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 11:22:06.315 UTC [msp] hasOURole -> DEBU 5ab[0m MSP Hospital1MSP checking if the identity is a client
[36m2019-01-29 11:22:06.315 UTC [msp] getCertificationChain -> DEBU 5ac[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 11:22:06.315 UTC [common/channelconfig] NewStandardValues -> DEBU 5ad[0m Initializing protos for *channelconfig.OrdererProtos
[36m2019-01-29 11:22:06.315 UTC [common/channelconfig] initializeProtosStruct -> DEBU 5ae[0m Processing field: ConsensusType
[36m2019-01-29 11:22:06.315 UTC [common/channelconfig] initializeProtosStruct -> DEBU 5af[0m Processing field: BatchSize
[36m2019-01-29 11:22:06.315 UTC [common/channelconfig] initializeProtosStruct -> DEBU 5b0[0m Processing field: BatchTimeout
[36m2019-01-29 11:22:06.315 UTC [common/channelconfig] initializeProtosStruct -> DEBU 5b1[0m Processing field: KafkaBrokers
[36m2019-01-29 11:22:06.315 UTC [common/channelconfig] initializeProtosStruct -> DEBU 5b2[0m Processing field: ChannelRestrictions
[36m2019-01-29 11:22:06.315 UTC [common/channelconfig] initializeProtosStruct -> DEBU 5b3[0m Processing field: Capabilities
[36m2019-01-29 11:22:06.316 UTC [common/channelconfig] NewStandardValues -> DEBU 5b4[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 11:22:06.316 UTC [common/channelconfig] initializeProtosStruct -> DEBU 5b5[0m Processing field: MSP
[36m2019-01-29 11:22:06.316 UTC [common/channelconfig] validateMSP -> DEBU 5b6[0m Setting up MSP for org OrdererOrg
[36m2019-01-29 11:22:06.316 UTC [msp] newBccspMsp -> DEBU 5b7[0m Creating BCCSP-based MSP instance
[36m2019-01-29 11:22:06.316 UTC [msp] New -> DEBU 5b8[0m Creating Cache-MSP instance
[36m2019-01-29 11:22:06.316 UTC [msp] Setup -> DEBU 5b9[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 11:22:06.316 UTC [msp/identity] newIdentity -> DEBU 5ba[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 11:22:06.316 UTC [msp/identity] newIdentity -> DEBU 5bb[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:06.317 UTC [msp] Validate -> DEBU 5bc[0m MSP OrdererMSP validating identity
[36m2019-01-29 11:22:06.317 UTC [msp] Setup -> DEBU 5bd[0m Setting up the MSP manager (4 msps)
[36m2019-01-29 11:22:06.317 UTC [msp] Setup -> DEBU 5be[0m MSP manager setup complete, setup 4 msps
[36m2019-01-29 11:22:06.317 UTC [policies] NewManagerImpl -> DEBU 5bf[0m Proposed new policy Readers for Channel/Application/Hospital2MSP
[36m2019-01-29 11:22:06.317 UTC [policies] NewManagerImpl -> DEBU 5c0[0m Proposed new policy Writers for Channel/Application/Hospital2MSP
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5c1[0m Proposed new policy Admins for Channel/Application/Hospital2MSP
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5c2[0m Proposed new policy Writers for Channel/Application/Hospital3MSP
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5c3[0m Proposed new policy Admins for Channel/Application/Hospital3MSP
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5c4[0m Proposed new policy Readers for Channel/Application/Hospital3MSP
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5c5[0m Proposed new policy Admins for Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5c6[0m Proposed new policy Readers for Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5c7[0m Proposed new policy Writers for Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5c8[0m Proposed new policy Writers for Channel/Application
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5c9[0m Proposed new policy Admins for Channel/Application
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5ca[0m Proposed new policy Readers for Channel/Application
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5cb[0m Proposed new policy Writers for Channel/Orderer/OrdererOrg
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5cc[0m Proposed new policy Admins for Channel/Orderer/OrdererOrg
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5cd[0m Proposed new policy Readers for Channel/Orderer/OrdererOrg
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5ce[0m Proposed new policy Readers for Channel/Orderer
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5cf[0m Proposed new policy Writers for Channel/Orderer
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5d0[0m Proposed new policy BlockValidation for Channel/Orderer
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5d1[0m Proposed new policy Admins for Channel/Orderer
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5d2[0m Proposed new policy Admins for Channel
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5d3[0m Proposed new policy Readers for Channel
[36m2019-01-29 11:22:06.318 UTC [policies] NewManagerImpl -> DEBU 5d4[0m Proposed new policy Writers for Channel
[36m2019-01-29 11:22:06.318 UTC [common/configtx] addToMap -> DEBU 5d5[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 11:22:06.318 UTC [common/configtx] addToMap -> DEBU 5d6[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 11:22:06.318 UTC [common/configtx] addToMap -> DEBU 5d7[0m Adding to config map: [Group]  /Channel/Application/Hospital2MSP
[36m2019-01-29 11:22:06.318 UTC [common/configtx] addToMap -> DEBU 5d8[0m Adding to config map: [Value]  /Channel/Application/Hospital2MSP/AnchorPeers
[36m2019-01-29 11:22:06.318 UTC [common/configtx] addToMap -> DEBU 5d9[0m Adding to config map: [Value]  /Channel/Application/Hospital2MSP/MSP
[36m2019-01-29 11:22:06.318 UTC [common/configtx] addToMap -> DEBU 5da[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:22:06.318 UTC [common/configtx] addToMap -> DEBU 5db[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Writers
[36m2019-01-29 11:22:06.318 UTC [common/configtx] addToMap -> DEBU 5dc[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Admins
[36m2019-01-29 11:22:06.318 UTC [common/configtx] addToMap -> DEBU 5dd[0m Adding to config map: [Group]  /Channel/Application/Hospital3MSP
[36m2019-01-29 11:22:06.318 UTC [common/configtx] addToMap -> DEBU 5de[0m Adding to config map: [Value]  /Channel/Application/Hospital3MSP/AnchorPeers
[36m2019-01-29 11:22:06.318 UTC [common/configtx] addToMap -> DEBU 5df[0m Adding to config map: [Value]  /Channel/Application/Hospital3MSP/MSP
[36m2019-01-29 11:22:06.318 UTC [common/configtx] addToMap -> DEBU 5e0[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Writers
[36m2019-01-29 11:22:06.318 UTC [common/configtx] addToMap -> DEBU 5e1[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Admins
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5e2[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5e3[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5e4[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/MSP
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5e5[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5e6[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5e7[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5e8[0m Adding to config map: [Value]  /Channel/Application/Capabilities
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5e9[0m Adding to config map: [Policy] /Channel/Application/Writers
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5ea[0m Adding to config map: [Policy] /Channel/Application/Admins
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5eb[0m Adding to config map: [Policy] /Channel/Application/Readers
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5ec[0m Adding to config map: [Group]  /Channel/Orderer
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5ed[0m Adding to config map: [Group]  /Channel/Orderer/OrdererOrg
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5ee[0m Adding to config map: [Value]  /Channel/Orderer/OrdererOrg/MSP
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5ef[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5f0[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5f1[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Admins
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5f2[0m Adding to config map: [Value]  /Channel/Orderer/BatchSize
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5f3[0m Adding to config map: [Value]  /Channel/Orderer/BatchTimeout
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5f4[0m Adding to config map: [Value]  /Channel/Orderer/ChannelRestrictions
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5f5[0m Adding to config map: [Value]  /Channel/Orderer/Capabilities
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5f6[0m Adding to config map: [Value]  /Channel/Orderer/KafkaBrokers
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5f7[0m Adding to config map: [Value]  /Channel/Orderer/ConsensusType
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5f8[0m Adding to config map: [Policy] /Channel/Orderer/Readers
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5f9[0m Adding to config map: [Policy] /Channel/Orderer/Writers
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5fa[0m Adding to config map: [Policy] /Channel/Orderer/BlockValidation
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5fb[0m Adding to config map: [Policy] /Channel/Orderer/Admins
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5fc[0m Adding to config map: [Value]  /Channel/Consortium
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5fd[0m Adding to config map: [Value]  /Channel/HashingAlgorithm
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5fe[0m Adding to config map: [Value]  /Channel/BlockDataHashingStructure
[36m2019-01-29 11:22:06.319 UTC [common/configtx] addToMap -> DEBU 5ff[0m Adding to config map: [Value]  /Channel/OrdererAddresses
[36m2019-01-29 11:22:06.320 UTC [common/configtx] addToMap -> DEBU 600[0m Adding to config map: [Value]  /Channel/Capabilities
[36m2019-01-29 11:22:06.320 UTC [common/configtx] addToMap -> DEBU 601[0m Adding to config map: [Policy] /Channel/Admins
[36m2019-01-29 11:22:06.320 UTC [common/configtx] addToMap -> DEBU 602[0m Adding to config map: [Policy] /Channel/Readers
[36m2019-01-29 11:22:06.320 UTC [common/configtx] addToMap -> DEBU 603[0m Adding to config map: [Policy] /Channel/Writers
[36m2019-01-29 11:22:06.320 UTC [common/channelconfig] LogSanityChecks -> DEBU 604[0m As expected, current configuration has policy '/Channel/Readers'
[36m2019-01-29 11:22:06.320 UTC [common/channelconfig] LogSanityChecks -> DEBU 605[0m As expected, current configuration has policy '/Channel/Writers'
[36m2019-01-29 11:22:06.320 UTC [policies] Manager -> DEBU 606[0m Manager Channel looking up path [Application]
[36m2019-01-29 11:22:06.320 UTC [policies] Manager -> DEBU 607[0m Manager Channel has managers Application
[36m2019-01-29 11:22:06.320 UTC [policies] Manager -> DEBU 608[0m Manager Channel has managers Orderer
[36m2019-01-29 11:22:06.320 UTC [policies] Manager -> DEBU 609[0m Manager Channel/Application looking up path []
[36m2019-01-29 11:22:06.320 UTC [policies] Manager -> DEBU 60a[0m Manager Channel/Application has managers Hospital2MSP
[36m2019-01-29 11:22:06.320 UTC [policies] Manager -> DEBU 60b[0m Manager Channel/Application has managers Hospital3MSP
[36m2019-01-29 11:22:06.320 UTC [policies] Manager -> DEBU 60c[0m Manager Channel/Application has managers Hospital1MSP
[36m2019-01-29 11:22:06.320 UTC [common/channelconfig] LogSanityChecks -> DEBU 60d[0m As expected, current configuration has policy '/Channel/Application/Readers'
[36m2019-01-29 11:22:06.320 UTC [common/channelconfig] LogSanityChecks -> DEBU 60e[0m As expected, current configuration has policy '/Channel/Application/Writers'
[36m2019-01-29 11:22:06.320 UTC [common/channelconfig] LogSanityChecks -> DEBU 60f[0m As expected, current configuration has policy '/Channel/Application/Admins'
[36m2019-01-29 11:22:06.320 UTC [policies] Manager -> DEBU 610[0m Manager Channel looking up path [Orderer]
[36m2019-01-29 11:22:06.320 UTC [policies] Manager -> DEBU 611[0m Manager Channel has managers Application
[36m2019-01-29 11:22:06.320 UTC [policies] Manager -> DEBU 612[0m Manager Channel has managers Orderer
[36m2019-01-29 11:22:06.320 UTC [policies] Manager -> DEBU 613[0m Manager Channel/Orderer looking up path []
[36m2019-01-29 11:22:06.320 UTC [policies] Manager -> DEBU 614[0m Manager Channel/Orderer has managers OrdererOrg
[36m2019-01-29 11:22:06.320 UTC [common/channelconfig] LogSanityChecks -> DEBU 615[0m As expected, current configuration has policy '/Channel/Orderer/BlockValidation'
[36m2019-01-29 11:22:06.320 UTC [common/capabilities] Supported -> DEBU 616[0m Orderer capability V1_1 is supported and is enabled
[36m2019-01-29 11:22:06.320 UTC [common/capabilities] Supported -> DEBU 617[0m Channel capability V1_1 is supported and is enabled
[36m2019-01-29 11:22:06.320 UTC [msp] GetDefaultSigningIdentity -> DEBU 618[0m Obtaining default signing identity
[36m2019-01-29 11:22:06.320 UTC [msp] GetDefaultSigningIdentity -> DEBU 619[0m Obtaining default signing identity
[36m2019-01-29 11:22:06.320 UTC [msp/identity] Sign -> DEBU 61a[0m Sign: plaintext: 0ABD060A0A4F7264657265724D535012...5425392F1F373837A67D6D1E818CA8E4 
[36m2019-01-29 11:22:06.320 UTC [msp/identity] Sign -> DEBU 61b[0m Sign: digest: 8C7E45783440AD6D0D2BD027DE8450A0A3C761AD2A36473AA20273027294644F 
[36m2019-01-29 11:22:06.320 UTC [orderer/commmon/multichannel] addLastConfigSignature -> DEBU 61c[0m [channel: comunitychannel] Detected lastConfigSeq transitioning from 2 to 3, setting lastConfigBlockNum from 1 to 2
[36m2019-01-29 11:22:06.320 UTC [msp] GetDefaultSigningIdentity -> DEBU 61d[0m Obtaining default signing identity
[36m2019-01-29 11:22:06.321 UTC [orderer/commmon/multichannel] addLastConfigSignature -> DEBU 61e[0m [channel: comunitychannel] About to write block, setting its LAST_CONFIG to 2
[36m2019-01-29 11:22:06.321 UTC [msp] GetDefaultSigningIdentity -> DEBU 61f[0m Obtaining default signing identity
[36m2019-01-29 11:22:06.321 UTC [msp/identity] Sign -> DEBU 620[0m Sign: plaintext: 08020ABD060A0A4F7264657265724D53...5425392F1F373837A67D6D1E818CA8E4 
[36m2019-01-29 11:22:06.321 UTC [msp/identity] Sign -> DEBU 621[0m Sign: digest: 0262462D7381814AE76B48F8831C19B50E9D214116B698EBA9DD65E79C6FB70B 
[36m2019-01-29 11:22:06.323 UTC [fsblkstorage] indexBlock -> DEBU 622[0m Indexing block [blockNum=2, blockHash=[]byte{0xfc, 0xfe, 0xf0, 0xa2, 0x3d, 0x94, 0xdf, 0x5a, 0x51, 0xb3, 0xb3, 0xcb, 0x76, 0x22, 0xee, 0xd6, 0x7b, 0x9b, 0x60, 0xb5, 0x59, 0x9b, 0x4f, 0xf2, 0x63, 0xc8, 0x8, 0x8f, 0xef, 0xc0, 0xa6, 0x6b} txOffsets=
txId= locPointer=offset=71, bytesLength=21751
]
[36m2019-01-29 11:22:06.325 UTC [fsblkstorage] updateCheckpoint -> DEBU 623[0m Broadcasting about update checkpointInfo: latestFileChunkSuffixNum=[0], latestFileChunksize=[69094], isChainEmpty=[false], lastBlockNumber=[2]
[36m2019-01-29 11:22:06.325 UTC [orderer/commmon/multichannel] commitBlock -> DEBU 624[0m [channel: comunitychannel] Wrote block 2
[36m2019-01-29 11:22:06.325 UTC [fsblkstorage] waitForBlock -> DEBU 625[0m Came out of wait. maxAvailaBlockNumber=[2]
[36m2019-01-29 11:22:06.325 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 626[0m Remaining bytes=[23709], Going to peek [8] bytes
[36m2019-01-29 11:22:06.325 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 627[0m Returning blockbytes - length=[23706], placementInfo={fileNum=[0], startOffset=[45385], bytesOffset=[45388]}
[36m2019-01-29 11:22:06.325 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 628[0m blockbytes [23706] read from file [0]
[36m2019-01-29 11:22:06.325 UTC [policies] Evaluate -> DEBU 629[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers ==
[36m2019-01-29 11:22:06.325 UTC [policies] Evaluate -> DEBU 62a[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:06.325 UTC [policies] Evaluate -> DEBU 62b[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers ==
[36m2019-01-29 11:22:06.325 UTC [policies] Evaluate -> DEBU 62c[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:06.326 UTC [policies] Evaluate -> DEBU 62d[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers ==
2019-01-29 11:22:06.326 UTC [msp] DeserializeIdentity -> INFO 62e[0m Obtaining identity
[36m2019-01-29 11:22:06.326 UTC [msp/identity] newIdentity -> DEBU 62f[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTDCCAfOgAwIBAgIQYZ1E6CL3TI5r9J0X5CY/6zAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMi5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMi5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB2MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzENMAsGA1UECxME
cGVlcjErMCkGA1UEAxMicGVlcjAuaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5jby56
YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABOoEzz0FFdYNv89IgW1kmLaRhSux
ZZyS6FfMn/s+eLJInhN8XZRcfo5jAL5wHQus9ka5err+7PFzn9+DQlEIb46jTTBL
MA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPECalij
2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQCIESI
6Ro/uHS6tC9ccjZumBNE+ffnKjI2UoZeL3zGRXVXAiBeirbdVLCZ46LhSPayifiC
LJj/EF+z8kPVLYe/SoDSRg==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:06.327 UTC [cauthdsl] func1 -> DEBU 630[0m 0xc42000f1f0 gate 1548760926327016953 evaluation starts
[36m2019-01-29 11:22:06.327 UTC [cauthdsl] func2 -> DEBU 631[0m 0xc42000f1f0 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:06.327 UTC [cauthdsl] func2 -> DEBU 632[0m 0xc42000f1f0 processing identity 0 with bytes of 0a0c486f73706974616c324d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435444434341664f674177494241674951595a314536434c3354493572394a30583543592f367a414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777794c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424f6f457a7a30464664594e763839496757316b6d4c6152685375780a5a5a79533646664d6e2f732b654c4a496e684e38585a5263666f356a414c357748517573396b61356572722b3750467a6e392b44516c45496234366a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149504543616c696a0a326c39754279446345557572434452557345512f754f4e503348457037785657686864754d416f4743437147534d343942414d43413063414d455143494553490a36526f2f7548533674433963636a5a756d424e452b66666e4b6a4932556f5a654c337a47525856584169426569726264564c435a34364c6853506179696669430a4c4a6a2f45462b7a386b50564c59652f536f445352673d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:06.327 UTC [cauthdsl] func2 -> DEBU 633[0m 0xc42000f1f0 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital3MSP, got Hospital2MSP)
[36m2019-01-29 11:22:06.327 UTC [cauthdsl] func2 -> DEBU 634[0m 0xc42000f1f0 principal evaluation fails
[36m2019-01-29 11:22:06.327 UTC [cauthdsl] func1 -> DEBU 635[0m 0xc42000f1f0 gate 1548760926327016953 evaluation fails
[36m2019-01-29 11:22:06.327 UTC [policies] Evaluate -> DEBU 636[0m Signature set did not satisfy policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:22:06.327 UTC [policies] Evaluate -> DEBU 637[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:22:06.327 UTC [policies] Evaluate -> DEBU 638[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers ==
[36m2019-01-29 11:22:06.327 UTC [cauthdsl] func1 -> DEBU 639[0m 0xc42000f208 gate 1548760926327392165 evaluation starts
[36m2019-01-29 11:22:06.327 UTC [cauthdsl] func2 -> DEBU 63a[0m 0xc42000f208 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:06.327 UTC [cauthdsl] func2 -> DEBU 63b[0m 0xc42000f208 processing identity 0 with bytes of 0a0c486f73706974616c324d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435444434341664f674177494241674951595a314536434c3354493572394a30583543592f367a414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777794c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424f6f457a7a30464664594e763839496757316b6d4c6152685375780a5a5a79533646664d6e2f732b654c4a496e684e38585a5263666f356a414c357748517573396b61356572722b3750467a6e392b44516c45496234366a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149504543616c696a0a326c39754279446345557572434452557345512f754f4e503348457037785657686864754d416f4743437147534d343942414d43413063414d455143494553490a36526f2f7548533674433963636a5a756d424e452b66666e4b6a4932556f5a654c337a47525856584169426569726264564c435a34364c6853506179696669430a4c4a6a2f45462b7a386b50564c59652f536f445352673d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:06.327 UTC [cauthdsl] func2 -> DEBU 63c[0m 0xc42000f208 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital1MSP, got Hospital2MSP)
[36m2019-01-29 11:22:06.327 UTC [cauthdsl] func2 -> DEBU 63d[0m 0xc42000f208 principal evaluation fails
[36m2019-01-29 11:22:06.327 UTC [cauthdsl] func1 -> DEBU 63e[0m 0xc42000f208 gate 1548760926327392165 evaluation fails
[36m2019-01-29 11:22:06.327 UTC [policies] Evaluate -> DEBU 63f[0m Signature set did not satisfy policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:22:06.327 UTC [policies] Evaluate -> DEBU 640[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:22:06.327 UTC [policies] Evaluate -> DEBU 641[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Readers ==
[36m2019-01-29 11:22:06.327 UTC [cauthdsl] func1 -> DEBU 642[0m 0xc42000f218 gate 1548760926327754289 evaluation starts
[36m2019-01-29 11:22:06.327 UTC [cauthdsl] func2 -> DEBU 643[0m 0xc42000f218 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:06.327 UTC [cauthdsl] func2 -> DEBU 644[0m 0xc42000f218 processing identity 0 with bytes of 0a0c486f73706974616c324d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435444434341664f674177494241674951595a314536434c3354493572394a30583543592f367a414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777794c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424f6f457a7a30464664594e763839496757316b6d4c6152685375780a5a5a79533646664d6e2f732b654c4a496e684e38585a5263666f356a414c357748517573396b61356572722b3750467a6e392b44516c45496234366a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149504543616c696a0a326c39754279446345557572434452557345512f754f4e503348457037785657686864754d416f4743437147534d343942414d43413063414d455143494553490a36526f2f7548533674433963636a5a756d424e452b66666e4b6a4932556f5a654c337a47525856584169426569726264564c435a34364c6853506179696669430a4c4a6a2f45462b7a386b50564c59652f536f445352673d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:06.327 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 645[0m Checking if identity satisfies MEMBER role for Hospital2MSP
[36m2019-01-29 11:22:06.327 UTC [msp] Validate -> DEBU 646[0m MSP Hospital2MSP validating identity
[36m2019-01-29 11:22:06.328 UTC [msp] getCertificationChain -> DEBU 647[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 11:22:06.328 UTC [cauthdsl] func2 -> DEBU 648[0m 0xc42000f218 principal matched by identity 0
[36m2019-01-29 11:22:06.328 UTC [msp/identity] Verify -> DEBU 649[0m Verify: digest = 00000000  76 23 69 11 6d b0 29 7c  82 09 7e a7 b2 2e e7 03  |v#i.m.)|..~.....|
00000010  96 f4 58 a2 6b 4f 33 b6  d4 0b 90 37 8e b6 d8 fe  |..X.kO3....7....|
[36m2019-01-29 11:22:06.328 UTC [msp/identity] Verify -> DEBU 64a[0m Verify: sig = 00000000  30 45 02 21 00 a5 39 97  6f 8a 30 42 89 db 48 e7  |0E.!..9.o.0B..H.|
00000010  40 0d 68 2c cd 2a 4e e0  55 7d 6d e7 d7 27 bf 97  |@.h,.*N.U}m..'..|
00000020  da 8e 53 6e c4 02 20 1d  41 e3 fc eb 8c 2c e2 db  |..Sn.. .A....,..|
00000030  15 ab a6 e7 bd e2 08 fa  0c 25 c1 03 e7 1e 60 3e  |.........%....`>|
00000040  a5 b1 85 5f 45 1e ea                              |..._E..|
[36m2019-01-29 11:22:06.329 UTC [cauthdsl] func2 -> DEBU 64b[0m 0xc42000f218 principal evaluation succeeds for identity 0
[36m2019-01-29 11:22:06.329 UTC [cauthdsl] func1 -> DEBU 64c[0m 0xc42000f218 gate 1548760926327754289 evaluation succeeds
[36m2019-01-29 11:22:06.329 UTC [policies] Evaluate -> DEBU 64d[0m Signature set satisfies policy /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:22:06.329 UTC [policies] Evaluate -> DEBU 64e[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:22:06.333 UTC [policies] Evaluate -> DEBU 64f[0m Signature set satisfies policy /Channel/Application/Readers
[36m2019-01-29 11:22:06.333 UTC [policies] Evaluate -> DEBU 650[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers
[36m2019-01-29 11:22:06.333 UTC [policies] Evaluate -> DEBU 651[0m Signature set satisfies policy /Channel/Readers
[36m2019-01-29 11:22:06.333 UTC [policies] Evaluate -> DEBU 652[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers
[36m2019-01-29 11:22:06.333 UTC [common/deliver] deliverBlocks -> DEBU 653[0m [channel: comunitychannel] Delivering block for (0xc42028ed00) for 10.0.0.39:47750
[36m2019-01-29 11:22:06.333 UTC [fsblkstorage] waitForBlock -> DEBU 654[0m Going to wait for newer blocks. maxAvailaBlockNumber=[2], waitForBlockNum=[3]
[36m2019-01-29 11:22:06.333 UTC [fsblkstorage] waitForBlock -> DEBU 655[0m Came out of wait. maxAvailaBlockNumber=[2]
[36m2019-01-29 11:22:06.333 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 656[0m Remaining bytes=[23709], Going to peek [8] bytes
[36m2019-01-29 11:22:06.334 UTC [fsblkstorage] waitForBlock -> DEBU 658[0m Came out of wait. maxAvailaBlockNumber=[2]
[36m2019-01-29 11:22:06.334 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 657[0m Returning blockbytes - length=[23706], placementInfo={fileNum=[0], startOffset=[45385], bytesOffset=[45388]}
[36m2019-01-29 11:22:06.334 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 65a[0m blockbytes [23706] read from file [0]
[36m2019-01-29 11:22:06.334 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 659[0m Remaining bytes=[23709], Going to peek [8] bytes
[36m2019-01-29 11:22:06.334 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 65b[0m Returning blockbytes - length=[23706], placementInfo={fileNum=[0], startOffset=[45385], bytesOffset=[45388]}
[36m2019-01-29 11:22:06.334 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 65c[0m blockbytes [23706] read from file [0]
[36m2019-01-29 11:22:06.334 UTC [policies] Evaluate -> DEBU 65d[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers ==
[36m2019-01-29 11:22:06.334 UTC [policies] Evaluate -> DEBU 65e[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:06.334 UTC [policies] Evaluate -> DEBU 65f[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers ==
[36m2019-01-29 11:22:06.334 UTC [policies] Evaluate -> DEBU 660[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:06.334 UTC [policies] Evaluate -> DEBU 662[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers ==
[36m2019-01-29 11:22:06.334 UTC [policies] Evaluate -> DEBU 661[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers ==
2019-01-29 11:22:06.334 UTC [msp] DeserializeIdentity -> INFO 663[0m Obtaining identity
[36m2019-01-29 11:22:06.334 UTC [policies] Evaluate -> DEBU 664[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:06.334 UTC [policies] Evaluate -> DEBU 665[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers ==
[36m2019-01-29 11:22:06.334 UTC [policies] Evaluate -> DEBU 666[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:06.334 UTC [policies] Evaluate -> DEBU 667[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers ==
2019-01-29 11:22:06.334 UTC [msp] DeserializeIdentity -> INFO 668[0m Obtaining identity
[36m2019-01-29 11:22:06.334 UTC [msp/identity] newIdentity -> DEBU 669[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTTCCAfOgAwIBAgIQDPIhkZbBMzAPRV2mdTJ2BTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB2MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzENMAsGA1UECxME
cGVlcjErMCkGA1UEAxMicGVlcjAuaG9zcGl0YWwxLnN3aXRjaDJsb2dpYy5jby56
YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABMHSfENe+q4SvhxLb6FxLr2SLYg+
qJnL3whOrSDaEyQgmDz7fwbLhWr0Mapiq8N3Xg1vudI6XbNZxyMTBmZbd3ujTTBL
MA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIFVTsOg/
0yx/+c1M2XrkXHABdnzZ5oYFPvNZh3KHhBsGMAoGCCqGSM49BAMCA0gAMEUCIQDn
gu8tRu0NvsaTNG17i4QtjRftqg56ptadF5YHXBQCGgIgeowGfoCWCIW5T09110Gv
cu4tVgSe4rBzE4yjal6cQVQ=
-----END CERTIFICATE-----
[36m2019-01-29 11:22:06.335 UTC [msp/identity] newIdentity -> DEBU 66a[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTjCCAfSgAwIBAgIRAKAnE4PbDWMeRl0DixdLiVswCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowdjELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDTALBgNVBAsT
BHBlZXIxKzApBgNVBAMTInBlZXIwLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28u
emEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAASAvWW2l9XjwfiMiZ6Pt8Qrk81E
DaIfG8hRpCV7KHzrHmpiy3y88PlgzQhh3sQ5iYqAqw0Qjz6BrEbhedGqpv6Po00w
SzAOBgNVHQ8BAf8EBAMCB4AwDAYDVR0TAQH/BAIwADArBgNVHSMEJDAigCA7vVWF
XWpgZBojQ1ZLxUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEA
2E0537skl5yXkVKSjrjr0UJC3GRPT8czRln3F2NB2ygCIApG3olic8fvYAJspMQH
92FzZlNWQ3BWDB4d6dq3Ae+/
-----END CERTIFICATE-----
[36m2019-01-29 11:22:06.335 UTC [cauthdsl] func1 -> DEBU 66b[0m 0xc420256040 gate 1548760926335333023 evaluation starts
[36m2019-01-29 11:22:06.335 UTC [cauthdsl] func2 -> DEBU 66c[0m 0xc420256040 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:06.335 UTC [cauthdsl] func2 -> DEBU 66d[0m 0xc420256040 processing identity 0 with bytes of 0a0c486f73706974616c314d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435454434341664f674177494241674951445049686b5a62424d7a41505256326d64544a324254414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777784c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424d485366454e652b7134537668784c623646784c7232534c59672b0a714a6e4c3377684f72534461457951676d447a376677624c685772304d61706971384e33586731767564493658624e5a78794d54426d5a626433756a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149465654734f672f0a3079782f2b63314d3258726b58484142646e7a5a356f594650764e5a68334b48684273474d416f4743437147534d343942414d43413067414d4555434951446e0a677538745275304e767361544e473137693451746a5266747167353670746164463559485842514347674967656f7747666f43574349573554303931313047760a63753474566753653472427a4534796a616c36635156513d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:06.335 UTC [cauthdsl] func1 -> DEBU 66e[0m 0xc42000f250 gate 1548760926335429475 evaluation starts
[36m2019-01-29 11:22:06.335 UTC [cauthdsl] func2 -> DEBU 670[0m 0xc42000f250 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:06.335 UTC [cauthdsl] func2 -> DEBU 671[0m 0xc42000f250 processing identity 0 with bytes of 0a0c486f73706974616c334d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943546a4343416653674177494241674952414b416e4534506244574d65526c30446978644c69567377436759494b6f5a497a6a304541774977675973780a437a414a42674e5642415954416c56544d524d77455159445651514945777044595778705a6d3979626d6c684d525977464159445651514845773154595734670a526e4a68626d4e7063324e764d535577497759445651514b4578786f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70680a4d5367774a675944565151444578396a5953356f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70684d423458445445350a4d4445794f5441334e4445314e466f58445449354d4445794e6a41334e4445314e466f77646a454c4d416b474131554542684d4356564d78457a415242674e560a42416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e6862694247636d467559326c7a593238784454414c42674e56424173540a4248426c5a5849784b7a417042674e5642414d54496e426c5a5849774c6d687663334270644746734d79357a64326c30593267796247396e61574d75593238750a656d45775754415442676371686b6a4f5051494242676771686b6a4f50514d4242774e4341415341765757326c39586a7766694d695a3650743851726b3831450a4461496647386852704356374b487a72486d70697933793838506c677a5168683373513569597141717730516a7a36427245626865644771707636506f3030770a537a414f42674e56485138424166384542414d434234417744415944565230544151482f424149774144417242674e5648534d454a44416967434137765657460a585770675a426f6a51315a4c78557667424c65696135394d69764952636a786d59616e707954414b42676771686b6a4f5051514441674e4941444246416945410a324530353337736b6c3579586b564b536a726a7230554a43334752505438637a526c6e3346324e423279674349417047336f6c696338667659414a73704d51480a3932467a5a6c4e5751334257444234643664713341652b2f0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:06.335 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 672[0m Checking if identity satisfies MEMBER role for Hospital3MSP
[36m2019-01-29 11:22:06.335 UTC [msp] Validate -> DEBU 673[0m MSP Hospital3MSP validating identity
[36m2019-01-29 11:22:06.336 UTC [msp] getCertificationChain -> DEBU 674[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 11:22:06.336 UTC [cauthdsl] func2 -> DEBU 675[0m 0xc42000f250 principal matched by identity 0
[36m2019-01-29 11:22:06.336 UTC [msp/identity] Verify -> DEBU 676[0m Verify: digest = 00000000  e8 ed 40 b8 9e af b2 ef  06 7b 8c 10 da 7a e2 8d  |..@......{...z..|
00000010  b6 64 38 ca 51 7f 36 1a  cc a8 45 f2 4d 5d d3 49  |.d8.Q.6...E.M].I|
[36m2019-01-29 11:22:06.336 UTC [msp/identity] Verify -> DEBU 677[0m Verify: sig = 00000000  30 45 02 21 00 cf 67 37  c5 fb 18 5d e8 8e ff 76  |0E.!..g7...]...v|
00000010  16 89 7b d5 ad fd 0d ea  64 bc fd a0 19 ee 91 80  |..{.....d.......|
00000020  25 3f 3d 8b d3 02 20 40  ee 79 e7 36 16 b4 15 0c  |%?=... @.y.6....|
00000030  be 6e e7 19 98 3a 3a 54  51 5b ec 7e ff ef 8c 43  |.n...::TQ[.~...C|
00000040  98 48 cd f9 75 a5 31                              |.H..u.1|
[36m2019-01-29 11:22:06.336 UTC [cauthdsl] func2 -> DEBU 678[0m 0xc42000f250 principal evaluation succeeds for identity 0
[36m2019-01-29 11:22:06.336 UTC [cauthdsl] func1 -> DEBU 679[0m 0xc42000f250 gate 1548760926335429475 evaluation succeeds
[36m2019-01-29 11:22:06.337 UTC [policies] Evaluate -> DEBU 67a[0m Signature set satisfies policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:22:06.337 UTC [policies] Evaluate -> DEBU 67b[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:22:06.337 UTC [policies] Evaluate -> DEBU 67c[0m Signature set satisfies policy /Channel/Application/Readers
[36m2019-01-29 11:22:06.337 UTC [policies] Evaluate -> DEBU 67d[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers
[36m2019-01-29 11:22:06.337 UTC [policies] Evaluate -> DEBU 67e[0m Signature set satisfies policy /Channel/Readers
[36m2019-01-29 11:22:06.337 UTC [policies] Evaluate -> DEBU 67f[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers
[36m2019-01-29 11:22:06.337 UTC [common/deliver] deliverBlocks -> DEBU 680[0m [channel: comunitychannel] Delivering block for (0xc42024a5a0) for 10.0.0.18:59908
[36m2019-01-29 11:22:06.337 UTC [fsblkstorage] waitForBlock -> DEBU 681[0m Going to wait for newer blocks. maxAvailaBlockNumber=[2], waitForBlockNum=[3]
[36m2019-01-29 11:22:06.335 UTC [cauthdsl] func2 -> DEBU 66f[0m 0xc420256040 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital3MSP, got Hospital1MSP)
[36m2019-01-29 11:22:06.337 UTC [cauthdsl] func2 -> DEBU 682[0m 0xc420256040 principal evaluation fails
[36m2019-01-29 11:22:06.337 UTC [cauthdsl] func1 -> DEBU 683[0m 0xc420256040 gate 1548760926335333023 evaluation fails
[36m2019-01-29 11:22:06.337 UTC [policies] Evaluate -> DEBU 684[0m Signature set did not satisfy policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:22:06.337 UTC [policies] Evaluate -> DEBU 685[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:22:06.337 UTC [policies] Evaluate -> DEBU 686[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers ==
[36m2019-01-29 11:22:06.337 UTC [cauthdsl] func1 -> DEBU 687[0m 0xc42000f280 gate 1548760926337344628 evaluation starts
[36m2019-01-29 11:22:06.337 UTC [cauthdsl] func2 -> DEBU 688[0m 0xc42000f280 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:06.337 UTC [cauthdsl] func2 -> DEBU 689[0m 0xc42000f280 processing identity 0 with bytes of 0a0c486f73706974616c314d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435454434341664f674177494241674951445049686b5a62424d7a41505256326d64544a324254414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777784c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424d485366454e652b7134537668784c623646784c7232534c59672b0a714a6e4c3377684f72534461457951676d447a376677624c685772304d61706971384e33586731767564493658624e5a78794d54426d5a626433756a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149465654734f672f0a3079782f2b63314d3258726b58484142646e7a5a356f594650764e5a68334b48684273474d416f4743437147534d343942414d43413067414d4555434951446e0a677538745275304e767361544e473137693451746a5266747167353670746164463559485842514347674967656f7747666f43574349573554303931313047760a63753474566753653472427a4534796a616c36635156513d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:06.337 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 68a[0m Checking if identity satisfies MEMBER role for Hospital1MSP
[36m2019-01-29 11:22:06.337 UTC [msp] Validate -> DEBU 68b[0m MSP Hospital1MSP validating identity
[36m2019-01-29 11:22:06.337 UTC [msp] getCertificationChain -> DEBU 68c[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 11:22:06.338 UTC [cauthdsl] func2 -> DEBU 68d[0m 0xc42000f280 principal matched by identity 0
[36m2019-01-29 11:22:06.338 UTC [msp/identity] Verify -> DEBU 68e[0m Verify: digest = 00000000  32 4a 77 18 98 fe c5 2b  5c 1b b2 b7 16 e6 8b a0  |2Jw....+\.......|
00000010  3a 68 8a 92 b9 2c 2d c1  74 ad f1 95 77 0b 20 c4  |:h...,-.t...w. .|
[36m2019-01-29 11:22:06.338 UTC [msp/identity] Verify -> DEBU 68f[0m Verify: sig = 00000000  30 45 02 21 00 f2 71 66  5d d5 91 1f 73 38 68 65  |0E.!..qf]...s8he|
00000010  ef a6 15 ca f0 99 09 57  ac 81 30 6f 20 6c 6d f1  |.......W..0o lm.|
00000020  61 9f 5b c5 a3 02 20 6b  74 ab 2e a3 ea cf 54 16  |a.[... kt.....T.|
00000030  21 36 de 34 b2 84 d8 71  74 fb a7 8b 10 77 f0 de  |!6.4...qt....w..|
00000040  e5 74 af 6c 7d 89 a6                              |.t.l}..|
[36m2019-01-29 11:22:06.338 UTC [cauthdsl] func2 -> DEBU 690[0m 0xc42000f280 principal evaluation succeeds for identity 0
[36m2019-01-29 11:22:06.338 UTC [cauthdsl] func1 -> DEBU 691[0m 0xc42000f280 gate 1548760926337344628 evaluation succeeds
[36m2019-01-29 11:22:06.338 UTC [policies] Evaluate -> DEBU 692[0m Signature set satisfies policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:22:06.338 UTC [policies] Evaluate -> DEBU 693[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:22:06.338 UTC [policies] Evaluate -> DEBU 694[0m Signature set satisfies policy /Channel/Application/Readers
[36m2019-01-29 11:22:06.338 UTC [policies] Evaluate -> DEBU 695[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers
[36m2019-01-29 11:22:06.338 UTC [policies] Evaluate -> DEBU 696[0m Signature set satisfies policy /Channel/Readers
[36m2019-01-29 11:22:06.338 UTC [policies] Evaluate -> DEBU 697[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers
[36m2019-01-29 11:22:06.338 UTC [common/deliver] deliverBlocks -> DEBU 698[0m [channel: comunitychannel] Delivering block for (0xc42024bba0) for 10.0.0.34:59894
[36m2019-01-29 11:22:06.338 UTC [fsblkstorage] waitForBlock -> DEBU 699[0m Going to wait for newer blocks. maxAvailaBlockNumber=[2], waitForBlockNum=[3]
[36m2019-01-29 11:22:46.157 UTC [orderer/common/server] Deliver -> DEBU 69a[0m Starting new Deliver handler
[36m2019-01-29 11:22:46.157 UTC [common/deliver] Handle -> DEBU 69b[0m Starting new deliver loop for 10.0.0.9:46804
[36m2019-01-29 11:22:46.157 UTC [common/deliver] Handle -> DEBU 69c[0m Attempting to read seek info message from 10.0.0.9:46804
[36m2019-01-29 11:22:46.161 UTC [orderer/common/server] Broadcast -> DEBU 69d[0m Starting new Broadcast handler
[36m2019-01-29 11:22:46.161 UTC [orderer/common/broadcast] Handle -> DEBU 69e[0m Starting new broadcast loop for 10.0.0.9:46806
[36m2019-01-29 11:22:46.162 UTC [orderer/common/broadcast] Handle -> DEBU 69f[0m [channel: comunitychannel] Broadcast is processing config update message from 10.0.0.9:46806
[36m2019-01-29 11:22:46.162 UTC [orderer/common/msgprocessor] ProcessConfigUpdateMsg -> DEBU 6a0[0m Processing config update message for channel comunitychannel
[36m2019-01-29 11:22:46.162 UTC [policies] Evaluate -> DEBU 6a1[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Writers ==
[36m2019-01-29 11:22:46.162 UTC [policies] Evaluate -> DEBU 6a2[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:46.162 UTC [policies] Evaluate -> DEBU 6a3[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Writers ==
[36m2019-01-29 11:22:46.162 UTC [policies] Evaluate -> DEBU 6a4[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:46.162 UTC [policies] Evaluate -> DEBU 6a5[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Writers ==
2019-01-29 11:22:46.162 UTC [msp] DeserializeIdentity -> INFO 6a6[0m Obtaining identity
[36m2019-01-29 11:22:46.162 UTC [msp/identity] newIdentity -> DEBU 6a7[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.163 UTC [cauthdsl] func1 -> DEBU 6a8[0m 0xc42000f2b8 gate 1548760966163295210 evaluation starts
[36m2019-01-29 11:22:46.163 UTC [cauthdsl] func2 -> DEBU 6a9[0m 0xc42000f2b8 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.163 UTC [cauthdsl] func2 -> DEBU 6aa[0m 0xc42000f2b8 processing identity 0 with bytes of 0a0c486f73706974616c314d535012df062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943547a4343416657674177494241674951627a7870534b69754a437a4469446c5a702b58625744414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42344d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a45504d4130474131554543784d470a593278705a5735304d5373774b5159445651514444434a425a473170626b426f62334e7761585268624445756333647064474e6f4d6d78765a326c6a4c6d4e760a4c6e70684d466b77457759484b6f5a497a6a3043415159494b6f5a497a6a304441516344516741456e7041446c382b6155426356577636713349674d513479670a49536177513077616d486d7053762b2f62314235686634563248796f50726177497234352b6f342f517a2b2b57617a5a6c795937506b6f54323772374b614e4e0a4d45737744675944565230504151482f42415144416765414d41774741315564457745422f7751434d4141774b7759445652306a42435177496f416756564f770a36442f544c482f357a557a5a6575526363414632664e6e6d6867552b38316d48636f654547775977436759494b6f5a497a6a30454177494453414177525149680a414c6176644d6d3969367853736e57646c6c694d6831307336646d474261584f30437861436f774c4a494d744169424564757564474a796d444445317a6269580a59775838336359575a4855635956384958655a5046796d432b773d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.163 UTC [cauthdsl] func2 -> DEBU 6ab[0m 0xc42000f2b8 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital2MSP, got Hospital1MSP)
[36m2019-01-29 11:22:46.163 UTC [cauthdsl] func2 -> DEBU 6ac[0m 0xc42000f2b8 principal evaluation fails
[36m2019-01-29 11:22:46.163 UTC [cauthdsl] func1 -> DEBU 6ad[0m 0xc42000f2b8 gate 1548760966163295210 evaluation fails
[36m2019-01-29 11:22:46.163 UTC [policies] Evaluate -> DEBU 6ae[0m Signature set did not satisfy policy /Channel/Application/Hospital2MSP/Writers
[36m2019-01-29 11:22:46.163 UTC [policies] Evaluate -> DEBU 6af[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Writers
[36m2019-01-29 11:22:46.163 UTC [policies] Evaluate -> DEBU 6b0[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Writers ==
[36m2019-01-29 11:22:46.163 UTC [cauthdsl] func1 -> DEBU 6b1[0m 0xc42000f2d0 gate 1548760966163615817 evaluation starts
[36m2019-01-29 11:22:46.164 UTC [cauthdsl] func2 -> DEBU 6b2[0m 0xc42000f2d0 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.164 UTC [cauthdsl] func2 -> DEBU 6b3[0m 0xc42000f2d0 processing identity 0 with bytes of 0a0c486f73706974616c314d535012df062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943547a4343416657674177494241674951627a7870534b69754a437a4469446c5a702b58625744414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42344d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a45504d4130474131554543784d470a593278705a5735304d5373774b5159445651514444434a425a473170626b426f62334e7761585268624445756333647064474e6f4d6d78765a326c6a4c6d4e760a4c6e70684d466b77457759484b6f5a497a6a3043415159494b6f5a497a6a304441516344516741456e7041446c382b6155426356577636713349674d513479670a49536177513077616d486d7053762b2f62314235686634563248796f50726177497234352b6f342f517a2b2b57617a5a6c795937506b6f54323772374b614e4e0a4d45737744675944565230504151482f42415144416765414d41774741315564457745422f7751434d4141774b7759445652306a42435177496f416756564f770a36442f544c482f357a557a5a6575526363414632664e6e6d6867552b38316d48636f654547775977436759494b6f5a497a6a30454177494453414177525149680a414c6176644d6d3969367853736e57646c6c694d6831307336646d474261584f30437861436f774c4a494d744169424564757564474a796d444445317a6269580a59775838336359575a4855635956384958655a5046796d432b773d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.164 UTC [cauthdsl] func2 -> DEBU 6b4[0m 0xc42000f2d0 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital3MSP, got Hospital1MSP)
[36m2019-01-29 11:22:46.164 UTC [cauthdsl] func2 -> DEBU 6b5[0m 0xc42000f2d0 principal evaluation fails
[36m2019-01-29 11:22:46.164 UTC [cauthdsl] func1 -> DEBU 6b6[0m 0xc42000f2d0 gate 1548760966163615817 evaluation fails
[36m2019-01-29 11:22:46.164 UTC [policies] Evaluate -> DEBU 6b7[0m Signature set did not satisfy policy /Channel/Application/Hospital3MSP/Writers
[36m2019-01-29 11:22:46.164 UTC [policies] Evaluate -> DEBU 6b8[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Writers
[36m2019-01-29 11:22:46.164 UTC [policies] Evaluate -> DEBU 6b9[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Writers ==
[36m2019-01-29 11:22:46.164 UTC [cauthdsl] func1 -> DEBU 6ba[0m 0xc42000f2e8 gate 1548760966164707449 evaluation starts
[36m2019-01-29 11:22:46.164 UTC [cauthdsl] func2 -> DEBU 6bb[0m 0xc42000f2e8 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.164 UTC [cauthdsl] func2 -> DEBU 6bc[0m 0xc42000f2e8 processing identity 0 with bytes of 0a0c486f73706974616c314d535012df062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943547a4343416657674177494241674951627a7870534b69754a437a4469446c5a702b58625744414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42344d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a45504d4130474131554543784d470a593278705a5735304d5373774b5159445651514444434a425a473170626b426f62334e7761585268624445756333647064474e6f4d6d78765a326c6a4c6d4e760a4c6e70684d466b77457759484b6f5a497a6a3043415159494b6f5a497a6a304441516344516741456e7041446c382b6155426356577636713349674d513479670a49536177513077616d486d7053762b2f62314235686634563248796f50726177497234352b6f342f517a2b2b57617a5a6c795937506b6f54323772374b614e4e0a4d45737744675944565230504151482f42415144416765414d41774741315564457745422f7751434d4141774b7759445652306a42435177496f416756564f770a36442f544c482f357a557a5a6575526363414632664e6e6d6867552b38316d48636f654547775977436759494b6f5a497a6a30454177494453414177525149680a414c6176644d6d3969367853736e57646c6c694d6831307336646d474261584f30437861436f774c4a494d744169424564757564474a796d444445317a6269580a59775838336359575a4855635956384958655a5046796d432b773d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.165 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 6bd[0m Checking if identity satisfies MEMBER role for Hospital1MSP
[36m2019-01-29 11:22:46.165 UTC [msp] Validate -> DEBU 6be[0m MSP Hospital1MSP validating identity
[36m2019-01-29 11:22:46.165 UTC [msp] getCertificationChain -> DEBU 6bf[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 11:22:46.166 UTC [cauthdsl] func2 -> DEBU 6c0[0m 0xc42000f2e8 principal matched by identity 0
[36m2019-01-29 11:22:46.166 UTC [msp/identity] Verify -> DEBU 6c1[0m Verify: digest = 00000000  e5 ca 00 8e f7 20 12 e5  3b 03 b9 be c7 6e 15 57  |..... ..;....n.W|
00000010  8e 32 30 63 f8 b6 52 1f  ea 40 47 44 d1 a1 9e 18  |.20c..R..@GD....|
[36m2019-01-29 11:22:46.166 UTC [msp/identity] Verify -> DEBU 6c2[0m Verify: sig = 00000000  30 45 02 21 00 cb 6d d4  36 8c 49 e0 14 b1 9c b1  |0E.!..m.6.I.....|
00000010  49 57 32 dd a9 96 d6 26  1a 50 85 68 2a 96 30 5e  |IW2....&.P.h*.0^|
00000020  3d 70 c0 9c 54 02 20 7d  1b 4e fc 11 0b 37 be 2e  |=p..T. }.N...7..|
00000030  19 36 90 60 e0 c2 e0 19  4a 87 e1 97 e2 1b 57 67  |.6.`....J.....Wg|
00000040  d4 03 62 28 3b b8 6b                              |..b(;.k|
[36m2019-01-29 11:22:46.166 UTC [cauthdsl] func2 -> DEBU 6c3[0m 0xc42000f2e8 principal evaluation succeeds for identity 0
[36m2019-01-29 11:22:46.166 UTC [cauthdsl] func1 -> DEBU 6c4[0m 0xc42000f2e8 gate 1548760966164707449 evaluation succeeds
[36m2019-01-29 11:22:46.166 UTC [policies] Evaluate -> DEBU 6c5[0m Signature set satisfies policy /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 11:22:46.166 UTC [policies] Evaluate -> DEBU 6c6[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 11:22:46.166 UTC [policies] Evaluate -> DEBU 6c7[0m Signature set satisfies policy /Channel/Application/Writers
[36m2019-01-29 11:22:46.166 UTC [policies] Evaluate -> DEBU 6c8[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Writers
[36m2019-01-29 11:22:46.166 UTC [policies] Evaluate -> DEBU 6c9[0m Signature set satisfies policy /Channel/Writers
[36m2019-01-29 11:22:46.166 UTC [policies] Evaluate -> DEBU 6ca[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Writers
[36m2019-01-29 11:22:46.167 UTC [common/configtx] addToMap -> DEBU 6cb[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 11:22:46.167 UTC [common/configtx] addToMap -> DEBU 6cc[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 11:22:46.167 UTC [common/configtx] addToMap -> DEBU 6cd[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:46.167 UTC [common/configtx] addToMap -> DEBU 6ce[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/MSP
[36m2019-01-29 11:22:46.167 UTC [common/configtx] addToMap -> DEBU 6cf[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 11:22:46.167 UTC [common/configtx] addToMap -> DEBU 6d0[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:22:46.167 UTC [common/configtx] addToMap -> DEBU 6d1[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 11:22:46.167 UTC [common/configtx] addToMap -> DEBU 6d2[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 11:22:46.167 UTC [common/configtx] addToMap -> DEBU 6d3[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 11:22:46.167 UTC [common/configtx] addToMap -> DEBU 6d4[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:46.167 UTC [common/configtx] addToMap -> DEBU 6d5[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/MSP
[36m2019-01-29 11:22:46.167 UTC [common/configtx] addToMap -> DEBU 6d6[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/AnchorPeers
[36m2019-01-29 11:22:46.168 UTC [common/configtx] addToMap -> DEBU 6d7[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:22:46.168 UTC [common/configtx] addToMap -> DEBU 6d8[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 11:22:46.168 UTC [common/configtx] addToMap -> DEBU 6d9[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 11:22:46.168 UTC [common/configtx] verifyDeltaSet -> DEBU 6da[0m Processing change to key: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:46.175 UTC [common/configtx] policyForItem -> DEBU 6db[0m Getting policy for item Hospital1MSP with mod_policy Admins
[36m2019-01-29 11:22:46.175 UTC [policies] Manager -> DEBU 6dc[0m Manager Channel looking up path [Application]
[36m2019-01-29 11:22:46.175 UTC [policies] Manager -> DEBU 6dd[0m Manager Channel has managers Application
[36m2019-01-29 11:22:46.175 UTC [policies] Manager -> DEBU 6de[0m Manager Channel has managers Orderer
[36m2019-01-29 11:22:46.175 UTC [policies] Manager -> DEBU 6df[0m Manager Channel/Application looking up path []
[36m2019-01-29 11:22:46.175 UTC [policies] Manager -> DEBU 6e0[0m Manager Channel/Application has managers Hospital2MSP
[36m2019-01-29 11:22:46.175 UTC [policies] Manager -> DEBU 6e1[0m Manager Channel/Application has managers Hospital3MSP
[36m2019-01-29 11:22:46.175 UTC [policies] Manager -> DEBU 6e2[0m Manager Channel/Application has managers Hospital1MSP
[36m2019-01-29 11:22:46.175 UTC [policies] Manager -> DEBU 6e3[0m Manager Channel/Application looking up path [Hospital1MSP]
[36m2019-01-29 11:22:46.175 UTC [policies] Manager -> DEBU 6e4[0m Manager Channel/Application has managers Hospital2MSP
[36m2019-01-29 11:22:46.175 UTC [policies] Manager -> DEBU 6e5[0m Manager Channel/Application has managers Hospital3MSP
[36m2019-01-29 11:22:46.175 UTC [policies] Manager -> DEBU 6e6[0m Manager Channel/Application has managers Hospital1MSP
[36m2019-01-29 11:22:46.175 UTC [policies] Manager -> DEBU 6e7[0m Manager Channel/Application/Hospital1MSP looking up path []
[36m2019-01-29 11:22:46.175 UTC [policies] Evaluate -> DEBU 6e8[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Admins ==
[36m2019-01-29 11:22:46.175 UTC [cauthdsl] func1 -> DEBU 6e9[0m 0xc4202562e0 gate 1548760966175562252 evaluation starts
[36m2019-01-29 11:22:46.175 UTC [cauthdsl] func2 -> DEBU 6ea[0m 0xc4202562e0 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.175 UTC [cauthdsl] func2 -> DEBU 6eb[0m 0xc4202562e0 processing identity 0 with bytes of 0a0c486f73706974616c314d535012df062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943547a4343416657674177494241674951627a7870534b69754a437a4469446c5a702b58625744414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42344d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a45504d4130474131554543784d470a593278705a5735304d5373774b5159445651514444434a425a473170626b426f62334e7761585268624445756333647064474e6f4d6d78765a326c6a4c6d4e760a4c6e70684d466b77457759484b6f5a497a6a3043415159494b6f5a497a6a304441516344516741456e7041446c382b6155426356577636713349674d513479670a49536177513077616d486d7053762b2f62314235686634563248796f50726177497234352b6f342f517a2b2b57617a5a6c795937506b6f54323772374b614e4e0a4d45737744675944565230504151482f42415144416765414d41774741315564457745422f7751434d4141774b7759445652306a42435177496f416756564f770a36442f544c482f357a557a5a6575526363414632664e6e6d6867552b38316d48636f654547775977436759494b6f5a497a6a30454177494453414177525149680a414c6176644d6d3969367853736e57646c6c694d6831307336646d474261584f30437861436f774c4a494d744169424564757564474a796d444445317a6269580a59775838336359575a4855635956384958655a5046796d432b773d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.175 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 6ec[0m Checking if identity satisfies ADMIN role for Hospital1MSP
[36m2019-01-29 11:22:46.175 UTC [cauthdsl] func2 -> DEBU 6ed[0m 0xc4202562e0 principal matched by identity 0
[36m2019-01-29 11:22:46.175 UTC [msp/identity] Verify -> DEBU 6ee[0m Verify: digest = 00000000  2a b7 e0 fc 5e 0e fa d9  8c 68 56 4b 83 df 68 8e  |*...^....hVK..h.|
00000010  52 74 fd e1 61 6d b6 a3  f4 b4 13 5e cc ae 0c d6  |Rt..am.....^....|
[36m2019-01-29 11:22:46.175 UTC [msp/identity] Verify -> DEBU 6ef[0m Verify: sig = 00000000  30 45 02 21 00 fd 8b 6f  88 cf 7e 8b af 32 6e df  |0E.!...o..~..2n.|
00000010  f3 0a 58 cc 86 42 b1 60  8e b1 e5 6d 6d ee 48 57  |..X..B.`...mm.HW|
00000020  2e b1 97 f9 1a 02 20 32  72 f0 68 23 a9 15 3f 45  |...... 2r.h#..?E|
00000030  31 84 b4 61 dc f1 5d b0  a5 e6 ec 01 f9 b5 5b 97  |1..a..].......[.|
00000040  77 8e 39 8a 75 b5 cc                              |w.9.u..|
[36m2019-01-29 11:22:46.176 UTC [cauthdsl] func2 -> DEBU 6f0[0m 0xc4202562e0 principal evaluation succeeds for identity 0
[36m2019-01-29 11:22:46.176 UTC [cauthdsl] func1 -> DEBU 6f1[0m 0xc4202562e0 gate 1548760966175562252 evaluation succeeds
[36m2019-01-29 11:22:46.176 UTC [policies] Evaluate -> DEBU 6f2[0m Signature set satisfies policy /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 11:22:46.176 UTC [policies] Evaluate -> DEBU 6f3[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 11:22:46.176 UTC [common/configtx] verifyDeltaSet -> DEBU 6f4[0m Processing change to key: [Value]  /Channel/Application/Hospital1MSP/AnchorPeers
[36m2019-01-29 11:22:46.176 UTC [common/configtx] recurseConfigMap -> DEBU 6f5[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital2MSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.176 UTC [common/configtx] recurseConfigMap -> DEBU 6f6[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital2MSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.176 UTC [common/configtx] recurseConfigMap -> DEBU 6f7[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\022\022\020\n\014Hospital2MSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.176 UTC [common/configtx] recurseConfigMap -> DEBU 6f8[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital3MSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.176 UTC [common/configtx] recurseConfigMap -> DEBU 6f9[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\022\022\020\n\014Hospital3MSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.176 UTC [common/configtx] recurseConfigMap -> DEBU 6fa[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital3MSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.176 UTC [common/configtx] recurseConfigMap -> DEBU 6fb[0m Setting policy for key Admins to 
[36m2019-01-29 11:22:46.176 UTC [common/configtx] recurseConfigMap -> DEBU 6fc[0m Setting policy for key Readers to 
[36m2019-01-29 11:22:46.176 UTC [common/configtx] recurseConfigMap -> DEBU 6fd[0m Setting policy for key Writers to 
[36m2019-01-29 11:22:46.177 UTC [common/configtx] recurseConfigMap -> DEBU 6fe[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.177 UTC [common/configtx] recurseConfigMap -> DEBU 6ff[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.177 UTC [common/configtx] recurseConfigMap -> DEBU 700[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.177 UTC [common/configtx] recurseConfigMap -> DEBU 701[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\nOrdererMSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.177 UTC [common/configtx] recurseConfigMap -> DEBU 702[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\016\022\014\n\nOrdererMSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.177 UTC [common/configtx] recurseConfigMap -> DEBU 703[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\016\022\014\n\nOrdererMSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.177 UTC [common/configtx] recurseConfigMap -> DEBU 704[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.177 UTC [common/configtx] recurseConfigMap -> DEBU 705[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.177 UTC [common/configtx] recurseConfigMap -> DEBU 706[0m Setting policy for key BlockValidation to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.177 UTC [common/configtx] recurseConfigMap -> DEBU 707[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.177 UTC [common/configtx] recurseConfigMap -> DEBU 708[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.177 UTC [common/configtx] recurseConfigMap -> DEBU 709[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.177 UTC [common/configtx] recurseConfigMap -> DEBU 70a[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.177 UTC [common/channelconfig] NewStandardValues -> DEBU 70b[0m Initializing protos for *channelconfig.ChannelProtos
[36m2019-01-29 11:22:46.177 UTC [common/channelconfig] initializeProtosStruct -> DEBU 70c[0m Processing field: HashingAlgorithm
[36m2019-01-29 11:22:46.177 UTC [common/channelconfig] initializeProtosStruct -> DEBU 70d[0m Processing field: BlockDataHashingStructure
[36m2019-01-29 11:22:46.177 UTC [common/channelconfig] initializeProtosStruct -> DEBU 70e[0m Processing field: OrdererAddresses
[36m2019-01-29 11:22:46.177 UTC [common/channelconfig] initializeProtosStruct -> DEBU 70f[0m Processing field: Consortium
[36m2019-01-29 11:22:46.177 UTC [common/channelconfig] initializeProtosStruct -> DEBU 710[0m Processing field: Capabilities
[36m2019-01-29 11:22:46.177 UTC [common/channelconfig] NewStandardValues -> DEBU 711[0m Initializing protos for *channelconfig.OrdererProtos
[36m2019-01-29 11:22:46.177 UTC [common/channelconfig] initializeProtosStruct -> DEBU 712[0m Processing field: ConsensusType
[36m2019-01-29 11:22:46.177 UTC [common/channelconfig] initializeProtosStruct -> DEBU 713[0m Processing field: BatchSize
[36m2019-01-29 11:22:46.177 UTC [common/channelconfig] initializeProtosStruct -> DEBU 714[0m Processing field: BatchTimeout
[36m2019-01-29 11:22:46.177 UTC [common/channelconfig] initializeProtosStruct -> DEBU 715[0m Processing field: KafkaBrokers
[36m2019-01-29 11:22:46.177 UTC [common/channelconfig] initializeProtosStruct -> DEBU 716[0m Processing field: ChannelRestrictions
[36m2019-01-29 11:22:46.177 UTC [common/channelconfig] initializeProtosStruct -> DEBU 717[0m Processing field: Capabilities
[36m2019-01-29 11:22:46.178 UTC [common/channelconfig] NewStandardValues -> DEBU 718[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 11:22:46.178 UTC [common/channelconfig] initializeProtosStruct -> DEBU 719[0m Processing field: MSP
[36m2019-01-29 11:22:46.178 UTC [common/channelconfig] validateMSP -> DEBU 71a[0m Setting up MSP for org OrdererOrg
[36m2019-01-29 11:22:46.178 UTC [msp] newBccspMsp -> DEBU 71b[0m Creating BCCSP-based MSP instance
[36m2019-01-29 11:22:46.178 UTC [msp] New -> DEBU 71c[0m Creating Cache-MSP instance
[36m2019-01-29 11:22:46.178 UTC [msp] Setup -> DEBU 71d[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 11:22:46.179 UTC [msp/identity] newIdentity -> DEBU 71e[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.179 UTC [msp/identity] newIdentity -> DEBU 71f[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.180 UTC [msp] Validate -> DEBU 720[0m MSP OrdererMSP validating identity
[36m2019-01-29 11:22:46.180 UTC [common/channelconfig] NewStandardValues -> DEBU 721[0m Initializing protos for *channelconfig.ApplicationProtos
[36m2019-01-29 11:22:46.180 UTC [common/channelconfig] initializeProtosStruct -> DEBU 722[0m Processing field: ACLs
[36m2019-01-29 11:22:46.180 UTC [common/channelconfig] initializeProtosStruct -> DEBU 723[0m Processing field: Capabilities
[36m2019-01-29 11:22:46.180 UTC [common/channelconfig] NewStandardValues -> DEBU 724[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 11:22:46.180 UTC [common/channelconfig] initializeProtosStruct -> DEBU 725[0m Processing field: AnchorPeers
[36m2019-01-29 11:22:46.180 UTC [common/channelconfig] NewStandardValues -> DEBU 726[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 11:22:46.180 UTC [common/channelconfig] initializeProtosStruct -> DEBU 727[0m Processing field: MSP
[36m2019-01-29 11:22:46.180 UTC [common/channelconfig] Validate -> DEBU 728[0m Anchor peers for org Hospital1MSP are anchor_peers:<host:"peer0.hospital1.switch2logic.co.za" port:7051 > 
[36m2019-01-29 11:22:46.181 UTC [common/channelconfig] validateMSP -> DEBU 729[0m Setting up MSP for org Hospital1MSP
[36m2019-01-29 11:22:46.181 UTC [msp] newBccspMsp -> DEBU 72a[0m Creating BCCSP-based MSP instance
[36m2019-01-29 11:22:46.181 UTC [msp] New -> DEBU 72b[0m Creating Cache-MSP instance
[36m2019-01-29 11:22:46.181 UTC [msp] Setup -> DEBU 72c[0m Setting up MSP instance Hospital1MSP
[36m2019-01-29 11:22:46.181 UTC [msp/identity] newIdentity -> DEBU 72d[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdDCCAhugAwIBAgIQZxH8VsnceZxXgKxlURLgVTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMS5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQX9V4k/h/3pFxByKDscjgQrC0YS/bSbKvzSG2Z2PvltEWp40qB9OaKUN0GQYzo
yK2Etdvyjfd1OFwaTjpGSvv9o18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCBVU7DoP9Msf/nNTNl6
5FxwAXZ82eaGBT7zWYdyh4QbBjAKBggqhkjOPQQDAgNHADBEAiBmXGWQbxCGRp0c
5/y9qJyciB1CDHsknRZDjNzt5f563wIgMy7VPi1R8DpIIIRN9B3mfhDpRa+J+8PA
S4DOJy7rjp4=
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.181 UTC [msp/identity] newIdentity -> DEBU 72e[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.182 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 72f[0m Checking if identity satisfies role [CLIENT] for Hospital1MSP
[36m2019-01-29 11:22:46.182 UTC [msp] Validate -> DEBU 730[0m MSP Hospital1MSP validating identity
[36m2019-01-29 11:22:46.183 UTC [msp] getCertificationChain -> DEBU 731[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 11:22:46.183 UTC [msp] hasOURole -> DEBU 732[0m MSP Hospital1MSP checking if the identity is a client
[36m2019-01-29 11:22:46.183 UTC [msp] getCertificationChain -> DEBU 733[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 11:22:46.183 UTC [common/channelconfig] NewStandardValues -> DEBU 734[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 11:22:46.183 UTC [common/channelconfig] initializeProtosStruct -> DEBU 735[0m Processing field: AnchorPeers
[36m2019-01-29 11:22:46.183 UTC [common/channelconfig] NewStandardValues -> DEBU 736[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 11:22:46.183 UTC [common/channelconfig] initializeProtosStruct -> DEBU 737[0m Processing field: MSP
[36m2019-01-29 11:22:46.183 UTC [common/channelconfig] Validate -> DEBU 738[0m Anchor peers for org Hospital2MSP are anchor_peers:<host:"peer0.hospital2.switch2logic.co.za" port:7051 > 
[36m2019-01-29 11:22:46.184 UTC [common/channelconfig] validateMSP -> DEBU 739[0m Setting up MSP for org Hospital2MSP
[36m2019-01-29 11:22:46.184 UTC [msp] newBccspMsp -> DEBU 73a[0m Creating BCCSP-based MSP instance
[36m2019-01-29 11:22:46.184 UTC [msp] New -> DEBU 73b[0m Creating Cache-MSP instance
[36m2019-01-29 11:22:46.184 UTC [msp] Setup -> DEBU 73c[0m Setting up MSP instance Hospital2MSP
[36m2019-01-29 11:22:46.184 UTC [msp/identity] newIdentity -> DEBU 73d[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdjCCAhygAwIBAgIRALZ3Pffn/xWUtUWPSK6CYVAwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowgYsxCzAJBgNVBAYTAlVTMRMwEQYD
VQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4gRnJhbmNpc2NvMSUwIwYDVQQK
Exxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMSgwJgYDVQQDEx9jYS5ob3Nw
aXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcD
QgAESzVpiz6P4mIb+KwvrttcXJKZR5i137Is4298E7OrkYdAg0tB4rpfKTXzUMwW
vI12cTDujpyaZSiqQ3/zP4fOQqNfMF0wDgYDVR0PAQH/BAQDAgGmMA8GA1UdJQQI
MAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQg8QJqWKPaX24HINwR
S6sINFSwRD+440/ccSnvFVaGF24wCgYIKoZIzj0EAwIDSAAwRQIhAPO2Nng3/nqb
cKPliUqVPVjBMu+D55M90+jexWSOpkjcAiADPx8qv9exZ4BfP3+G49Qzpx0Meblj
53vzUgYFa/0EXw==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.185 UTC [msp/identity] newIdentity -> DEBU 73e[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAJ6XtHMTTW1JzdpD54qCc3wwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA+z1rb9aA1HwHfNLi9V9VZe
bdwT4MXak445zGz4DXzRUsmNeBngzntKb6qpgHQpeFtzbzLx+zwiynRDvfdzORGj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPEC
alij2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQC
IA8A3CCJeCasyy7gYd/M1aHYmwWsecY7UY7cVUGy4XOxAiB0rrmfnGW6JA11PDCu
+5c5Rpn0jF5ddsFn+UIpzBFvmg==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.186 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 73f[0m Checking if identity satisfies role [CLIENT] for Hospital2MSP
[36m2019-01-29 11:22:46.186 UTC [msp] Validate -> DEBU 740[0m MSP Hospital2MSP validating identity
[36m2019-01-29 11:22:46.186 UTC [msp] getCertificationChain -> DEBU 741[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 11:22:46.186 UTC [msp] hasOURole -> DEBU 742[0m MSP Hospital2MSP checking if the identity is a client
[36m2019-01-29 11:22:46.186 UTC [msp] getCertificationChain -> DEBU 743[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 11:22:46.187 UTC [common/channelconfig] NewStandardValues -> DEBU 744[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 11:22:46.187 UTC [common/channelconfig] initializeProtosStruct -> DEBU 745[0m Processing field: AnchorPeers
[36m2019-01-29 11:22:46.187 UTC [common/channelconfig] NewStandardValues -> DEBU 746[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 11:22:46.187 UTC [common/channelconfig] initializeProtosStruct -> DEBU 747[0m Processing field: MSP
[36m2019-01-29 11:22:46.187 UTC [common/channelconfig] Validate -> DEBU 748[0m Anchor peers for org Hospital3MSP are anchor_peers:<host:"peer0.hospital3.switch2logic.co.za" port:7051 > 
[36m2019-01-29 11:22:46.187 UTC [common/channelconfig] validateMSP -> DEBU 749[0m Setting up MSP for org Hospital3MSP
[36m2019-01-29 11:22:46.187 UTC [msp] newBccspMsp -> DEBU 74a[0m Creating BCCSP-based MSP instance
[36m2019-01-29 11:22:46.187 UTC [msp] New -> DEBU 74b[0m Creating Cache-MSP instance
[36m2019-01-29 11:22:46.187 UTC [msp] Setup -> DEBU 74c[0m Setting up MSP instance Hospital3MSP
[36m2019-01-29 11:22:46.187 UTC [msp/identity] newIdentity -> DEBU 74d[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdTCCAhugAwIBAgIQCCXeHGgObIJo8aq4AdvgVjAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMy5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQm1Oo/AEu7ZZToE15Y5ntlaN128Hs/exX3jNH5nO6r4tvyFtBBNEn1ecXxxPtm
dmQ+nPHONorLwDC42SMAKwfro18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCA7vVWFXWpgZBojQ1ZL
xUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEAlOUoPtpfO8V7
vFRCanjU5zZWGO5ajFlPDJjjDTy3UrwCICrUYV32JrR/m3dslCl15dSThHrKtYCC
Ehvyw1+lNqiG
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.187 UTC [msp/identity] newIdentity -> DEBU 74e[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAI8BA/5Jf+5m8ncI+D7aSiMwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwzLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABF1IehIKIjh8tXYZ7rw4QOuc
g/tpBAkJeOt44zr/LepbjhyFW8vIRnkw75i6Wcg+LOoAS+2NFNg2oNjqg//yCHWj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIDu9
VYVdamBkGiNDVkvFS+AEt6Jrn0yK8hFyPGZhqenJMAoGCCqGSM49BAMCA0cAMEQC
IFyc85pwbCAxGNdw4fflsX2I3HrP3h93YAB2zSDC8HtkAiAQxblAZfEnXUiQKrqE
8I10bYJqD6pJz1jb3Pv+4xplTA==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.188 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 74f[0m Checking if identity satisfies role [CLIENT] for Hospital3MSP
[36m2019-01-29 11:22:46.188 UTC [msp] Validate -> DEBU 750[0m MSP Hospital3MSP validating identity
[36m2019-01-29 11:22:46.188 UTC [msp] getCertificationChain -> DEBU 751[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 11:22:46.188 UTC [msp] hasOURole -> DEBU 752[0m MSP Hospital3MSP checking if the identity is a client
[36m2019-01-29 11:22:46.188 UTC [msp] getCertificationChain -> DEBU 753[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 11:22:46.189 UTC [msp] Setup -> DEBU 754[0m Setting up the MSP manager (4 msps)
[36m2019-01-29 11:22:46.189 UTC [msp] Setup -> DEBU 755[0m MSP manager setup complete, setup 4 msps
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 756[0m Proposed new policy Writers for Channel/Orderer/OrdererOrg
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 757[0m Proposed new policy Admins for Channel/Orderer/OrdererOrg
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 758[0m Proposed new policy Readers for Channel/Orderer/OrdererOrg
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 759[0m Proposed new policy BlockValidation for Channel/Orderer
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 75a[0m Proposed new policy Admins for Channel/Orderer
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 75b[0m Proposed new policy Readers for Channel/Orderer
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 75c[0m Proposed new policy Writers for Channel/Orderer
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 75d[0m Proposed new policy Readers for Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 75e[0m Proposed new policy Writers for Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 75f[0m Proposed new policy Admins for Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 760[0m Proposed new policy Readers for Channel/Application/Hospital2MSP
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 761[0m Proposed new policy Writers for Channel/Application/Hospital2MSP
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 762[0m Proposed new policy Admins for Channel/Application/Hospital2MSP
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 763[0m Proposed new policy Writers for Channel/Application/Hospital3MSP
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 764[0m Proposed new policy Admins for Channel/Application/Hospital3MSP
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 765[0m Proposed new policy Readers for Channel/Application/Hospital3MSP
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 766[0m Proposed new policy Readers for Channel/Application
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 767[0m Proposed new policy Writers for Channel/Application
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 768[0m Proposed new policy Admins for Channel/Application
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 769[0m Proposed new policy Admins for Channel
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 76a[0m Proposed new policy Readers for Channel
[36m2019-01-29 11:22:46.189 UTC [policies] NewManagerImpl -> DEBU 76b[0m Proposed new policy Writers for Channel
[36m2019-01-29 11:22:46.189 UTC [common/configtx] addToMap -> DEBU 76c[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 11:22:46.189 UTC [common/configtx] addToMap -> DEBU 76d[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 11:22:46.189 UTC [common/configtx] addToMap -> DEBU 76e[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:46.189 UTC [common/configtx] addToMap -> DEBU 76f[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/MSP
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 770[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/AnchorPeers
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 771[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 772[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 773[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 774[0m Adding to config map: [Group]  /Channel/Application/Hospital2MSP
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 775[0m Adding to config map: [Value]  /Channel/Application/Hospital2MSP/MSP
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 776[0m Adding to config map: [Value]  /Channel/Application/Hospital2MSP/AnchorPeers
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 777[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 778[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Writers
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 779[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Admins
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 77a[0m Adding to config map: [Group]  /Channel/Application/Hospital3MSP
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 77b[0m Adding to config map: [Value]  /Channel/Application/Hospital3MSP/AnchorPeers
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 77c[0m Adding to config map: [Value]  /Channel/Application/Hospital3MSP/MSP
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 77d[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Writers
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 77e[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Admins
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 77f[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 780[0m Adding to config map: [Value]  /Channel/Application/Capabilities
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 781[0m Adding to config map: [Policy] /Channel/Application/Readers
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 782[0m Adding to config map: [Policy] /Channel/Application/Writers
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 783[0m Adding to config map: [Policy] /Channel/Application/Admins
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 784[0m Adding to config map: [Group]  /Channel/Orderer
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 785[0m Adding to config map: [Group]  /Channel/Orderer/OrdererOrg
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 786[0m Adding to config map: [Value]  /Channel/Orderer/OrdererOrg/MSP
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 787[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 788[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Admins
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 789[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 11:22:46.190 UTC [common/configtx] addToMap -> DEBU 78a[0m Adding to config map: [Value]  /Channel/Orderer/BatchSize
[36m2019-01-29 11:22:46.191 UTC [common/configtx] addToMap -> DEBU 78b[0m Adding to config map: [Value]  /Channel/Orderer/BatchTimeout
[36m2019-01-29 11:22:46.191 UTC [common/configtx] addToMap -> DEBU 78c[0m Adding to config map: [Value]  /Channel/Orderer/ChannelRestrictions
[36m2019-01-29 11:22:46.191 UTC [common/configtx] addToMap -> DEBU 78d[0m Adding to config map: [Value]  /Channel/Orderer/Capabilities
[36m2019-01-29 11:22:46.191 UTC [common/configtx] addToMap -> DEBU 78e[0m Adding to config map: [Value]  /Channel/Orderer/KafkaBrokers
[36m2019-01-29 11:22:46.191 UTC [common/configtx] addToMap -> DEBU 78f[0m Adding to config map: [Value]  /Channel/Orderer/ConsensusType
[36m2019-01-29 11:22:46.191 UTC [common/configtx] addToMap -> DEBU 790[0m Adding to config map: [Policy] /Channel/Orderer/Admins
[36m2019-01-29 11:22:46.191 UTC [common/configtx] addToMap -> DEBU 791[0m Adding to config map: [Policy] /Channel/Orderer/Readers
[36m2019-01-29 11:22:46.191 UTC [common/configtx] addToMap -> DEBU 792[0m Adding to config map: [Policy] /Channel/Orderer/Writers
[36m2019-01-29 11:22:46.191 UTC [common/configtx] addToMap -> DEBU 793[0m Adding to config map: [Policy] /Channel/Orderer/BlockValidation
[36m2019-01-29 11:22:46.191 UTC [common/configtx] addToMap -> DEBU 794[0m Adding to config map: [Value]  /Channel/HashingAlgorithm
[36m2019-01-29 11:22:46.191 UTC [common/configtx] addToMap -> DEBU 795[0m Adding to config map: [Value]  /Channel/BlockDataHashingStructure
[36m2019-01-29 11:22:46.191 UTC [common/configtx] addToMap -> DEBU 796[0m Adding to config map: [Value]  /Channel/OrdererAddresses
[36m2019-01-29 11:22:46.191 UTC [common/configtx] addToMap -> DEBU 797[0m Adding to config map: [Value]  /Channel/Capabilities
[36m2019-01-29 11:22:46.191 UTC [common/configtx] addToMap -> DEBU 798[0m Adding to config map: [Value]  /Channel/Consortium
[36m2019-01-29 11:22:46.191 UTC [common/configtx] addToMap -> DEBU 799[0m Adding to config map: [Policy] /Channel/Admins
[36m2019-01-29 11:22:46.191 UTC [common/configtx] addToMap -> DEBU 79a[0m Adding to config map: [Policy] /Channel/Readers
[36m2019-01-29 11:22:46.191 UTC [common/configtx] addToMap -> DEBU 79b[0m Adding to config map: [Policy] /Channel/Writers
[36m2019-01-29 11:22:46.191 UTC [common/channelconfig] LogSanityChecks -> DEBU 79c[0m As expected, current configuration has policy '/Channel/Readers'
[36m2019-01-29 11:22:46.191 UTC [common/channelconfig] LogSanityChecks -> DEBU 79d[0m As expected, current configuration has policy '/Channel/Writers'
[36m2019-01-29 11:22:46.191 UTC [policies] Manager -> DEBU 79e[0m Manager Channel looking up path [Application]
[36m2019-01-29 11:22:46.191 UTC [policies] Manager -> DEBU 79f[0m Manager Channel has managers Orderer
[36m2019-01-29 11:22:46.191 UTC [policies] Manager -> DEBU 7a0[0m Manager Channel has managers Application
[36m2019-01-29 11:22:46.191 UTC [policies] Manager -> DEBU 7a1[0m Manager Channel/Application looking up path []
[36m2019-01-29 11:22:46.191 UTC [policies] Manager -> DEBU 7a2[0m Manager Channel/Application has managers Hospital1MSP
[36m2019-01-29 11:22:46.191 UTC [policies] Manager -> DEBU 7a3[0m Manager Channel/Application has managers Hospital2MSP
[36m2019-01-29 11:22:46.191 UTC [policies] Manager -> DEBU 7a4[0m Manager Channel/Application has managers Hospital3MSP
[36m2019-01-29 11:22:46.191 UTC [common/channelconfig] LogSanityChecks -> DEBU 7a5[0m As expected, current configuration has policy '/Channel/Application/Readers'
[36m2019-01-29 11:22:46.191 UTC [common/channelconfig] LogSanityChecks -> DEBU 7a6[0m As expected, current configuration has policy '/Channel/Application/Writers'
[36m2019-01-29 11:22:46.191 UTC [common/channelconfig] LogSanityChecks -> DEBU 7a7[0m As expected, current configuration has policy '/Channel/Application/Admins'
[36m2019-01-29 11:22:46.191 UTC [policies] Manager -> DEBU 7a8[0m Manager Channel looking up path [Orderer]
[36m2019-01-29 11:22:46.191 UTC [policies] Manager -> DEBU 7a9[0m Manager Channel has managers Orderer
[36m2019-01-29 11:22:46.191 UTC [policies] Manager -> DEBU 7aa[0m Manager Channel has managers Application
[36m2019-01-29 11:22:46.191 UTC [policies] Manager -> DEBU 7ab[0m Manager Channel/Orderer looking up path []
[36m2019-01-29 11:22:46.191 UTC [policies] Manager -> DEBU 7ac[0m Manager Channel/Orderer has managers OrdererOrg
[36m2019-01-29 11:22:46.191 UTC [common/channelconfig] LogSanityChecks -> DEBU 7ad[0m As expected, current configuration has policy '/Channel/Orderer/BlockValidation'
[36m2019-01-29 11:22:46.191 UTC [common/capabilities] Supported -> DEBU 7ae[0m Orderer capability V1_1 is supported and is enabled
[36m2019-01-29 11:22:46.191 UTC [common/capabilities] Supported -> DEBU 7af[0m Channel capability V1_1 is supported and is enabled
[36m2019-01-29 11:22:46.191 UTC [msp] GetDefaultSigningIdentity -> DEBU 7b0[0m Obtaining default signing identity
[36m2019-01-29 11:22:46.191 UTC [msp] GetDefaultSigningIdentity -> DEBU 7b1[0m Obtaining default signing identity
[36m2019-01-29 11:22:46.192 UTC [msp/identity] Sign -> DEBU 7b2[0m Sign: plaintext: 0AFA060A1B08011A060886EFC0E20522...194A87E197E21B5767D40362283BB86B 
[36m2019-01-29 11:22:46.192 UTC [msp/identity] Sign -> DEBU 7b3[0m Sign: digest: EC0A5B222F1BEF0BE7781BDEDE6FAC8F42C826BEA9EAF77D2924F51A0376DFFB 
[36m2019-01-29 11:22:46.192 UTC [policies] Evaluate -> DEBU 7b4[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Writers ==
[36m2019-01-29 11:22:46.192 UTC [policies] Evaluate -> DEBU 7b5[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:46.192 UTC [policies] Evaluate -> DEBU 7b6[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Writers ==
[36m2019-01-29 11:22:46.192 UTC [policies] Evaluate -> DEBU 7b7[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:46.192 UTC [policies] Evaluate -> DEBU 7b8[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Writers ==
2019-01-29 11:22:46.192 UTC [msp] DeserializeIdentity -> INFO 7b9[0m Obtaining identity
[36m2019-01-29 11:22:46.192 UTC [msp/identity] newIdentity -> DEBU 7ba[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICLDCCAdKgAwIBAgIQIoN1EApgEjYu/O7unXVqYzAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBqMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEuMCwGA1UEAxMlb3JkZXJlcjAuaG9zcGl0YWwxLnN3
aXRjaDJsb2dpYy5jby56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABH2/gH3f
8uhcO2CKglQg53cAJL79dENZNdRWfaRikU5j3X60lh9Bx5YpznQ5l0qMaCK1Ogec
Nw5LGgie9ESxrHyjTTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsG
A1UdIwQkMCKAIBiGNntD5tBlG9C8QNsdXde3ItTRRa+p/+3yJwRCsbJxMAoGCCqG
SM49BAMCA0gAMEUCIQCBtzCy6LE3S8b/XG08Qcs01A8QjmCwqmZJQrDvRA8ZhgIg
aAwP9tYtEZs1DBv2bH7HwYIwmi8nlgD3+Xp6CWwvAkg=
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.192 UTC [cauthdsl] func1 -> DEBU 7bb[0m 0xc420256d50 gate 1548760966192715836 evaluation starts
[36m2019-01-29 11:22:46.192 UTC [cauthdsl] func2 -> DEBU 7bc[0m 0xc420256d50 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.192 UTC [cauthdsl] func2 -> DEBU 7bd[0m 0xc420256d50 processing identity 0 with bytes of 0a0a4f7264657265724d535012ae062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949434c44434341644b674177494241674951496f4e3145417067456a59752f4f37756e585671597a414b42676771686b6a4f50515144416a42334d5173770a435159445651514745774a56557a45544d4245474131554543424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a790a5957356a61584e6a627a45624d426b474131554543684d536333647064474e6f4d6d78765a326c6a4c6d4e764c6e70684d52347748415944565151444578566a0a5953357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b774d5449354d4463304d5455305768634e4d6a6b774d5449324d4463300a4d545530576a42714d517377435159445651514745774a56557a45544d4245474131554543424d4b5132467361575a76636d3570595445574d425147413155450a42784d4e5532467549455a795957356a61584e6a627a45754d4377474131554541784d6c62334a6b5a584a6c636a41756147397a63476c30595777784c6e4e330a6158526a61444a73623264705979356a627935365954425a4d424d4742797147534d34394167454743437147534d343941774548413049414248322f674833660a387568634f32434b676c5167353363414a4c373964454e5a4e645257666152696b55356a335836306c683942783559707a6e51356c30714d61434b314f6765630a4e77354c47676965394553787248796a5454424c4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d4373470a413155644977516b4d434b41494269474e6e74443574426c47394338514e7364586465334974545252612b702f2b33794a77524373624a784d416f47434371470a534d343942414d43413067414d45554349514342747a4379364c45335338622f5847303851637330314138516a6d4377716d5a4a517244765241385a686749670a6141775039745974455a73314442763262483748775949776d69386e6c6744332b58703643577776416b673d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.192 UTC [cauthdsl] func2 -> DEBU 7be[0m 0xc420256d50 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital2MSP, got OrdererMSP)
[36m2019-01-29 11:22:46.192 UTC [cauthdsl] func2 -> DEBU 7bf[0m 0xc420256d50 principal evaluation fails
[36m2019-01-29 11:22:46.192 UTC [cauthdsl] func1 -> DEBU 7c0[0m 0xc420256d50 gate 1548760966192715836 evaluation fails
[36m2019-01-29 11:22:46.192 UTC [policies] Evaluate -> DEBU 7c1[0m Signature set did not satisfy policy /Channel/Application/Hospital2MSP/Writers
[36m2019-01-29 11:22:46.192 UTC [policies] Evaluate -> DEBU 7c2[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Writers
[36m2019-01-29 11:22:46.192 UTC [policies] Evaluate -> DEBU 7c3[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Writers ==
[36m2019-01-29 11:22:46.192 UTC [cauthdsl] func1 -> DEBU 7c4[0m 0xc420256d68 gate 1548760966192877705 evaluation starts
[36m2019-01-29 11:22:46.192 UTC [cauthdsl] func2 -> DEBU 7c5[0m 0xc420256d68 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.192 UTC [cauthdsl] func2 -> DEBU 7c6[0m 0xc420256d68 processing identity 0 with bytes of 0a0a4f7264657265724d535012ae062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949434c44434341644b674177494241674951496f4e3145417067456a59752f4f37756e585671597a414b42676771686b6a4f50515144416a42334d5173770a435159445651514745774a56557a45544d4245474131554543424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a790a5957356a61584e6a627a45624d426b474131554543684d536333647064474e6f4d6d78765a326c6a4c6d4e764c6e70684d52347748415944565151444578566a0a5953357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b774d5449354d4463304d5455305768634e4d6a6b774d5449324d4463300a4d545530576a42714d517377435159445651514745774a56557a45544d4245474131554543424d4b5132467361575a76636d3570595445574d425147413155450a42784d4e5532467549455a795957356a61584e6a627a45754d4377474131554541784d6c62334a6b5a584a6c636a41756147397a63476c30595777784c6e4e330a6158526a61444a73623264705979356a627935365954425a4d424d4742797147534d34394167454743437147534d343941774548413049414248322f674833660a387568634f32434b676c5167353363414a4c373964454e5a4e645257666152696b55356a335836306c683942783559707a6e51356c30714d61434b314f6765630a4e77354c47676965394553787248796a5454424c4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d4373470a413155644977516b4d434b41494269474e6e74443574426c47394338514e7364586465334974545252612b702f2b33794a77524373624a784d416f47434371470a534d343942414d43413067414d45554349514342747a4379364c45335338622f5847303851637330314138516a6d4377716d5a4a517244765241385a686749670a6141775039745974455a73314442763262483748775949776d69386e6c6744332b58703643577776416b673d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.192 UTC [cauthdsl] func2 -> DEBU 7c7[0m 0xc420256d68 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital3MSP, got OrdererMSP)
[36m2019-01-29 11:22:46.192 UTC [cauthdsl] func2 -> DEBU 7c8[0m 0xc420256d68 principal evaluation fails
[36m2019-01-29 11:22:46.192 UTC [cauthdsl] func1 -> DEBU 7c9[0m 0xc420256d68 gate 1548760966192877705 evaluation fails
[36m2019-01-29 11:22:46.193 UTC [policies] Evaluate -> DEBU 7ca[0m Signature set did not satisfy policy /Channel/Application/Hospital3MSP/Writers
[36m2019-01-29 11:22:46.193 UTC [policies] Evaluate -> DEBU 7cb[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Writers
[36m2019-01-29 11:22:46.193 UTC [policies] Evaluate -> DEBU 7cc[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Writers ==
[36m2019-01-29 11:22:46.193 UTC [cauthdsl] func1 -> DEBU 7cd[0m 0xc420256d78 gate 1548760966193041695 evaluation starts
[36m2019-01-29 11:22:46.193 UTC [cauthdsl] func2 -> DEBU 7ce[0m 0xc420256d78 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.193 UTC [cauthdsl] func2 -> DEBU 7cf[0m 0xc420256d78 processing identity 0 with bytes of 0a0a4f7264657265724d535012ae062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949434c44434341644b674177494241674951496f4e3145417067456a59752f4f37756e585671597a414b42676771686b6a4f50515144416a42334d5173770a435159445651514745774a56557a45544d4245474131554543424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a790a5957356a61584e6a627a45624d426b474131554543684d536333647064474e6f4d6d78765a326c6a4c6d4e764c6e70684d52347748415944565151444578566a0a5953357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b774d5449354d4463304d5455305768634e4d6a6b774d5449324d4463300a4d545530576a42714d517377435159445651514745774a56557a45544d4245474131554543424d4b5132467361575a76636d3570595445574d425147413155450a42784d4e5532467549455a795957356a61584e6a627a45754d4377474131554541784d6c62334a6b5a584a6c636a41756147397a63476c30595777784c6e4e330a6158526a61444a73623264705979356a627935365954425a4d424d4742797147534d34394167454743437147534d343941774548413049414248322f674833660a387568634f32434b676c5167353363414a4c373964454e5a4e645257666152696b55356a335836306c683942783559707a6e51356c30714d61434b314f6765630a4e77354c47676965394553787248796a5454424c4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d4373470a413155644977516b4d434b41494269474e6e74443574426c47394338514e7364586465334974545252612b702f2b33794a77524373624a784d416f47434371470a534d343942414d43413067414d45554349514342747a4379364c45335338622f5847303851637330314138516a6d4377716d5a4a517244765241385a686749670a6141775039745974455a73314442763262483748775949776d69386e6c6744332b58703643577776416b673d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.193 UTC [cauthdsl] func2 -> DEBU 7d0[0m 0xc420256d78 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital1MSP, got OrdererMSP)
[36m2019-01-29 11:22:46.193 UTC [cauthdsl] func2 -> DEBU 7d1[0m 0xc420256d78 principal evaluation fails
[36m2019-01-29 11:22:46.193 UTC [cauthdsl] func1 -> DEBU 7d2[0m 0xc420256d78 gate 1548760966193041695 evaluation fails
[36m2019-01-29 11:22:46.193 UTC [policies] Evaluate -> DEBU 7d3[0m Signature set did not satisfy policy /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 11:22:46.193 UTC [policies] Evaluate -> DEBU 7d4[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 11:22:46.193 UTC [policies] func1 -> DEBU 7d5[0m Evaluation Failed: Only 0 policies were satisfied, but needed 1 of [ Hospital2MSP.Writers Hospital3MSP.Writers Hospital1MSP.Writers ]
[36m2019-01-29 11:22:46.193 UTC [policies] Evaluate -> DEBU 7d6[0m Signature set did not satisfy policy /Channel/Application/Writers
[36m2019-01-29 11:22:46.193 UTC [policies] Evaluate -> DEBU 7d7[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Writers
[36m2019-01-29 11:22:46.193 UTC [policies] Evaluate -> DEBU 7d8[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Orderer/Writers ==
[36m2019-01-29 11:22:46.193 UTC [policies] Evaluate -> DEBU 7d9[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:46.193 UTC [policies] Evaluate -> DEBU 7da[0m == Evaluating *cauthdsl.policy Policy /Channel/Orderer/OrdererOrg/Writers ==
[36m2019-01-29 11:22:46.193 UTC [cauthdsl] func1 -> DEBU 7db[0m 0xc420256d88 gate 1548760966193281246 evaluation starts
[36m2019-01-29 11:22:46.193 UTC [cauthdsl] func2 -> DEBU 7dc[0m 0xc420256d88 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.193 UTC [cauthdsl] func2 -> DEBU 7dd[0m 0xc420256d88 processing identity 0 with bytes of 0a0a4f7264657265724d535012ae062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949434c44434341644b674177494241674951496f4e3145417067456a59752f4f37756e585671597a414b42676771686b6a4f50515144416a42334d5173770a435159445651514745774a56557a45544d4245474131554543424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a790a5957356a61584e6a627a45624d426b474131554543684d536333647064474e6f4d6d78765a326c6a4c6d4e764c6e70684d52347748415944565151444578566a0a5953357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b774d5449354d4463304d5455305768634e4d6a6b774d5449324d4463300a4d545530576a42714d517377435159445651514745774a56557a45544d4245474131554543424d4b5132467361575a76636d3570595445574d425147413155450a42784d4e5532467549455a795957356a61584e6a627a45754d4377474131554541784d6c62334a6b5a584a6c636a41756147397a63476c30595777784c6e4e330a6158526a61444a73623264705979356a627935365954425a4d424d4742797147534d34394167454743437147534d343941774548413049414248322f674833660a387568634f32434b676c5167353363414a4c373964454e5a4e645257666152696b55356a335836306c683942783559707a6e51356c30714d61434b314f6765630a4e77354c47676965394553787248796a5454424c4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d4373470a413155644977516b4d434b41494269474e6e74443574426c47394338514e7364586465334974545252612b702f2b33794a77524373624a784d416f47434371470a534d343942414d43413067414d45554349514342747a4379364c45335338622f5847303851637330314138516a6d4377716d5a4a517244765241385a686749670a6141775039745974455a73314442763262483748775949776d69386e6c6744332b58703643577776416b673d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.193 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 7de[0m Checking if identity satisfies MEMBER role for OrdererMSP
[36m2019-01-29 11:22:46.193 UTC [msp] Validate -> DEBU 7df[0m MSP OrdererMSP validating identity
[36m2019-01-29 11:22:46.193 UTC [cauthdsl] func2 -> DEBU 7e0[0m 0xc420256d88 principal matched by identity 0
[36m2019-01-29 11:22:46.193 UTC [msp/identity] Verify -> DEBU 7e1[0m Verify: digest = 00000000  ec 0a 5b 22 2f 1b ef 0b  e7 78 1b de de 6f ac 8f  |..["/....x...o..|
00000010  42 c8 26 be a9 ea f7 7d  29 24 f5 1a 03 76 df fb  |B.&....})$...v..|
[36m2019-01-29 11:22:46.193 UTC [msp/identity] Verify -> DEBU 7e2[0m Verify: sig = 00000000  30 44 02 20 35 40 2d 97  25 43 bd 1c 34 87 94 85  |0D. 5@-.%C..4...|
00000010  39 fd bc 6f c0 b8 fa 8f  17 55 e8 9b 67 56 02 fe  |9..o.....U..gV..|
00000020  ba 79 f2 fb 02 20 26 d1  e1 46 fd 3e 89 d1 b9 15  |.y... &..F.>....|
00000030  3b f5 af 47 b3 e9 8d 00  d4 e4 b0 a3 d2 c4 e6 1e  |;..G............|
00000040  41 03 d7 fe 9d 43                                 |A....C|
[36m2019-01-29 11:22:46.193 UTC [cauthdsl] func2 -> DEBU 7e3[0m 0xc420256d88 principal evaluation succeeds for identity 0
[36m2019-01-29 11:22:46.193 UTC [cauthdsl] func1 -> DEBU 7e4[0m 0xc420256d88 gate 1548760966193281246 evaluation succeeds
[36m2019-01-29 11:22:46.193 UTC [policies] Evaluate -> DEBU 7e5[0m Signature set satisfies policy /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 11:22:46.194 UTC [policies] Evaluate -> DEBU 7e6[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 11:22:46.194 UTC [policies] Evaluate -> DEBU 7e7[0m Signature set satisfies policy /Channel/Orderer/Writers
[36m2019-01-29 11:22:46.194 UTC [policies] Evaluate -> DEBU 7e8[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Orderer/Writers
[36m2019-01-29 11:22:46.194 UTC [policies] Evaluate -> DEBU 7e9[0m Signature set satisfies policy /Channel/Writers
[36m2019-01-29 11:22:46.194 UTC [policies] Evaluate -> DEBU 7ea[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Writers
[36m2019-01-29 11:22:46.194 UTC [orderer/consensus/kafka] enqueue -> DEBU 7eb[0m [channel: comunitychannel] Enqueueing envelope...
[36m2019-01-29 11:22:46.194 UTC [orderer/consensus/kafka/sarama] handleResponse -> DEBU 7ec[0m producer/broker/0 state change to [closing] because EOF
[36m2019-01-29 11:22:46.194 UTC [orderer/consensus/kafka/sarama] handleError -> DEBU 7ed[0m Closed connection to broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:22:46.194 UTC [orderer/consensus/kafka/sarama] dispatch -> DEBU 7ee[0m producer/leader/comunitychannel/0 state change to [retrying-1]
[36m2019-01-29 11:22:46.194 UTC [orderer/consensus/kafka/sarama] dispatch -> DEBU 7ef[0m producer/leader/comunitychannel/0 abandoning broker 0
[36m2019-01-29 11:22:46.194 UTC [orderer/consensus/kafka/sarama] run -> DEBU 7f0[0m producer/broker/0 shut down
[36m2019-01-29 11:22:46.294 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 7f1[0m client/metadata fetching metadata for [comunitychannel] from broker kafka2.switch2logic.co.za:9092
[36m2019-01-29 11:22:46.295 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 7f2[0m client/metadata got error from broker while fetching metadata: EOF
[36m2019-01-29 11:22:46.295 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 7f3[0m Closed connection to broker kafka2.switch2logic.co.za:9092
[36m2019-01-29 11:22:46.295 UTC [orderer/consensus/kafka/sarama] tryRefreshMetadata -> DEBU 7f4[0m client/brokers deregistered broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 11:22:46.295 UTC [orderer/consensus/kafka/sarama] Open -> DEBU 7f5[0m ClientID is the default of 'sarama', you should consider setting it to something application-specific.
[36m2019-01-29 11:22:46.296 UTC [orderer/consensus/kafka/sarama] RefreshMetadata -> DEBU 7f6[0m client/metadata fetching metadata for [comunitychannel] from broker kafka0.switch2logic.co.za:9092
[36m2019-01-29 11:22:46.297 UTC [orderer/consensus/kafka/sarama] withRecover -> DEBU 7f7[0m Connected to broker at kafka0.switch2logic.co.za:9092 (registered as #0)
[36m2019-01-29 11:22:46.300 UTC [orderer/consensus/kafka/sarama] updateMetadata -> DEBU 7f8[0m client/brokers registered new broker #2 at kafka2.switch2logic.co.za:9092
[36m2019-01-29 11:22:46.300 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU 7f9[0m producer/broker/0 starting up
[36m2019-01-29 11:22:46.300 UTC [orderer/consensus/kafka/sarama] run)-fm -> DEBU 7fa[0m producer/broker/0 state change to [open] on comunitychannel/0
[36m2019-01-29 11:22:46.300 UTC [orderer/consensus/kafka/sarama] dispatch)-fm -> DEBU 7fb[0m producer/leader/comunitychannel/0 selected broker 0
[36m2019-01-29 11:22:46.300 UTC [orderer/consensus/kafka/sarama] dispatch -> DEBU 7fc[0m producer/leader/comunitychannel/0 state change to [flushing-1]
[36m2019-01-29 11:22:46.300 UTC [orderer/consensus/kafka/sarama] dispatch -> DEBU 7fd[0m producer/leader/comunitychannel/0 state change to [normal]
[36m2019-01-29 11:22:46.317 UTC [orderer/consensus/kafka] processMessagesToBlocks -> DEBU 7fe[0m [channel: comunitychannel] Successfully unmarshalled consumed message, offset is 7. Inspecting type...
[36m2019-01-29 11:22:46.318 UTC [orderer/consensus/kafka] processRegular -> DEBU 7ff[0m [channel: comunitychannel] Processing regular Kafka message of type CONFIG
[36m2019-01-29 11:22:46.318 UTC [orderer/consensus/kafka] func2 -> DEBU 800[0m [channel: comunitychannel] Received config message
[36m2019-01-29 11:22:46.318 UTC [orderer/consensus/kafka] func2 -> DEBU 801[0m [channel: comunitychannel] Creating isolated block for config message
[36m2019-01-29 11:22:46.318 UTC [common/configtx] addToMap -> DEBU 802[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 11:22:46.318 UTC [common/configtx] addToMap -> DEBU 803[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 11:22:46.319 UTC [common/configtx] addToMap -> DEBU 804[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:46.319 UTC [common/configtx] addToMap -> DEBU 805[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/MSP
[36m2019-01-29 11:22:46.319 UTC [common/configtx] addToMap -> DEBU 806[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 11:22:46.319 UTC [common/configtx] addToMap -> DEBU 807[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 11:22:46.319 UTC [common/configtx] addToMap -> DEBU 808[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:22:46.319 UTC [common/configtx] addToMap -> DEBU 809[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 11:22:46.319 UTC [common/configtx] addToMap -> DEBU 80a[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 11:22:46.319 UTC [common/configtx] addToMap -> DEBU 80b[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:46.319 UTC [common/configtx] addToMap -> DEBU 80c[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/AnchorPeers
[36m2019-01-29 11:22:46.319 UTC [common/configtx] addToMap -> DEBU 80d[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/MSP
[36m2019-01-29 11:22:46.319 UTC [common/configtx] addToMap -> DEBU 80e[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 11:22:46.319 UTC [common/configtx] addToMap -> DEBU 80f[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 11:22:46.319 UTC [common/configtx] addToMap -> DEBU 810[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:22:46.319 UTC [common/configtx] verifyDeltaSet -> DEBU 811[0m Processing change to key: [Value]  /Channel/Application/Hospital1MSP/AnchorPeers
[36m2019-01-29 11:22:46.319 UTC [common/configtx] verifyDeltaSet -> DEBU 812[0m Processing change to key: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:46.319 UTC [common/configtx] policyForItem -> DEBU 813[0m Getting policy for item Hospital1MSP with mod_policy Admins
[36m2019-01-29 11:22:46.319 UTC [policies] Manager -> DEBU 814[0m Manager Channel looking up path [Application]
[36m2019-01-29 11:22:46.319 UTC [policies] Manager -> DEBU 815[0m Manager Channel has managers Application
[36m2019-01-29 11:22:46.319 UTC [policies] Manager -> DEBU 816[0m Manager Channel has managers Orderer
[36m2019-01-29 11:22:46.320 UTC [policies] Manager -> DEBU 817[0m Manager Channel/Application looking up path []
[36m2019-01-29 11:22:46.320 UTC [policies] Manager -> DEBU 818[0m Manager Channel/Application has managers Hospital3MSP
[36m2019-01-29 11:22:46.320 UTC [policies] Manager -> DEBU 819[0m Manager Channel/Application has managers Hospital1MSP
[36m2019-01-29 11:22:46.320 UTC [policies] Manager -> DEBU 81a[0m Manager Channel/Application has managers Hospital2MSP
[36m2019-01-29 11:22:46.320 UTC [policies] Manager -> DEBU 81b[0m Manager Channel/Application looking up path [Hospital1MSP]
[36m2019-01-29 11:22:46.320 UTC [policies] Manager -> DEBU 81c[0m Manager Channel/Application has managers Hospital2MSP
[36m2019-01-29 11:22:46.320 UTC [policies] Manager -> DEBU 81d[0m Manager Channel/Application has managers Hospital3MSP
[36m2019-01-29 11:22:46.320 UTC [policies] Manager -> DEBU 81e[0m Manager Channel/Application has managers Hospital1MSP
[36m2019-01-29 11:22:46.320 UTC [policies] Manager -> DEBU 81f[0m Manager Channel/Application/Hospital1MSP looking up path []
[36m2019-01-29 11:22:46.320 UTC [policies] Evaluate -> DEBU 820[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Admins ==
[36m2019-01-29 11:22:46.321 UTC [cauthdsl] func1 -> DEBU 821[0m 0xc42000e858 gate 1548760966321156352 evaluation starts
[36m2019-01-29 11:22:46.321 UTC [cauthdsl] func2 -> DEBU 822[0m 0xc42000e858 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.321 UTC [cauthdsl] func2 -> DEBU 823[0m 0xc42000e858 processing identity 0 with bytes of 0a0c486f73706974616c314d535012df062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943547a4343416657674177494241674951627a7870534b69754a437a4469446c5a702b58625744414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42344d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a45504d4130474131554543784d470a593278705a5735304d5373774b5159445651514444434a425a473170626b426f62334e7761585268624445756333647064474e6f4d6d78765a326c6a4c6d4e760a4c6e70684d466b77457759484b6f5a497a6a3043415159494b6f5a497a6a304441516344516741456e7041446c382b6155426356577636713349674d513479670a49536177513077616d486d7053762b2f62314235686634563248796f50726177497234352b6f342f517a2b2b57617a5a6c795937506b6f54323772374b614e4e0a4d45737744675944565230504151482f42415144416765414d41774741315564457745422f7751434d4141774b7759445652306a42435177496f416756564f770a36442f544c482f357a557a5a6575526363414632664e6e6d6867552b38316d48636f654547775977436759494b6f5a497a6a30454177494453414177525149680a414c6176644d6d3969367853736e57646c6c694d6831307336646d474261584f30437861436f774c4a494d744169424564757564474a796d444445317a6269580a59775838336359575a4855635956384958655a5046796d432b773d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.322 UTC [cauthdsl] func2 -> DEBU 824[0m 0xc42000e858 principal matched by identity 0
[36m2019-01-29 11:22:46.326 UTC [orderer/consensus/kafka] enqueue -> DEBU 826[0m [channel: comunitychannel] Envelope enqueued successfully
[36m2019-01-29 11:22:46.326 UTC [orderer/common/broadcast] Handle -> DEBU 827[0m [channel: comunitychannel] Broadcast has successfully enqueued message of type CONFIG_UPDATE from 10.0.0.9:46806
[36m2019-01-29 11:22:46.322 UTC [msp/identity] Verify -> DEBU 825[0m Verify: digest = 00000000  2a b7 e0 fc 5e 0e fa d9  8c 68 56 4b 83 df 68 8e  |*...^....hVK..h.|
00000010  52 74 fd e1 61 6d b6 a3  f4 b4 13 5e cc ae 0c d6  |Rt..am.....^....|
[36m2019-01-29 11:22:46.329 UTC [msp/identity] Verify -> DEBU 828[0m Verify: sig = 00000000  30 45 02 21 00 fd 8b 6f  88 cf 7e 8b af 32 6e df  |0E.!...o..~..2n.|
00000010  f3 0a 58 cc 86 42 b1 60  8e b1 e5 6d 6d ee 48 57  |..X..B.`...mm.HW|
00000020  2e b1 97 f9 1a 02 20 32  72 f0 68 23 a9 15 3f 45  |...... 2r.h#..?E|
00000030  31 84 b4 61 dc f1 5d b0  a5 e6 ec 01 f9 b5 5b 97  |1..a..].......[.|
00000040  77 8e 39 8a 75 b5 cc                              |w.9.u..|
[36m2019-01-29 11:22:46.330 UTC [cauthdsl] func2 -> DEBU 829[0m 0xc42000e858 principal evaluation succeeds for identity 0
[36m2019-01-29 11:22:46.330 UTC [cauthdsl] func1 -> DEBU 82a[0m 0xc42000e858 gate 1548760966321156352 evaluation succeeds
[36m2019-01-29 11:22:46.331 UTC [policies] Evaluate -> DEBU 82b[0m Signature set satisfies policy /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 11:22:46.331 UTC [policies] Evaluate -> DEBU 82c[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 11:22:46.332 UTC [common/configtx] recurseConfigMap -> DEBU 82e[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital2MSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.332 UTC [common/configtx] recurseConfigMap -> DEBU 82f[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital2MSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.332 UTC [common/configtx] recurseConfigMap -> DEBU 830[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\022\022\020\n\014Hospital2MSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.332 UTC [common/configtx] recurseConfigMap -> DEBU 831[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital3MSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.332 UTC [common/configtx] recurseConfigMap -> DEBU 832[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\022\022\020\n\014Hospital3MSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.332 UTC [common/configtx] recurseConfigMap -> DEBU 833[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\014Hospital3MSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.333 UTC [common/configtx] recurseConfigMap -> DEBU 834[0m Setting policy for key Readers to 
[36m2019-01-29 11:22:46.333 UTC [common/configtx] recurseConfigMap -> DEBU 835[0m Setting policy for key Writers to 
[36m2019-01-29 11:22:46.333 UTC [common/configtx] recurseConfigMap -> DEBU 836[0m Setting policy for key Admins to 
[36m2019-01-29 11:22:46.333 UTC [common/configtx] recurseConfigMap -> DEBU 837[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.333 UTC [common/configtx] recurseConfigMap -> DEBU 838[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.333 UTC [common/configtx] recurseConfigMap -> DEBU 839[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.333 UTC [common/configtx] recurseConfigMap -> DEBU 83a[0m Setting policy for key Writers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\016\022\014\n\nOrdererMSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.333 UTC [common/configtx] recurseConfigMap -> DEBU 83b[0m Setting policy for key Admins to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\020\022\016\n\nOrdererMSP\020\001" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.333 UTC [common/configtx] recurseConfigMap -> DEBU 83c[0m Setting policy for key Readers to policy:<type:1 value:"\022\010\022\006\010\001\022\002\010\000\032\016\022\014\n\nOrdererMSP" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.333 UTC [common/configtx] recurseConfigMap -> DEBU 83d[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.333 UTC [common/configtx] recurseConfigMap -> DEBU 83e[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.333 UTC [common/configtx] recurseConfigMap -> DEBU 83f[0m Setting policy for key BlockValidation to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[33m2019-01-29 11:22:46.331 UTC [orderer/common/broadcast] Handle -> WARN 82d[0m Error reading from 10.0.0.9:46806: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 11:22:46.333 UTC [orderer/common/server] func1 -> DEBU 841[0m Closing Broadcast stream
[36m2019-01-29 11:22:46.333 UTC [common/configtx] recurseConfigMap -> DEBU 840[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.333 UTC [common/configtx] recurseConfigMap -> DEBU 842[0m Setting policy for key Admins to policy:<type:3 value:"\n\006Admins\020\002" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.333 UTC [common/configtx] recurseConfigMap -> DEBU 843[0m Setting policy for key Readers to policy:<type:3 value:"\n\007Readers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.333 UTC [common/configtx] recurseConfigMap -> DEBU 844[0m Setting policy for key Writers to policy:<type:3 value:"\n\007Writers" > mod_policy:"Admins" 
[36m2019-01-29 11:22:46.334 UTC [common/channelconfig] NewStandardValues -> DEBU 845[0m Initializing protos for *channelconfig.ChannelProtos
[36m2019-01-29 11:22:46.334 UTC [common/channelconfig] initializeProtosStruct -> DEBU 846[0m Processing field: HashingAlgorithm
[36m2019-01-29 11:22:46.334 UTC [common/channelconfig] initializeProtosStruct -> DEBU 847[0m Processing field: BlockDataHashingStructure
[36m2019-01-29 11:22:46.334 UTC [common/channelconfig] initializeProtosStruct -> DEBU 848[0m Processing field: OrdererAddresses
[36m2019-01-29 11:22:46.334 UTC [common/channelconfig] initializeProtosStruct -> DEBU 849[0m Processing field: Consortium
[36m2019-01-29 11:22:46.334 UTC [common/channelconfig] initializeProtosStruct -> DEBU 84a[0m Processing field: Capabilities
[36m2019-01-29 11:22:46.334 UTC [common/channelconfig] NewStandardValues -> DEBU 84b[0m Initializing protos for *channelconfig.OrdererProtos
[36m2019-01-29 11:22:46.334 UTC [common/channelconfig] initializeProtosStruct -> DEBU 84c[0m Processing field: ConsensusType
[36m2019-01-29 11:22:46.334 UTC [common/channelconfig] initializeProtosStruct -> DEBU 84d[0m Processing field: BatchSize
[36m2019-01-29 11:22:46.334 UTC [common/channelconfig] initializeProtosStruct -> DEBU 84e[0m Processing field: BatchTimeout
[36m2019-01-29 11:22:46.334 UTC [common/channelconfig] initializeProtosStruct -> DEBU 84f[0m Processing field: KafkaBrokers
[36m2019-01-29 11:22:46.334 UTC [common/channelconfig] initializeProtosStruct -> DEBU 850[0m Processing field: ChannelRestrictions
[36m2019-01-29 11:22:46.334 UTC [common/channelconfig] initializeProtosStruct -> DEBU 851[0m Processing field: Capabilities
[36m2019-01-29 11:22:46.334 UTC [common/channelconfig] NewStandardValues -> DEBU 852[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 11:22:46.334 UTC [common/channelconfig] initializeProtosStruct -> DEBU 853[0m Processing field: MSP
[36m2019-01-29 11:22:46.334 UTC [common/channelconfig] validateMSP -> DEBU 854[0m Setting up MSP for org OrdererOrg
[36m2019-01-29 11:22:46.334 UTC [msp] newBccspMsp -> DEBU 855[0m Creating BCCSP-based MSP instance
[36m2019-01-29 11:22:46.334 UTC [msp] New -> DEBU 856[0m Creating Cache-MSP instance
[36m2019-01-29 11:22:46.334 UTC [msp] Setup -> DEBU 857[0m Setting up MSP instance OrdererMSP
[36m2019-01-29 11:22:46.335 UTC [msp/identity] newIdentity -> DEBU 858[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICSzCCAfKgAwIBAgIRAO7EFp+tGDrZX/4Yn1ExG4IwCgYIKoZIzj0EAwIwdzEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEeMBwGA1UEAxMV
Y2Euc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5MDEyOTA3NDE1NFoXDTI5MDEyNjA3
NDE1NFowdzELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNV
BAcTDVNhbiBGcmFuY2lzY28xGzAZBgNVBAoTEnN3aXRjaDJsb2dpYy5jby56YTEe
MBwGA1UEAxMVY2Euc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZI
zj0DAQcDQgAE+ci0VrQQEnBN2VlYMEvGL0RvqwQ9zVhgyxdKDi9AB8iyQ0dhyWzJ
CZ9MnAWGk29pw9b+9HdNkbalpJKWkcctJ6NfMF0wDgYDVR0PAQH/BAQDAgGmMA8G
A1UdJQQIMAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQgGIY2e0Pm
0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDRwAwRAIgcoBa
RHN5r57Qu7sd8TwCzfyJI+uozczAYI2SsN06iKgCIAeAP4FbTHxjqSLcNZMNGbTZ
YWA1h+rDBMa99duId/0D
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.335 UTC [msp/identity] newIdentity -> DEBU 859[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICHzCCAcWgAwIBAgIQRo5Qm6m11gCv1i2SXwRJAjAKBggqhkjOPQQDAjB3MQsw
CQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy
YW5jaXNjbzEbMBkGA1UEChMSc3dpdGNoMmxvZ2ljLmNvLnphMR4wHAYDVQQDExVj
YS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkwMTI5MDc0MTU0WhcNMjkwMTI2MDc0
MTU0WjBdMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UE
BxMNU2FuIEZyYW5jaXNjbzEhMB8GA1UEAwwYQWRtaW5Ac3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0QeSwnrMZdy7TKSYF/JzQ12C
vfINZcjx7xyeUuj/96gj42fjc1ctn7oo/tDl1eJNZbDhFvkRzefYmxtoSnH5Y6NN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgGIY2
e0Pm0GUb0LxA2x1d17ci1NFFr6n/7fInBEKxsnEwCgYIKoZIzj0EAwIDSAAwRQIh
AM8EkjCZIKrAf9sL1d1fThizng+66jmEVlKPbTpimVbiAiAhCJZoeBIJGcgxbCr2
5Rd2IW34zWcPhemkNZvCKv27WA==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.335 UTC [msp] Validate -> DEBU 85a[0m MSP OrdererMSP validating identity
[36m2019-01-29 11:22:46.336 UTC [common/channelconfig] NewStandardValues -> DEBU 85b[0m Initializing protos for *channelconfig.ApplicationProtos
[36m2019-01-29 11:22:46.336 UTC [common/channelconfig] initializeProtosStruct -> DEBU 85c[0m Processing field: ACLs
[36m2019-01-29 11:22:46.336 UTC [common/channelconfig] initializeProtosStruct -> DEBU 85d[0m Processing field: Capabilities
[36m2019-01-29 11:22:46.351 UTC [common/channelconfig] NewStandardValues -> DEBU 85e[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 11:22:46.351 UTC [common/channelconfig] initializeProtosStruct -> DEBU 85f[0m Processing field: AnchorPeers
[36m2019-01-29 11:22:46.351 UTC [common/channelconfig] NewStandardValues -> DEBU 860[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 11:22:46.351 UTC [common/channelconfig] initializeProtosStruct -> DEBU 861[0m Processing field: MSP
[36m2019-01-29 11:22:46.351 UTC [common/channelconfig] Validate -> DEBU 862[0m Anchor peers for org Hospital1MSP are anchor_peers:<host:"peer0.hospital1.switch2logic.co.za" port:7051 > 
[36m2019-01-29 11:22:46.351 UTC [common/channelconfig] validateMSP -> DEBU 863[0m Setting up MSP for org Hospital1MSP
[36m2019-01-29 11:22:46.351 UTC [msp] newBccspMsp -> DEBU 864[0m Creating BCCSP-based MSP instance
[36m2019-01-29 11:22:46.351 UTC [msp] New -> DEBU 865[0m Creating Cache-MSP instance
[36m2019-01-29 11:22:46.351 UTC [msp] Setup -> DEBU 866[0m Setting up MSP instance Hospital1MSP
[36m2019-01-29 11:22:46.352 UTC [msp/identity] newIdentity -> DEBU 867[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdDCCAhugAwIBAgIQZxH8VsnceZxXgKxlURLgVTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMS5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQX9V4k/h/3pFxByKDscjgQrC0YS/bSbKvzSG2Z2PvltEWp40qB9OaKUN0GQYzo
yK2Etdvyjfd1OFwaTjpGSvv9o18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCBVU7DoP9Msf/nNTNl6
5FxwAXZ82eaGBT7zWYdyh4QbBjAKBggqhkjOPQQDAgNHADBEAiBmXGWQbxCGRp0c
5/y9qJyciB1CDHsknRZDjNzt5f563wIgMy7VPi1R8DpIIIRN9B3mfhDpRa+J+8PA
S4DOJy7rjp4=
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.352 UTC [msp/identity] newIdentity -> DEBU 868[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfWgAwIBAgIQbzxpSKiuJCzDiDlZp+XbWDAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB4MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEPMA0GA1UECxMG
Y2xpZW50MSswKQYDVQQDDCJBZG1pbkBob3NwaXRhbDEuc3dpdGNoMmxvZ2ljLmNv
LnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEnpADl8+aUBcVWv6q3IgMQ4yg
ISawQ0wamHmpSv+/b1B5hf4V2HyoPrawIr45+o4/Qz++WazZlyY7PkoT27r7KaNN
MEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0jBCQwIoAgVVOw
6D/TLH/5zUzZeuRccAF2fNnmhgU+81mHcoeEGwYwCgYIKoZIzj0EAwIDSAAwRQIh
ALavdMm9i6xSsnWdlliMh10s6dmGBaXO0CxaCowLJIMtAiBEduudGJymDDE1zbiX
YwX83cYWZHUcYV8IXeZPFymC+w==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.353 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 869[0m Checking if identity satisfies role [CLIENT] for Hospital1MSP
[36m2019-01-29 11:22:46.353 UTC [msp] Validate -> DEBU 86a[0m MSP Hospital1MSP validating identity
[36m2019-01-29 11:22:46.354 UTC [msp] getCertificationChain -> DEBU 86b[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 11:22:46.354 UTC [msp] hasOURole -> DEBU 86c[0m MSP Hospital1MSP checking if the identity is a client
[36m2019-01-29 11:22:46.354 UTC [msp] getCertificationChain -> DEBU 86d[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 11:22:46.354 UTC [common/channelconfig] NewStandardValues -> DEBU 86e[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 11:22:46.354 UTC [common/channelconfig] initializeProtosStruct -> DEBU 86f[0m Processing field: AnchorPeers
[36m2019-01-29 11:22:46.354 UTC [common/channelconfig] NewStandardValues -> DEBU 870[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 11:22:46.354 UTC [common/channelconfig] initializeProtosStruct -> DEBU 871[0m Processing field: MSP
[36m2019-01-29 11:22:46.354 UTC [common/channelconfig] Validate -> DEBU 872[0m Anchor peers for org Hospital2MSP are anchor_peers:<host:"peer0.hospital2.switch2logic.co.za" port:7051 > 
[36m2019-01-29 11:22:46.355 UTC [common/channelconfig] validateMSP -> DEBU 873[0m Setting up MSP for org Hospital2MSP
[36m2019-01-29 11:22:46.355 UTC [msp] newBccspMsp -> DEBU 874[0m Creating BCCSP-based MSP instance
[36m2019-01-29 11:22:46.355 UTC [msp] New -> DEBU 875[0m Creating Cache-MSP instance
[36m2019-01-29 11:22:46.355 UTC [msp] Setup -> DEBU 876[0m Setting up MSP instance Hospital2MSP
[36m2019-01-29 11:22:46.356 UTC [msp/identity] newIdentity -> DEBU 877[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdjCCAhygAwIBAgIRALZ3Pffn/xWUtUWPSK6CYVAwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowgYsxCzAJBgNVBAYTAlVTMRMwEQYD
VQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4gRnJhbmNpc2NvMSUwIwYDVQQK
Exxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMSgwJgYDVQQDEx9jYS5ob3Nw
aXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcD
QgAESzVpiz6P4mIb+KwvrttcXJKZR5i137Is4298E7OrkYdAg0tB4rpfKTXzUMwW
vI12cTDujpyaZSiqQ3/zP4fOQqNfMF0wDgYDVR0PAQH/BAQDAgGmMA8GA1UdJQQI
MAYGBFUdJQAwDwYDVR0TAQH/BAUwAwEB/zApBgNVHQ4EIgQg8QJqWKPaX24HINwR
S6sINFSwRD+440/ccSnvFVaGF24wCgYIKoZIzj0EAwIDSAAwRQIhAPO2Nng3/nqb
cKPliUqVPVjBMu+D55M90+jexWSOpkjcAiADPx8qv9exZ4BfP3+G49Qzpx0Meblj
53vzUgYFa/0EXw==
-----END CERTIFICATE-----
[33m2019-01-29 11:22:46.360 UTC [common/deliver] Handle -> WARN 878[0m Error reading from 10.0.0.9:46804: rpc error: code = Canceled desc = context canceled
[36m2019-01-29 11:22:46.360 UTC [orderer/common/server] func1 -> DEBU 879[0m Closing Deliver stream
[36m2019-01-29 11:22:46.361 UTC [msp/identity] newIdentity -> DEBU 87a[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAJ6XtHMTTW1JzdpD54qCc3wwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDIuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA+z1rb9aA1HwHfNLi9V9VZe
bdwT4MXak445zGz4DXzRUsmNeBngzntKb6qpgHQpeFtzbzLx+zwiynRDvfdzORGj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPEC
alij2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQC
IA8A3CCJeCasyy7gYd/M1aHYmwWsecY7UY7cVUGy4XOxAiB0rrmfnGW6JA11PDCu
+5c5Rpn0jF5ddsFn+UIpzBFvmg==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.363 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 87b[0m Checking if identity satisfies role [CLIENT] for Hospital2MSP
[36m2019-01-29 11:22:46.363 UTC [msp] Validate -> DEBU 87c[0m MSP Hospital2MSP validating identity
[36m2019-01-29 11:22:46.364 UTC [msp] getCertificationChain -> DEBU 87d[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 11:22:46.364 UTC [msp] hasOURole -> DEBU 87e[0m MSP Hospital2MSP checking if the identity is a client
[36m2019-01-29 11:22:46.364 UTC [msp] getCertificationChain -> DEBU 87f[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 11:22:46.367 UTC [common/channelconfig] NewStandardValues -> DEBU 880[0m Initializing protos for *channelconfig.ApplicationOrgProtos
[36m2019-01-29 11:22:46.367 UTC [common/channelconfig] initializeProtosStruct -> DEBU 881[0m Processing field: AnchorPeers
[36m2019-01-29 11:22:46.367 UTC [common/channelconfig] NewStandardValues -> DEBU 882[0m Initializing protos for *channelconfig.OrganizationProtos
[36m2019-01-29 11:22:46.367 UTC [common/channelconfig] initializeProtosStruct -> DEBU 883[0m Processing field: MSP
[36m2019-01-29 11:22:46.367 UTC [common/channelconfig] Validate -> DEBU 884[0m Anchor peers for org Hospital3MSP are anchor_peers:<host:"peer0.hospital3.switch2logic.co.za" port:7051 > 
[36m2019-01-29 11:22:46.367 UTC [common/channelconfig] validateMSP -> DEBU 885[0m Setting up MSP for org Hospital3MSP
[36m2019-01-29 11:22:46.367 UTC [msp] newBccspMsp -> DEBU 886[0m Creating BCCSP-based MSP instance
[36m2019-01-29 11:22:46.367 UTC [msp] New -> DEBU 887[0m Creating Cache-MSP instance
[36m2019-01-29 11:22:46.367 UTC [msp] Setup -> DEBU 888[0m Setting up MSP instance Hospital3MSP
[36m2019-01-29 11:22:46.367 UTC [msp/identity] newIdentity -> DEBU 889[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICdTCCAhugAwIBAgIQCCXeHGgObIJo8aq4AdvgVjAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjCBizELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xJTAjBgNVBAoT
HGhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28uemExKDAmBgNVBAMTH2NhLmhvc3Bp
dGFsMy5zd2l0Y2gybG9naWMuY28uemEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNC
AAQm1Oo/AEu7ZZToE15Y5ntlaN128Hs/exX3jNH5nO6r4tvyFtBBNEn1ecXxxPtm
dmQ+nPHONorLwDC42SMAKwfro18wXTAOBgNVHQ8BAf8EBAMCAaYwDwYDVR0lBAgw
BgYEVR0lADAPBgNVHRMBAf8EBTADAQH/MCkGA1UdDgQiBCA7vVWFXWpgZBojQ1ZL
xUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEAlOUoPtpfO8V7
vFRCanjU5zZWGO5ajFlPDJjjDTy3UrwCICrUYV32JrR/m3dslCl15dSThHrKtYCC
Ehvyw1+lNqiG
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.367 UTC [msp/identity] newIdentity -> DEBU 88a[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTzCCAfagAwIBAgIRAI8BA/5Jf+5m8ncI+D7aSiMwCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFoweDELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDzANBgNVBAsT
BmNsaWVudDErMCkGA1UEAwwiQWRtaW5AaG9zcGl0YWwzLnN3aXRjaDJsb2dpYy5j
by56YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABF1IehIKIjh8tXYZ7rw4QOuc
g/tpBAkJeOt44zr/LepbjhyFW8vIRnkw75i6Wcg+LOoAS+2NFNg2oNjqg//yCHWj
TTBLMA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIDu9
VYVdamBkGiNDVkvFS+AEt6Jrn0yK8hFyPGZhqenJMAoGCCqGSM49BAMCA0cAMEQC
IFyc85pwbCAxGNdw4fflsX2I3HrP3h93YAB2zSDC8HtkAiAQxblAZfEnXUiQKrqE
8I10bYJqD6pJz1jb3Pv+4xplTA==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.368 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 88b[0m Checking if identity satisfies role [CLIENT] for Hospital3MSP
[36m2019-01-29 11:22:46.368 UTC [msp] Validate -> DEBU 88c[0m MSP Hospital3MSP validating identity
[36m2019-01-29 11:22:46.369 UTC [msp] getCertificationChain -> DEBU 88d[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 11:22:46.369 UTC [msp] hasOURole -> DEBU 88e[0m MSP Hospital3MSP checking if the identity is a client
[36m2019-01-29 11:22:46.369 UTC [msp] getCertificationChain -> DEBU 88f[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 11:22:46.369 UTC [msp] Setup -> DEBU 890[0m Setting up the MSP manager (4 msps)
[36m2019-01-29 11:22:46.369 UTC [msp] Setup -> DEBU 891[0m MSP manager setup complete, setup 4 msps
[36m2019-01-29 11:22:46.369 UTC [policies] NewManagerImpl -> DEBU 892[0m Proposed new policy Writers for Channel/Orderer/OrdererOrg
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 893[0m Proposed new policy Admins for Channel/Orderer/OrdererOrg
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 894[0m Proposed new policy Readers for Channel/Orderer/OrdererOrg
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 895[0m Proposed new policy Readers for Channel/Orderer
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 896[0m Proposed new policy Writers for Channel/Orderer
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 897[0m Proposed new policy BlockValidation for Channel/Orderer
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 898[0m Proposed new policy Admins for Channel/Orderer
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 899[0m Proposed new policy Writers for Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 89a[0m Proposed new policy Admins for Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 89b[0m Proposed new policy Readers for Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 89c[0m Proposed new policy Readers for Channel/Application/Hospital2MSP
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 89d[0m Proposed new policy Writers for Channel/Application/Hospital2MSP
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 89e[0m Proposed new policy Admins for Channel/Application/Hospital2MSP
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 89f[0m Proposed new policy Writers for Channel/Application/Hospital3MSP
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 8a0[0m Proposed new policy Admins for Channel/Application/Hospital3MSP
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 8a1[0m Proposed new policy Readers for Channel/Application/Hospital3MSP
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 8a2[0m Proposed new policy Admins for Channel/Application
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 8a3[0m Proposed new policy Readers for Channel/Application
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 8a4[0m Proposed new policy Writers for Channel/Application
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 8a5[0m Proposed new policy Admins for Channel
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 8a6[0m Proposed new policy Readers for Channel
[36m2019-01-29 11:22:46.370 UTC [policies] NewManagerImpl -> DEBU 8a7[0m Proposed new policy Writers for Channel
[36m2019-01-29 11:22:46.370 UTC [common/configtx] addToMap -> DEBU 8a8[0m Adding to config map: [Group]  /Channel
[36m2019-01-29 11:22:46.370 UTC [common/configtx] addToMap -> DEBU 8a9[0m Adding to config map: [Group]  /Channel/Orderer
[36m2019-01-29 11:22:46.370 UTC [common/configtx] addToMap -> DEBU 8aa[0m Adding to config map: [Group]  /Channel/Orderer/OrdererOrg
[36m2019-01-29 11:22:46.370 UTC [common/configtx] addToMap -> DEBU 8ab[0m Adding to config map: [Value]  /Channel/Orderer/OrdererOrg/MSP
[36m2019-01-29 11:22:46.370 UTC [common/configtx] addToMap -> DEBU 8ac[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Writers
[36m2019-01-29 11:22:46.370 UTC [common/configtx] addToMap -> DEBU 8ad[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Admins
[36m2019-01-29 11:22:46.370 UTC [common/configtx] addToMap -> DEBU 8ae[0m Adding to config map: [Policy] /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 11:22:46.370 UTC [common/configtx] addToMap -> DEBU 8af[0m Adding to config map: [Value]  /Channel/Orderer/BatchSize
[36m2019-01-29 11:22:46.370 UTC [common/configtx] addToMap -> DEBU 8b0[0m Adding to config map: [Value]  /Channel/Orderer/BatchTimeout
[36m2019-01-29 11:22:46.370 UTC [common/configtx] addToMap -> DEBU 8b1[0m Adding to config map: [Value]  /Channel/Orderer/ChannelRestrictions
[36m2019-01-29 11:22:46.370 UTC [common/configtx] addToMap -> DEBU 8b2[0m Adding to config map: [Value]  /Channel/Orderer/Capabilities
[36m2019-01-29 11:22:46.370 UTC [common/configtx] addToMap -> DEBU 8b3[0m Adding to config map: [Value]  /Channel/Orderer/KafkaBrokers
[36m2019-01-29 11:22:46.371 UTC [common/configtx] addToMap -> DEBU 8b4[0m Adding to config map: [Value]  /Channel/Orderer/ConsensusType
[36m2019-01-29 11:22:46.371 UTC [common/configtx] addToMap -> DEBU 8b5[0m Adding to config map: [Policy] /Channel/Orderer/Readers
[36m2019-01-29 11:22:46.371 UTC [common/configtx] addToMap -> DEBU 8b6[0m Adding to config map: [Policy] /Channel/Orderer/Writers
[36m2019-01-29 11:22:46.371 UTC [common/configtx] addToMap -> DEBU 8b7[0m Adding to config map: [Policy] /Channel/Orderer/BlockValidation
[36m2019-01-29 11:22:46.371 UTC [common/configtx] addToMap -> DEBU 8b8[0m Adding to config map: [Policy] /Channel/Orderer/Admins
[36m2019-01-29 11:22:46.371 UTC [common/configtx] addToMap -> DEBU 8b9[0m Adding to config map: [Group]  /Channel/Application
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8ba[0m Adding to config map: [Group]  /Channel/Application/Hospital1MSP
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8bb[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/MSP
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8bc[0m Adding to config map: [Value]  /Channel/Application/Hospital1MSP/AnchorPeers
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8bd[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8be[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Writers
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8bf[0m Adding to config map: [Policy] /Channel/Application/Hospital1MSP/Admins
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8c0[0m Adding to config map: [Group]  /Channel/Application/Hospital2MSP
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8c1[0m Adding to config map: [Value]  /Channel/Application/Hospital2MSP/MSP
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8c2[0m Adding to config map: [Value]  /Channel/Application/Hospital2MSP/AnchorPeers
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8c3[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8c4[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Writers
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8c5[0m Adding to config map: [Policy] /Channel/Application/Hospital2MSP/Admins
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8c6[0m Adding to config map: [Group]  /Channel/Application/Hospital3MSP
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8c7[0m Adding to config map: [Value]  /Channel/Application/Hospital3MSP/AnchorPeers
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8c8[0m Adding to config map: [Value]  /Channel/Application/Hospital3MSP/MSP
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8c9[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Admins
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8ca[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8cb[0m Adding to config map: [Policy] /Channel/Application/Hospital3MSP/Writers
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8cc[0m Adding to config map: [Value]  /Channel/Application/Capabilities
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8cd[0m Adding to config map: [Policy] /Channel/Application/Admins
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8ce[0m Adding to config map: [Policy] /Channel/Application/Readers
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8cf[0m Adding to config map: [Policy] /Channel/Application/Writers
[36m2019-01-29 11:22:46.372 UTC [common/configtx] addToMap -> DEBU 8d0[0m Adding to config map: [Value]  /Channel/HashingAlgorithm
[36m2019-01-29 11:22:46.373 UTC [common/configtx] addToMap -> DEBU 8d1[0m Adding to config map: [Value]  /Channel/BlockDataHashingStructure
[36m2019-01-29 11:22:46.373 UTC [common/configtx] addToMap -> DEBU 8d2[0m Adding to config map: [Value]  /Channel/OrdererAddresses
[36m2019-01-29 11:22:46.373 UTC [common/configtx] addToMap -> DEBU 8d3[0m Adding to config map: [Value]  /Channel/Capabilities
[36m2019-01-29 11:22:46.373 UTC [common/configtx] addToMap -> DEBU 8d4[0m Adding to config map: [Value]  /Channel/Consortium
[36m2019-01-29 11:22:46.373 UTC [common/configtx] addToMap -> DEBU 8d5[0m Adding to config map: [Policy] /Channel/Admins
[36m2019-01-29 11:22:46.373 UTC [common/configtx] addToMap -> DEBU 8d6[0m Adding to config map: [Policy] /Channel/Readers
[36m2019-01-29 11:22:46.373 UTC [common/configtx] addToMap -> DEBU 8d7[0m Adding to config map: [Policy] /Channel/Writers
[36m2019-01-29 11:22:46.373 UTC [common/channelconfig] LogSanityChecks -> DEBU 8d8[0m As expected, current configuration has policy '/Channel/Readers'
[36m2019-01-29 11:22:46.373 UTC [common/channelconfig] LogSanityChecks -> DEBU 8d9[0m As expected, current configuration has policy '/Channel/Writers'
[36m2019-01-29 11:22:46.373 UTC [policies] Manager -> DEBU 8da[0m Manager Channel looking up path [Application]
[36m2019-01-29 11:22:46.373 UTC [policies] Manager -> DEBU 8db[0m Manager Channel has managers Orderer
[36m2019-01-29 11:22:46.373 UTC [policies] Manager -> DEBU 8dc[0m Manager Channel has managers Application
[36m2019-01-29 11:22:46.373 UTC [policies] Manager -> DEBU 8dd[0m Manager Channel/Application looking up path []
[36m2019-01-29 11:22:46.373 UTC [policies] Manager -> DEBU 8de[0m Manager Channel/Application has managers Hospital2MSP
[36m2019-01-29 11:22:46.373 UTC [policies] Manager -> DEBU 8df[0m Manager Channel/Application has managers Hospital3MSP
[36m2019-01-29 11:22:46.373 UTC [policies] Manager -> DEBU 8e0[0m Manager Channel/Application has managers Hospital1MSP
[36m2019-01-29 11:22:46.373 UTC [common/channelconfig] LogSanityChecks -> DEBU 8e1[0m As expected, current configuration has policy '/Channel/Application/Readers'
[36m2019-01-29 11:22:46.373 UTC [common/channelconfig] LogSanityChecks -> DEBU 8e2[0m As expected, current configuration has policy '/Channel/Application/Writers'
[36m2019-01-29 11:22:46.373 UTC [common/channelconfig] LogSanityChecks -> DEBU 8e3[0m As expected, current configuration has policy '/Channel/Application/Admins'
[36m2019-01-29 11:22:46.373 UTC [policies] Manager -> DEBU 8e4[0m Manager Channel looking up path [Orderer]
[36m2019-01-29 11:22:46.373 UTC [policies] Manager -> DEBU 8e5[0m Manager Channel has managers Application
[36m2019-01-29 11:22:46.373 UTC [policies] Manager -> DEBU 8e6[0m Manager Channel has managers Orderer
[36m2019-01-29 11:22:46.373 UTC [policies] Manager -> DEBU 8e7[0m Manager Channel/Orderer looking up path []
[36m2019-01-29 11:22:46.373 UTC [policies] Manager -> DEBU 8e8[0m Manager Channel/Orderer has managers OrdererOrg
[36m2019-01-29 11:22:46.373 UTC [common/channelconfig] LogSanityChecks -> DEBU 8e9[0m As expected, current configuration has policy '/Channel/Orderer/BlockValidation'
[36m2019-01-29 11:22:46.373 UTC [common/capabilities] Supported -> DEBU 8ea[0m Orderer capability V1_1 is supported and is enabled
[36m2019-01-29 11:22:46.373 UTC [common/capabilities] Supported -> DEBU 8eb[0m Channel capability V1_1 is supported and is enabled
[36m2019-01-29 11:22:46.375 UTC [msp] GetDefaultSigningIdentity -> DEBU 8ec[0m Obtaining default signing identity
[36m2019-01-29 11:22:46.375 UTC [msp] GetDefaultSigningIdentity -> DEBU 8ed[0m Obtaining default signing identity
[36m2019-01-29 11:22:46.375 UTC [msp/identity] Sign -> DEBU 8ee[0m Sign: plaintext: 0ABD060A0A4F7264657265724D535012...8DD5B893371CB6035011BD69E7B67DC9 
[36m2019-01-29 11:22:46.375 UTC [msp/identity] Sign -> DEBU 8ef[0m Sign: digest: 52661A02E2767A405D92A8BA0F16BFE89405C13AD42154CDC5D5F4EDF190C240 
[36m2019-01-29 11:22:46.375 UTC [orderer/commmon/multichannel] addLastConfigSignature -> DEBU 8f0[0m [channel: comunitychannel] Detected lastConfigSeq transitioning from 3 to 4, setting lastConfigBlockNum from 2 to 3
[36m2019-01-29 11:22:46.375 UTC [msp] GetDefaultSigningIdentity -> DEBU 8f1[0m Obtaining default signing identity
[36m2019-01-29 11:22:46.375 UTC [orderer/commmon/multichannel] addLastConfigSignature -> DEBU 8f2[0m [channel: comunitychannel] About to write block, setting its LAST_CONFIG to 3
[36m2019-01-29 11:22:46.375 UTC [msp] GetDefaultSigningIdentity -> DEBU 8f3[0m Obtaining default signing identity
[36m2019-01-29 11:22:46.375 UTC [msp/identity] Sign -> DEBU 8f4[0m Sign: plaintext: 08030ABD060A0A4F7264657265724D53...8DD5B893371CB6035011BD69E7B67DC9 
[36m2019-01-29 11:22:46.375 UTC [msp/identity] Sign -> DEBU 8f5[0m Sign: digest: BCC9BBF5374A98EC67FC22F710314B19474E7CC268ED59C6FFB22A303C7EE2DD 
[36m2019-01-29 11:22:46.378 UTC [fsblkstorage] indexBlock -> DEBU 8f6[0m Indexing block [blockNum=3, blockHash=[]byte{0xe3, 0x53, 0x1a, 0x6c, 0x50, 0xfb, 0xc9, 0xd0, 0xac, 0xe3, 0xa5, 0x40, 0x98, 0x5f, 0x53, 0x2f, 0x52, 0x42, 0x2c, 0xda, 0xdd, 0xd3, 0x19, 0x17, 0x3b, 0xf3, 0xca, 0xa5, 0x43, 0xcc, 0x4b, 0x8a} txOffsets=
txId= locPointer=offset=71, bytesLength=21822
]
[36m2019-01-29 11:22:46.380 UTC [fsblkstorage] updateCheckpoint -> DEBU 8f7[0m Broadcasting about update checkpointInfo: latestFileChunkSuffixNum=[0], latestFileChunksize=[92875], isChainEmpty=[false], lastBlockNumber=[3]
[36m2019-01-29 11:22:46.381 UTC [orderer/commmon/multichannel] commitBlock -> DEBU 8f8[0m [channel: comunitychannel] Wrote block 3
[36m2019-01-29 11:22:46.381 UTC [fsblkstorage] waitForBlock -> DEBU 8f9[0m Came out of wait. maxAvailaBlockNumber=[3]
[36m2019-01-29 11:22:46.381 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 8fa[0m Remaining bytes=[23781], Going to peek [8] bytes
[36m2019-01-29 11:22:46.381 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 8fb[0m Returning blockbytes - length=[23778], placementInfo={fileNum=[0], startOffset=[69094], bytesOffset=[69097]}
[36m2019-01-29 11:22:46.381 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 8fc[0m blockbytes [23778] read from file [0]
[36m2019-01-29 11:22:46.381 UTC [policies] Evaluate -> DEBU 8fd[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers ==
[36m2019-01-29 11:22:46.381 UTC [policies] Evaluate -> DEBU 8fe[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:46.381 UTC [policies] Evaluate -> DEBU 8ff[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Orderer/Readers ==
[36m2019-01-29 11:22:46.381 UTC [policies] Evaluate -> DEBU 900[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:46.381 UTC [policies] Evaluate -> DEBU 901[0m == Evaluating *cauthdsl.policy Policy /Channel/Orderer/OrdererOrg/Readers ==
2019-01-29 11:22:46.381 UTC [msp] DeserializeIdentity -> INFO 902[0m Obtaining identity
[36m2019-01-29 11:22:46.382 UTC [msp/identity] newIdentity -> DEBU 903[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTTCCAfOgAwIBAgIQDPIhkZbBMzAPRV2mdTJ2BTAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMS5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB2MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzENMAsGA1UECxME
cGVlcjErMCkGA1UEAxMicGVlcjAuaG9zcGl0YWwxLnN3aXRjaDJsb2dpYy5jby56
YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABMHSfENe+q4SvhxLb6FxLr2SLYg+
qJnL3whOrSDaEyQgmDz7fwbLhWr0Mapiq8N3Xg1vudI6XbNZxyMTBmZbd3ujTTBL
MA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIFVTsOg/
0yx/+c1M2XrkXHABdnzZ5oYFPvNZh3KHhBsGMAoGCCqGSM49BAMCA0gAMEUCIQDn
gu8tRu0NvsaTNG17i4QtjRftqg56ptadF5YHXBQCGgIgeowGfoCWCIW5T09110Gv
cu4tVgSe4rBzE4yjal6cQVQ=
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.382 UTC [cauthdsl] func1 -> DEBU 904[0m 0xc420166648 gate 1548760966382782881 evaluation starts
[36m2019-01-29 11:22:46.382 UTC [cauthdsl] func2 -> DEBU 905[0m 0xc420166648 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.382 UTC [cauthdsl] func2 -> DEBU 906[0m 0xc420166648 processing identity 0 with bytes of 0a0c486f73706974616c314d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435454434341664f674177494241674951445049686b5a62424d7a41505256326d64544a324254414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777784c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424d485366454e652b7134537668784c623646784c7232534c59672b0a714a6e4c3377684f72534461457951676d447a376677624c685772304d61706971384e33586731767564493658624e5a78794d54426d5a626433756a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149465654734f672f0a3079782f2b63314d3258726b58484142646e7a5a356f594650764e5a68334b48684273474d416f4743437147534d343942414d43413067414d4555434951446e0a677538745275304e767361544e473137693451746a5266747167353670746164463559485842514347674967656f7747666f43574349573554303931313047760a63753474566753653472427a4534796a616c36635156513d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func2 -> DEBU 907[0m 0xc420166648 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected OrdererMSP, got Hospital1MSP)
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func2 -> DEBU 908[0m 0xc420166648 principal evaluation fails
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func1 -> DEBU 909[0m 0xc420166648 gate 1548760966382782881 evaluation fails
[36m2019-01-29 11:22:46.383 UTC [policies] Evaluate -> DEBU 90a[0m Signature set did not satisfy policy /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 11:22:46.383 UTC [policies] Evaluate -> DEBU 90b[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 11:22:46.383 UTC [policies] func1 -> DEBU 90c[0m Evaluation Failed: Only 0 policies were satisfied, but needed 1 of [ OrdererOrg.Readers ]
[36m2019-01-29 11:22:46.383 UTC [policies] Evaluate -> DEBU 90d[0m Signature set did not satisfy policy /Channel/Orderer/Readers
[36m2019-01-29 11:22:46.383 UTC [policies] Evaluate -> DEBU 90e[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Orderer/Readers
[36m2019-01-29 11:22:46.383 UTC [policies] Evaluate -> DEBU 90f[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers ==
[36m2019-01-29 11:22:46.383 UTC [policies] Evaluate -> DEBU 910[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:46.383 UTC [policies] Evaluate -> DEBU 911[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Readers ==
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func1 -> DEBU 912[0m 0xc420166660 gate 1548760966383280586 evaluation starts
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func2 -> DEBU 913[0m 0xc420166660 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func2 -> DEBU 914[0m 0xc420166660 processing identity 0 with bytes of 0a0c486f73706974616c314d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435454434341664f674177494241674951445049686b5a62424d7a41505256326d64544a324254414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777784c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424d485366454e652b7134537668784c623646784c7232534c59672b0a714a6e4c3377684f72534461457951676d447a376677624c685772304d61706971384e33586731767564493658624e5a78794d54426d5a626433756a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149465654734f672f0a3079782f2b63314d3258726b58484142646e7a5a356f594650764e5a68334b48684273474d416f4743437147534d343942414d43413067414d4555434951446e0a677538745275304e767361544e473137693451746a5266747167353670746164463559485842514347674967656f7747666f43574349573554303931313047760a63753474566753653472427a4534796a616c36635156513d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func2 -> DEBU 915[0m 0xc420166660 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital2MSP, got Hospital1MSP)
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func2 -> DEBU 916[0m 0xc420166660 principal evaluation fails
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func1 -> DEBU 917[0m 0xc420166660 gate 1548760966383280586 evaluation fails
[36m2019-01-29 11:22:46.383 UTC [policies] Evaluate -> DEBU 918[0m Signature set did not satisfy policy /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:22:46.383 UTC [policies] Evaluate -> DEBU 919[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:22:46.383 UTC [policies] Evaluate -> DEBU 91a[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers ==
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func1 -> DEBU 91b[0m 0xc420166670 gate 1548760966383469834 evaluation starts
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func2 -> DEBU 91c[0m 0xc420166670 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func2 -> DEBU 91d[0m 0xc420166670 processing identity 0 with bytes of 0a0c486f73706974616c314d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435454434341664f674177494241674951445049686b5a62424d7a41505256326d64544a324254414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777784c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424d485366454e652b7134537668784c623646784c7232534c59672b0a714a6e4c3377684f72534461457951676d447a376677624c685772304d61706971384e33586731767564493658624e5a78794d54426d5a626433756a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149465654734f672f0a3079782f2b63314d3258726b58484142646e7a5a356f594650764e5a68334b48684273474d416f4743437147534d343942414d43413067414d4555434951446e0a677538745275304e767361544e473137693451746a5266747167353670746164463559485842514347674967656f7747666f43574349573554303931313047760a63753474566753653472427a4534796a616c36635156513d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func2 -> DEBU 91e[0m 0xc420166670 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital3MSP, got Hospital1MSP)
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func2 -> DEBU 91f[0m 0xc420166670 principal evaluation fails
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func1 -> DEBU 920[0m 0xc420166670 gate 1548760966383469834 evaluation fails
[36m2019-01-29 11:22:46.383 UTC [policies] Evaluate -> DEBU 921[0m Signature set did not satisfy policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:22:46.383 UTC [policies] Evaluate -> DEBU 922[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:22:46.383 UTC [policies] Evaluate -> DEBU 923[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers ==
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func1 -> DEBU 924[0m 0xc420166680 gate 1548760966383647203 evaluation starts
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func2 -> DEBU 925[0m 0xc420166680 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.383 UTC [cauthdsl] func2 -> DEBU 926[0m 0xc420166680 processing identity 0 with bytes of 0a0c486f73706974616c314d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435454434341664f674177494241674951445049686b5a62424d7a41505256326d64544a324254414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d53357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777784c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424d485366454e652b7134537668784c623646784c7232534c59672b0a714a6e4c3377684f72534461457951676d447a376677624c685772304d61706971384e33586731767564493658624e5a78794d54426d5a626433756a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149465654734f672f0a3079782f2b63314d3258726b58484142646e7a5a356f594650764e5a68334b48684273474d416f4743437147534d343942414d43413067414d4555434951446e0a677538745275304e767361544e473137693451746a5266747167353670746164463559485842514347674967656f7747666f43574349573554303931313047760a63753474566753653472427a4534796a616c36635156513d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.383 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 927[0m Checking if identity satisfies MEMBER role for Hospital1MSP
[36m2019-01-29 11:22:46.383 UTC [msp] Validate -> DEBU 928[0m MSP Hospital1MSP validating identity
[36m2019-01-29 11:22:46.384 UTC [msp] getCertificationChain -> DEBU 929[0m MSP Hospital1MSP getting certification chain
[36m2019-01-29 11:22:46.384 UTC [cauthdsl] func2 -> DEBU 92a[0m 0xc420166680 principal matched by identity 0
[36m2019-01-29 11:22:46.384 UTC [msp/identity] Verify -> DEBU 92b[0m Verify: digest = 00000000  32 4a 77 18 98 fe c5 2b  5c 1b b2 b7 16 e6 8b a0  |2Jw....+\.......|
00000010  3a 68 8a 92 b9 2c 2d c1  74 ad f1 95 77 0b 20 c4  |:h...,-.t...w. .|
[36m2019-01-29 11:22:46.384 UTC [msp/identity] Verify -> DEBU 92c[0m Verify: sig = 00000000  30 45 02 21 00 f2 71 66  5d d5 91 1f 73 38 68 65  |0E.!..qf]...s8he|
00000010  ef a6 15 ca f0 99 09 57  ac 81 30 6f 20 6c 6d f1  |.......W..0o lm.|
00000020  61 9f 5b c5 a3 02 20 6b  74 ab 2e a3 ea cf 54 16  |a.[... kt.....T.|
00000030  21 36 de 34 b2 84 d8 71  74 fb a7 8b 10 77 f0 de  |!6.4...qt....w..|
00000040  e5 74 af 6c 7d 89 a6                              |.t.l}..|
[36m2019-01-29 11:22:46.384 UTC [cauthdsl] func2 -> DEBU 92d[0m 0xc420166680 principal evaluation succeeds for identity 0
[36m2019-01-29 11:22:46.384 UTC [cauthdsl] func1 -> DEBU 92e[0m 0xc420166680 gate 1548760966383647203 evaluation succeeds
[36m2019-01-29 11:22:46.384 UTC [policies] Evaluate -> DEBU 92f[0m Signature set satisfies policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:22:46.384 UTC [policies] Evaluate -> DEBU 930[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital1MSP/Readers
[36m2019-01-29 11:22:46.384 UTC [policies] Evaluate -> DEBU 931[0m Signature set satisfies policy /Channel/Application/Readers
[36m2019-01-29 11:22:46.385 UTC [policies] Evaluate -> DEBU 932[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers
[36m2019-01-29 11:22:46.385 UTC [policies] Evaluate -> DEBU 933[0m Signature set satisfies policy /Channel/Readers
[36m2019-01-29 11:22:46.385 UTC [policies] Evaluate -> DEBU 934[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers
[36m2019-01-29 11:22:46.385 UTC [common/deliver] deliverBlocks -> DEBU 935[0m [channel: comunitychannel] Delivering block for (0xc42024bba0) for 10.0.0.34:59894
[36m2019-01-29 11:22:46.386 UTC [fsblkstorage] waitForBlock -> DEBU 936[0m Came out of wait. maxAvailaBlockNumber=[3]
[36m2019-01-29 11:22:46.386 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 937[0m Remaining bytes=[23781], Going to peek [8] bytes
[36m2019-01-29 11:22:46.386 UTC [fsblkstorage] waitForBlock -> DEBU 938[0m Came out of wait. maxAvailaBlockNumber=[3]
[36m2019-01-29 11:22:46.386 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 939[0m Remaining bytes=[23781], Going to peek [8] bytes
[36m2019-01-29 11:22:46.386 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 93a[0m Returning blockbytes - length=[23778], placementInfo={fileNum=[0], startOffset=[69094], bytesOffset=[69097]}
[36m2019-01-29 11:22:46.386 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 93b[0m blockbytes [23778] read from file [0]
[36m2019-01-29 11:22:46.386 UTC [policies] Evaluate -> DEBU 93c[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers ==
[36m2019-01-29 11:22:46.386 UTC [policies] Evaluate -> DEBU 93d[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:46.386 UTC [policies] Evaluate -> DEBU 93e[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Orderer/Readers ==
[36m2019-01-29 11:22:46.386 UTC [policies] Evaluate -> DEBU 93f[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:46.386 UTC [policies] Evaluate -> DEBU 940[0m == Evaluating *cauthdsl.policy Policy /Channel/Orderer/OrdererOrg/Readers ==
2019-01-29 11:22:46.386 UTC [msp] DeserializeIdentity -> INFO 941[0m Obtaining identity
[36m2019-01-29 11:22:46.387 UTC [msp/identity] newIdentity -> DEBU 942[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTjCCAfSgAwIBAgIRAKAnE4PbDWMeRl0DixdLiVswCgYIKoZIzj0EAwIwgYsx
CzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1TYW4g
RnJhbmNpc2NvMSUwIwYDVQQKExxob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnph
MSgwJgYDVQQDEx9jYS5ob3NwaXRhbDMuc3dpdGNoMmxvZ2ljLmNvLnphMB4XDTE5
MDEyOTA3NDE1NFoXDTI5MDEyNjA3NDE1NFowdjELMAkGA1UEBhMCVVMxEzARBgNV
BAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xDTALBgNVBAsT
BHBlZXIxKzApBgNVBAMTInBlZXIwLmhvc3BpdGFsMy5zd2l0Y2gybG9naWMuY28u
emEwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAASAvWW2l9XjwfiMiZ6Pt8Qrk81E
DaIfG8hRpCV7KHzrHmpiy3y88PlgzQhh3sQ5iYqAqw0Qjz6BrEbhedGqpv6Po00w
SzAOBgNVHQ8BAf8EBAMCB4AwDAYDVR0TAQH/BAIwADArBgNVHSMEJDAigCA7vVWF
XWpgZBojQ1ZLxUvgBLeia59MivIRcjxmYanpyTAKBggqhkjOPQQDAgNIADBFAiEA
2E0537skl5yXkVKSjrjr0UJC3GRPT8czRln3F2NB2ygCIApG3olic8fvYAJspMQH
92FzZlNWQ3BWDB4d6dq3Ae+/
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.388 UTC [cauthdsl] func1 -> DEBU 943[0m 0xc420174120 gate 1548760966388147053 evaluation starts
[36m2019-01-29 11:22:46.388 UTC [cauthdsl] func2 -> DEBU 944[0m 0xc420174120 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.388 UTC [cauthdsl] func2 -> DEBU 945[0m 0xc420174120 processing identity 0 with bytes of 0a0c486f73706974616c334d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943546a4343416653674177494241674952414b416e4534506244574d65526c30446978644c69567377436759494b6f5a497a6a304541774977675973780a437a414a42674e5642415954416c56544d524d77455159445651514945777044595778705a6d3979626d6c684d525977464159445651514845773154595734670a526e4a68626d4e7063324e764d535577497759445651514b4578786f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70680a4d5367774a675944565151444578396a5953356f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70684d423458445445350a4d4445794f5441334e4445314e466f58445449354d4445794e6a41334e4445314e466f77646a454c4d416b474131554542684d4356564d78457a415242674e560a42416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e6862694247636d467559326c7a593238784454414c42674e56424173540a4248426c5a5849784b7a417042674e5642414d54496e426c5a5849774c6d687663334270644746734d79357a64326c30593267796247396e61574d75593238750a656d45775754415442676371686b6a4f5051494242676771686b6a4f50514d4242774e4341415341765757326c39586a7766694d695a3650743851726b3831450a4461496647386852704356374b487a72486d70697933793838506c677a5168683373513569597141717730516a7a36427245626865644771707636506f3030770a537a414f42674e56485138424166384542414d434234417744415944565230544151482f424149774144417242674e5648534d454a44416967434137765657460a585770675a426f6a51315a4c78557667424c65696135394d69764952636a786d59616e707954414b42676771686b6a4f5051514441674e4941444246416945410a324530353337736b6c3579586b564b536a726a7230554a43334752505438637a526c6e3346324e423279674349417047336f6c696338667659414a73704d51480a3932467a5a6c4e5751334257444234643664713341652b2f0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.388 UTC [cauthdsl] func2 -> DEBU 946[0m 0xc420174120 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected OrdererMSP, got Hospital3MSP)
[36m2019-01-29 11:22:46.388 UTC [cauthdsl] func2 -> DEBU 947[0m 0xc420174120 principal evaluation fails
[36m2019-01-29 11:22:46.388 UTC [cauthdsl] func1 -> DEBU 948[0m 0xc420174120 gate 1548760966388147053 evaluation fails
[36m2019-01-29 11:22:46.388 UTC [policies] Evaluate -> DEBU 949[0m Signature set did not satisfy policy /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 11:22:46.388 UTC [policies] Evaluate -> DEBU 94a[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 11:22:46.388 UTC [policies] func1 -> DEBU 94b[0m Evaluation Failed: Only 0 policies were satisfied, but needed 1 of [ OrdererOrg.Readers ]
[36m2019-01-29 11:22:46.388 UTC [policies] Evaluate -> DEBU 94c[0m Signature set did not satisfy policy /Channel/Orderer/Readers
[36m2019-01-29 11:22:46.388 UTC [policies] Evaluate -> DEBU 94d[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Orderer/Readers
[36m2019-01-29 11:22:46.388 UTC [policies] Evaluate -> DEBU 94e[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers ==
[36m2019-01-29 11:22:46.388 UTC [policies] Evaluate -> DEBU 94f[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:46.388 UTC [policies] Evaluate -> DEBU 950[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Readers ==
[36m2019-01-29 11:22:46.388 UTC [cauthdsl] func1 -> DEBU 951[0m 0xc420174140 gate 1548760966388494625 evaluation starts
[36m2019-01-29 11:22:46.388 UTC [cauthdsl] func2 -> DEBU 952[0m 0xc420174140 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.388 UTC [cauthdsl] func2 -> DEBU 953[0m 0xc420174140 processing identity 0 with bytes of 0a0c486f73706974616c334d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943546a4343416653674177494241674952414b416e4534506244574d65526c30446978644c69567377436759494b6f5a497a6a304541774977675973780a437a414a42674e5642415954416c56544d524d77455159445651514945777044595778705a6d3979626d6c684d525977464159445651514845773154595734670a526e4a68626d4e7063324e764d535577497759445651514b4578786f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70680a4d5367774a675944565151444578396a5953356f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70684d423458445445350a4d4445794f5441334e4445314e466f58445449354d4445794e6a41334e4445314e466f77646a454c4d416b474131554542684d4356564d78457a415242674e560a42416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e6862694247636d467559326c7a593238784454414c42674e56424173540a4248426c5a5849784b7a417042674e5642414d54496e426c5a5849774c6d687663334270644746734d79357a64326c30593267796247396e61574d75593238750a656d45775754415442676371686b6a4f5051494242676771686b6a4f50514d4242774e4341415341765757326c39586a7766694d695a3650743851726b3831450a4461496647386852704356374b487a72486d70697933793838506c677a5168683373513569597141717730516a7a36427245626865644771707636506f3030770a537a414f42674e56485138424166384542414d434234417744415944565230544151482f424149774144417242674e5648534d454a44416967434137765657460a585770675a426f6a51315a4c78557667424c65696135394d69764952636a786d59616e707954414b42676771686b6a4f5051514441674e4941444246416945410a324530353337736b6c3579586b564b536a726a7230554a43334752505438637a526c6e3346324e423279674349417047336f6c696338667659414a73704d51480a3932467a5a6c4e5751334257444234643664713341652b2f0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.388 UTC [cauthdsl] func2 -> DEBU 954[0m 0xc420174140 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected Hospital2MSP, got Hospital3MSP)
[36m2019-01-29 11:22:46.388 UTC [cauthdsl] func2 -> DEBU 955[0m 0xc420174140 principal evaluation fails
[36m2019-01-29 11:22:46.388 UTC [cauthdsl] func1 -> DEBU 956[0m 0xc420174140 gate 1548760966388494625 evaluation fails
[36m2019-01-29 11:22:46.388 UTC [policies] Evaluate -> DEBU 957[0m Signature set did not satisfy policy /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:22:46.388 UTC [policies] Evaluate -> DEBU 958[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:22:46.388 UTC [policies] Evaluate -> DEBU 959[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers ==
[36m2019-01-29 11:22:46.388 UTC [cauthdsl] func1 -> DEBU 95a[0m 0xc4201741b0 gate 1548760966388691621 evaluation starts
[36m2019-01-29 11:22:46.388 UTC [cauthdsl] func2 -> DEBU 95b[0m 0xc4201741b0 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.388 UTC [cauthdsl] func2 -> DEBU 95c[0m 0xc4201741b0 processing identity 0 with bytes of 0a0c486f73706974616c334d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d494943546a4343416653674177494241674952414b416e4534506244574d65526c30446978644c69567377436759494b6f5a497a6a304541774977675973780a437a414a42674e5642415954416c56544d524d77455159445651514945777044595778705a6d3979626d6c684d525977464159445651514845773154595734670a526e4a68626d4e7063324e764d535577497759445651514b4578786f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70680a4d5367774a675944565151444578396a5953356f62334e776158526862444d756333647064474e6f4d6d78765a326c6a4c6d4e764c6e70684d423458445445350a4d4445794f5441334e4445314e466f58445449354d4445794e6a41334e4445314e466f77646a454c4d416b474131554542684d4356564d78457a415242674e560a42416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e6862694247636d467559326c7a593238784454414c42674e56424173540a4248426c5a5849784b7a417042674e5642414d54496e426c5a5849774c6d687663334270644746734d79357a64326c30593267796247396e61574d75593238750a656d45775754415442676371686b6a4f5051494242676771686b6a4f50514d4242774e4341415341765757326c39586a7766694d695a3650743851726b3831450a4461496647386852704356374b487a72486d70697933793838506c677a5168683373513569597141717730516a7a36427245626865644771707636506f3030770a537a414f42674e56485138424166384542414d434234417744415944565230544151482f424149774144417242674e5648534d454a44416967434137765657460a585770675a426f6a51315a4c78557667424c65696135394d69764952636a786d59616e707954414b42676771686b6a4f5051514441674e4941444246416945410a324530353337736b6c3579586b564b536a726a7230554a43334752505438637a526c6e3346324e423279674349417047336f6c696338667659414a73704d51480a3932467a5a6c4e5751334257444234643664713341652b2f0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.388 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 95d[0m Checking if identity satisfies MEMBER role for Hospital3MSP
[36m2019-01-29 11:22:46.388 UTC [msp] Validate -> DEBU 95e[0m MSP Hospital3MSP validating identity
[36m2019-01-29 11:22:46.389 UTC [msp] getCertificationChain -> DEBU 95f[0m MSP Hospital3MSP getting certification chain
[36m2019-01-29 11:22:46.389 UTC [cauthdsl] func2 -> DEBU 960[0m 0xc4201741b0 principal matched by identity 0
[36m2019-01-29 11:22:46.389 UTC [msp/identity] Verify -> DEBU 961[0m Verify: digest = 00000000  e8 ed 40 b8 9e af b2 ef  06 7b 8c 10 da 7a e2 8d  |..@......{...z..|
00000010  b6 64 38 ca 51 7f 36 1a  cc a8 45 f2 4d 5d d3 49  |.d8.Q.6...E.M].I|
[36m2019-01-29 11:22:46.389 UTC [msp/identity] Verify -> DEBU 962[0m Verify: sig = 00000000  30 45 02 21 00 cf 67 37  c5 fb 18 5d e8 8e ff 76  |0E.!..g7...]...v|
00000010  16 89 7b d5 ad fd 0d ea  64 bc fd a0 19 ee 91 80  |..{.....d.......|
00000020  25 3f 3d 8b d3 02 20 40  ee 79 e7 36 16 b4 15 0c  |%?=... @.y.6....|
00000030  be 6e e7 19 98 3a 3a 54  51 5b ec 7e ff ef 8c 43  |.n...::TQ[.~...C|
00000040  98 48 cd f9 75 a5 31                              |.H..u.1|
[36m2019-01-29 11:22:46.389 UTC [cauthdsl] func2 -> DEBU 963[0m 0xc4201741b0 principal evaluation succeeds for identity 0
[36m2019-01-29 11:22:46.389 UTC [cauthdsl] func1 -> DEBU 964[0m 0xc4201741b0 gate 1548760966388691621 evaluation succeeds
[36m2019-01-29 11:22:46.389 UTC [policies] Evaluate -> DEBU 965[0m Signature set satisfies policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:22:46.389 UTC [policies] Evaluate -> DEBU 966[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital3MSP/Readers
[36m2019-01-29 11:22:46.389 UTC [policies] Evaluate -> DEBU 967[0m Signature set satisfies policy /Channel/Application/Readers
[36m2019-01-29 11:22:46.389 UTC [policies] Evaluate -> DEBU 968[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers
[36m2019-01-29 11:22:46.389 UTC [policies] Evaluate -> DEBU 969[0m Signature set satisfies policy /Channel/Readers
[36m2019-01-29 11:22:46.389 UTC [policies] Evaluate -> DEBU 96a[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers
[36m2019-01-29 11:22:46.389 UTC [common/deliver] deliverBlocks -> DEBU 96b[0m [channel: comunitychannel] Delivering block for (0xc42024a5a0) for 10.0.0.18:59908
[36m2019-01-29 11:22:46.390 UTC [fsblkstorage] waitForBlock -> DEBU 96c[0m Going to wait for newer blocks. maxAvailaBlockNumber=[3], waitForBlockNum=[4]
[36m2019-01-29 11:22:46.390 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 96d[0m Returning blockbytes - length=[23778], placementInfo={fileNum=[0], startOffset=[69094], bytesOffset=[69097]}
[36m2019-01-29 11:22:46.390 UTC [fsblkstorage] nextBlockBytesAndPlacementInfo -> DEBU 96e[0m blockbytes [23778] read from file [0]
[36m2019-01-29 11:22:46.390 UTC [policies] Evaluate -> DEBU 96f[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers ==
[36m2019-01-29 11:22:46.390 UTC [policies] Evaluate -> DEBU 970[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:46.390 UTC [policies] Evaluate -> DEBU 971[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Orderer/Readers ==
[36m2019-01-29 11:22:46.390 UTC [policies] Evaluate -> DEBU 972[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:46.390 UTC [policies] Evaluate -> DEBU 973[0m == Evaluating *cauthdsl.policy Policy /Channel/Orderer/OrdererOrg/Readers ==
2019-01-29 11:22:46.390 UTC [msp] DeserializeIdentity -> INFO 974[0m Obtaining identity
[36m2019-01-29 11:22:46.391 UTC [msp/identity] newIdentity -> DEBU 975[0m Creating identity instance for cert -----BEGIN CERTIFICATE-----
MIICTDCCAfOgAwIBAgIQYZ1E6CL3TI5r9J0X5CY/6zAKBggqhkjOPQQDAjCBizEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWExFjAUBgNVBAcTDVNhbiBG
cmFuY2lzY28xJTAjBgNVBAoTHGhvc3BpdGFsMi5zd2l0Y2gybG9naWMuY28uemEx
KDAmBgNVBAMTH2NhLmhvc3BpdGFsMi5zd2l0Y2gybG9naWMuY28uemEwHhcNMTkw
MTI5MDc0MTU0WhcNMjkwMTI2MDc0MTU0WjB2MQswCQYDVQQGEwJVUzETMBEGA1UE
CBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzENMAsGA1UECxME
cGVlcjErMCkGA1UEAxMicGVlcjAuaG9zcGl0YWwyLnN3aXRjaDJsb2dpYy5jby56
YTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABOoEzz0FFdYNv89IgW1kmLaRhSux
ZZyS6FfMn/s+eLJInhN8XZRcfo5jAL5wHQus9ka5err+7PFzn9+DQlEIb46jTTBL
MA4GA1UdDwEB/wQEAwIHgDAMBgNVHRMBAf8EAjAAMCsGA1UdIwQkMCKAIPECalij
2l9uByDcEUurCDRUsEQ/uONP3HEp7xVWhhduMAoGCCqGSM49BAMCA0cAMEQCIESI
6Ro/uHS6tC9ccjZumBNE+ffnKjI2UoZeL3zGRXVXAiBeirbdVLCZ46LhSPayifiC
LJj/EF+z8kPVLYe/SoDSRg==
-----END CERTIFICATE-----
[36m2019-01-29 11:22:46.391 UTC [cauthdsl] func1 -> DEBU 976[0m 0xc42000e060 gate 1548760966391436498 evaluation starts
[36m2019-01-29 11:22:46.391 UTC [cauthdsl] func2 -> DEBU 977[0m 0xc42000e060 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.391 UTC [cauthdsl] func2 -> DEBU 978[0m 0xc42000e060 processing identity 0 with bytes of 0a0c486f73706974616c324d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435444434341664f674177494241674951595a314536434c3354493572394a30583543592f367a414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777794c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424f6f457a7a30464664594e763839496757316b6d4c6152685375780a5a5a79533646664d6e2f732b654c4a496e684e38585a5263666f356a414c357748517573396b61356572722b3750467a6e392b44516c45496234366a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149504543616c696a0a326c39754279446345557572434452557345512f754f4e503348457037785657686864754d416f4743437147534d343942414d43413063414d455143494553490a36526f2f7548533674433963636a5a756d424e452b66666e4b6a4932556f5a654c337a47525856584169426569726264564c435a34364c6853506179696669430a4c4a6a2f45462b7a386b50564c59652f536f445352673d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.391 UTC [cauthdsl] func2 -> DEBU 979[0m 0xc42000e060 identity 0 does not satisfy principal: the identity is a member of a different MSP (expected OrdererMSP, got Hospital2MSP)
[36m2019-01-29 11:22:46.391 UTC [cauthdsl] func2 -> DEBU 97a[0m 0xc42000e060 principal evaluation fails
[36m2019-01-29 11:22:46.391 UTC [cauthdsl] func1 -> DEBU 97b[0m 0xc42000e060 gate 1548760966391436498 evaluation fails
[36m2019-01-29 11:22:46.391 UTC [policies] Evaluate -> DEBU 97c[0m Signature set did not satisfy policy /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 11:22:46.391 UTC [policies] Evaluate -> DEBU 97d[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Orderer/OrdererOrg/Readers
[36m2019-01-29 11:22:46.391 UTC [policies] func1 -> DEBU 97e[0m Evaluation Failed: Only 0 policies were satisfied, but needed 1 of [ OrdererOrg.Readers ]
[36m2019-01-29 11:22:46.391 UTC [policies] Evaluate -> DEBU 97f[0m Signature set did not satisfy policy /Channel/Orderer/Readers
[36m2019-01-29 11:22:46.391 UTC [policies] Evaluate -> DEBU 980[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Orderer/Readers
[36m2019-01-29 11:22:46.391 UTC [policies] Evaluate -> DEBU 981[0m == Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers ==
[36m2019-01-29 11:22:46.391 UTC [policies] Evaluate -> DEBU 982[0m This is an implicit meta policy, it will trigger other policy evaluations, whose failures may be benign
[36m2019-01-29 11:22:46.391 UTC [policies] Evaluate -> DEBU 983[0m == Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Readers ==
[36m2019-01-29 11:22:46.391 UTC [cauthdsl] func1 -> DEBU 984[0m 0xc42000e0a0 gate 1548760966391741614 evaluation starts
[36m2019-01-29 11:22:46.391 UTC [cauthdsl] func2 -> DEBU 985[0m 0xc42000e0a0 signed by 0 principal evaluation starts (used [false])
[36m2019-01-29 11:22:46.391 UTC [cauthdsl] func2 -> DEBU 986[0m 0xc42000e0a0 processing identity 0 with bytes of 0a0c486f73706974616c324d535012db062d2d2d2d2d424547494e2043455254494649434154452d2d2d2d2d0a4d4949435444434341664f674177494241674951595a314536434c3354493572394a30583543592f367a414b42676771686b6a4f50515144416a4342697a454c0a4d416b474131554542684d4356564d78457a415242674e5642416754436b4e6862476c6d62334a7561574578466a415542674e564241635444564e68626942470a636d467559326c7a593238784a54416a42674e5642416f544847687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45780a4b44416d42674e5642414d5448324e684c6d687663334270644746734d69357a64326c30593267796247396e61574d7559323875656d45774868634e4d546b770a4d5449354d4463304d5455305768634e4d6a6b774d5449324d4463304d545530576a42324d517377435159445651514745774a56557a45544d424547413155450a43424d4b5132467361575a76636d3570595445574d4251474131554542784d4e5532467549455a795957356a61584e6a627a454e4d4173474131554543784d450a6347566c636a45724d436b474131554541784d696347566c636a41756147397a63476c30595777794c6e4e336158526a61444a73623264705979356a627935360a5954425a4d424d4742797147534d34394167454743437147534d34394177454841304941424f6f457a7a30464664594e763839496757316b6d4c6152685375780a5a5a79533646664d6e2f732b654c4a496e684e38585a5263666f356a414c357748517573396b61356572722b3750467a6e392b44516c45496234366a5454424c0a4d41344741315564447745422f775145417749486744414d42674e5648524d4241663845416a41414d437347413155644977516b4d434b4149504543616c696a0a326c39754279446345557572434452557345512f754f4e503348457037785657686864754d416f4743437147534d343942414d43413063414d455143494553490a36526f2f7548533674433963636a5a756d424e452b66666e4b6a4932556f5a654c337a47525856584169426569726264564c435a34364c6853506179696669430a4c4a6a2f45462b7a386b50564c59652f536f445352673d3d0a2d2d2d2d2d454e442043455254494649434154452d2d2d2d2d0a
[36m2019-01-29 11:22:46.391 UTC [msp] satisfiesPrincipalInternalPreV13 -> DEBU 987[0m Checking if identity satisfies MEMBER role for Hospital2MSP
[36m2019-01-29 11:22:46.391 UTC [msp] Validate -> DEBU 988[0m MSP Hospital2MSP validating identity
[36m2019-01-29 11:22:46.392 UTC [msp] getCertificationChain -> DEBU 989[0m MSP Hospital2MSP getting certification chain
[36m2019-01-29 11:22:46.392 UTC [cauthdsl] func2 -> DEBU 98a[0m 0xc42000e0a0 principal matched by identity 0
[36m2019-01-29 11:22:46.392 UTC [msp/identity] Verify -> DEBU 98b[0m Verify: digest = 00000000  76 23 69 11 6d b0 29 7c  82 09 7e a7 b2 2e e7 03  |v#i.m.)|..~.....|
00000010  96 f4 58 a2 6b 4f 33 b6  d4 0b 90 37 8e b6 d8 fe  |..X.kO3....7....|
[36m2019-01-29 11:22:46.392 UTC [msp/identity] Verify -> DEBU 98c[0m Verify: sig = 00000000  30 45 02 21 00 a5 39 97  6f 8a 30 42 89 db 48 e7  |0E.!..9.o.0B..H.|
00000010  40 0d 68 2c cd 2a 4e e0  55 7d 6d e7 d7 27 bf 97  |@.h,.*N.U}m..'..|
00000020  da 8e 53 6e c4 02 20 1d  41 e3 fc eb 8c 2c e2 db  |..Sn.. .A....,..|
00000030  15 ab a6 e7 bd e2 08 fa  0c 25 c1 03 e7 1e 60 3e  |.........%....`>|
00000040  a5 b1 85 5f 45 1e ea                              |..._E..|
[36m2019-01-29 11:22:46.392 UTC [cauthdsl] func2 -> DEBU 98d[0m 0xc42000e0a0 principal evaluation succeeds for identity 0
[36m2019-01-29 11:22:46.392 UTC [cauthdsl] func1 -> DEBU 98e[0m 0xc42000e0a0 gate 1548760966391741614 evaluation succeeds
[36m2019-01-29 11:22:46.392 UTC [policies] Evaluate -> DEBU 98f[0m Signature set satisfies policy /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:22:46.392 UTC [policies] Evaluate -> DEBU 990[0m == Done Evaluating *cauthdsl.policy Policy /Channel/Application/Hospital2MSP/Readers
[36m2019-01-29 11:22:46.392 UTC [policies] Evaluate -> DEBU 991[0m Signature set satisfies policy /Channel/Application/Readers
[36m2019-01-29 11:22:46.392 UTC [policies] Evaluate -> DEBU 992[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Application/Readers
[36m2019-01-29 11:22:46.393 UTC [policies] Evaluate -> DEBU 993[0m Signature set satisfies policy /Channel/Readers
[36m2019-01-29 11:22:46.393 UTC [policies] Evaluate -> DEBU 994[0m == Done Evaluating *policies.implicitMetaPolicy Policy /Channel/Readers
[36m2019-01-29 11:22:46.393 UTC [common/deliver] deliverBlocks -> DEBU 995[0m [channel: comunitychannel] Delivering block for (0xc42028ed00) for 10.0.0.39:47750
[36m2019-01-29 11:22:46.393 UTC [fsblkstorage] waitForBlock -> DEBU 996[0m Going to wait for newer blocks. maxAvailaBlockNumber=[3], waitForBlockNum=[4]
[36m2019-01-29 11:22:46.404 UTC [fsblkstorage] waitForBlock -> DEBU 997[0m Going to wait for newer blocks. maxAvailaBlockNumber=[3], waitForBlockNum=[4]
[36m2019-01-29 11:27:02.173 UTC [orderer/consensus/kafka] try -> DEBU 998[0m [channel: testchainid] Connecting to the Kafka cluster
[36m2019-01-29 11:27:02.178 UTC [orderer/consensus/kafka] try -> DEBU 999[0m [channel: testchainid] Need to retry because process failed = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.
